{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Becoming a Backprop Ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D://Datasets/names.txt\", 'r') as file:\n",
    "    names = file.read().splitlines()\n",
    "names[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = sorted(list(set(''.join(names))))\n",
    "chartoidx = {}\n",
    "idxtochar = {}\n",
    "chartoidx['.'] = 0   # Putting a special token to denote the start and the end of a sentence.\n",
    "idxtochar[0] = '.'\n",
    "for i,char in enumerate(vocabulary):\n",
    "    chartoidx[char] = i+1\n",
    "    idxtochar[i+1] = char\n",
    "\n",
    "chartoidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182778 182778\n",
      "22633 22633\n",
      "22735 22735\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # This would be used to set the number of context letters that would be in the word.\n",
    "# blank_context = idxtochar[0] * block_size  # This would produce the context consisting of only special character \".\"\n",
    "# print(f\"The blank context is {blank_context}\")\n",
    "\n",
    "# Now we would need to split the names into train, dev and test sets\n",
    "train_names = names[:int(0.8*len(names))]\n",
    "dev_names = names[int(0.8*len(names)): int(0.9*len(names))]\n",
    "test_names = names[int(0.9*len(names)):]\n",
    "\n",
    "# Now we need to define our xs and ys\n",
    "X_train, y_train = [],[]\n",
    "X_dev, y_dev = [],[]\n",
    "X_test, y_test = [], []\n",
    "\n",
    "# Now we need to add the data into our xs and ys:\n",
    "for i,split in enumerate([train_names, dev_names, test_names]):\n",
    "    for word in split:\n",
    "        word = word+\".\"\n",
    "        blank_context = [0] * block_size  # This would produce the context consisting of only special character \".\"\n",
    "        # xs.append(blank_context)\n",
    "        if i == 0:\n",
    "            for ch in word:\n",
    "                X_train.append(blank_context)\n",
    "                y_train.append(chartoidx[ch])\n",
    "                blank_context = blank_context[1:] + [chartoidx[ch]]\n",
    "        if i == 1:\n",
    "            for ch in word:\n",
    "                X_dev.append(blank_context)\n",
    "                y_dev.append(chartoidx[ch])\n",
    "                blank_context = blank_context[1:] + [chartoidx[ch]]\n",
    "        if i == 2:\n",
    "            for ch in word:\n",
    "                X_test.append(blank_context)\n",
    "                y_test.append(chartoidx[ch])\n",
    "                blank_context = blank_context[1:] + [chartoidx[ch]]\n",
    "      \n",
    "        # print(blank_context)\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_dev = torch.tensor(X_dev)\n",
    "y_dev = torch.tensor(y_dev)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)  \n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_dev), len(y_dev))\n",
    "print(len(X_test), len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
