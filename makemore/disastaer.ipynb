{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying Disater Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/Datasets/nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610        M1.94 [01:04 UTC]?5km S of Volcano Hawaii.        1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the url links of the tweets\n",
    "g = re.compile(r\"https?://[\\w.]*/?\\w*\\s*\")  # the regular expression used to remove the urls from the tweet data.\n",
    "data['text'] = data['text'].apply(lambda y: re.subn(g, '', string = y)[0])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@aria_ahrary , @TheTawniest ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii.</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610        M1.94 [01:04 UTC]?5km S of Volcano Hawaii.        1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                  hashtag                        mentions  \n",
       "0            [earthquake]                             NaN  \n",
       "1                      []                             NaN  \n",
       "2                      []                             NaN  \n",
       "3             [wildfires]                             NaN  \n",
       "4     [Alaska, wildfires]                             NaN  \n",
       "...                   ...                             ...  \n",
       "7608                   []                             NaN  \n",
       "7609                   []  [@aria_ahrary , @TheTawniest ]  \n",
       "7610                   []                             NaN  \n",
       "7611                   []                             NaN  \n",
       "7612                   []                             NaN  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the hashtags of the tweet and add them to a seperate column\n",
    "def get_hashtags(x):\n",
    "    g = re.compile(r\"#[A-Za-z]*\\s*\")\n",
    "    hash_tag_list = re.findall(g,string = x)\n",
    "    for i,tag in enumerate(hash_tag_list):\n",
    "        hash_tag_list[i] =  tag[1:].strip()\n",
    "    return hash_tag_list\n",
    "def get_mentions(x):\n",
    "    mention_list = re.findall(re.compile('@[a-zA-Z0-9_]*\\s*'),string = x)\n",
    "    if len(mention_list) >=1:\n",
    "        return mention_list\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "data['hashtag'] = data['text'].apply(lambda x : get_hashtags(x))\n",
    "data['mentions'] = data['text'].apply(lambda x : get_mentions(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(entry):\n",
    "    entry = entry.lower()\n",
    "    entry = entry.replace('\\n', '')\n",
    "    entry = re.subn(re.compile('=>'), '', string=entry)[0]\n",
    "    entry = re.subn(re.compile('!!+'), '', string=entry)[0]\n",
    "    entry = re.subn(re.compile('#[^a-z]+'), '', string = entry)[0]\n",
    "    entry = re.subn(re.compile('\\.'), '', string=entry)[0]\n",
    "    entry = re.subn(re.compile('@[a-zA-Z0-9_]*\\s*'), '', string = entry)[0]\n",
    "    entry = entry.replace('*','')\n",
    "    \n",
    "    entry_split = entry.split()\n",
    "    for i, word in enumerate(entry_split):\n",
    "        if word.startswith(\"'\"):\n",
    "            entry_split[i] = word[1:]\n",
    "        elif word.endswith(\"'\"):\n",
    "            entry_split[i] = word[:-1]\n",
    "    entry = ' '.join(entry_split)\n",
    "    return entry\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x : clean(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would use a mention token for tweets that have mentions in them.\n",
    "That would be @ in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18874\n"
     ]
    }
   ],
   "source": [
    "# Lets create a vocabulary\n",
    "vocab = set()\n",
    "vocab_dict = {}\n",
    "for entry in data['text']:\n",
    "    word_list = entry.split(' ')\n",
    "    for word in word_list:\n",
    "        # word = word.replace('\\n', '')\n",
    "        # word = re.subn(re.compile('!!+'), '', string=word)[0]\n",
    "        # word = re.subn(re.compile('#[^a-z]+'), '', string = word)[0]\n",
    "        if word in vocab_dict:\n",
    "            vocab_dict[word] += 1\n",
    "        else:\n",
    "            vocab_dict[word] = 1\n",
    "        vocab.add(word)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'our': 0,\n",
       " 'deeds': 1,\n",
       " 'are': 2,\n",
       " 'the': 3,\n",
       " 'reason': 4,\n",
       " 'of': 5,\n",
       " 'this': 6,\n",
       " '#earthquake': 7,\n",
       " 'may': 8,\n",
       " 'allah': 9,\n",
       " 'forgive': 10,\n",
       " 'us': 11,\n",
       " 'all': 12,\n",
       " 'forest': 13,\n",
       " 'fire': 14,\n",
       " 'near': 15,\n",
       " 'la': 16,\n",
       " 'ronge': 17,\n",
       " 'sask': 18,\n",
       " 'canada': 19,\n",
       " 'residents': 20,\n",
       " 'asked': 21,\n",
       " 'to': 22,\n",
       " 'shelter': 23,\n",
       " 'in': 24,\n",
       " 'place': 25,\n",
       " 'being': 26,\n",
       " 'notified': 27,\n",
       " 'by': 28,\n",
       " 'officers': 29,\n",
       " 'no': 30,\n",
       " 'other': 31,\n",
       " 'evacuation': 32,\n",
       " 'or': 33,\n",
       " 'orders': 34,\n",
       " 'expected': 35,\n",
       " '13,000': 36,\n",
       " 'people': 37,\n",
       " 'receive': 38,\n",
       " '#wildfires': 39,\n",
       " 'california': 40,\n",
       " 'just': 41,\n",
       " 'got': 42,\n",
       " 'sent': 43,\n",
       " 'photo': 44,\n",
       " 'from': 45,\n",
       " 'ruby': 46,\n",
       " '#alaska': 47,\n",
       " 'as': 48,\n",
       " 'smoke': 49,\n",
       " 'pours': 50,\n",
       " 'into': 51,\n",
       " 'a': 52,\n",
       " 'school': 53,\n",
       " '#rockyfire': 54,\n",
       " 'update': 55,\n",
       " 'hwy': 56,\n",
       " '20': 57,\n",
       " 'closed': 58,\n",
       " 'both': 59,\n",
       " 'directions': 60,\n",
       " 'due': 61,\n",
       " 'lake': 62,\n",
       " 'county': 63,\n",
       " '-': 64,\n",
       " '#cafire': 65,\n",
       " '#flood': 66,\n",
       " '#disaster': 67,\n",
       " 'heavy': 68,\n",
       " 'rain': 69,\n",
       " 'causes': 70,\n",
       " 'flash': 71,\n",
       " 'flooding': 72,\n",
       " 'streets': 73,\n",
       " 'manitou,': 74,\n",
       " 'colorado': 75,\n",
       " 'springs': 76,\n",
       " 'areas': 77,\n",
       " \"i'm\": 78,\n",
       " 'on': 79,\n",
       " 'top': 80,\n",
       " 'hill': 81,\n",
       " 'and': 82,\n",
       " 'i': 83,\n",
       " 'can': 84,\n",
       " 'see': 85,\n",
       " 'woods': 86,\n",
       " \"there's\": 87,\n",
       " 'an': 88,\n",
       " 'emergency': 89,\n",
       " 'happening': 90,\n",
       " 'now': 91,\n",
       " 'building': 92,\n",
       " 'across': 93,\n",
       " 'street': 94,\n",
       " 'afraid': 95,\n",
       " 'that': 96,\n",
       " 'tornado': 97,\n",
       " 'is': 98,\n",
       " 'coming': 99,\n",
       " 'area': 100,\n",
       " 'three': 101,\n",
       " 'died': 102,\n",
       " 'heat': 103,\n",
       " 'wave': 104,\n",
       " 'so': 105,\n",
       " 'far': 106,\n",
       " 'haha': 107,\n",
       " 'south': 108,\n",
       " 'tampa': 109,\n",
       " 'getting': 110,\n",
       " 'flooded': 111,\n",
       " 'hah-': 112,\n",
       " 'wait': 113,\n",
       " 'second': 114,\n",
       " 'live': 115,\n",
       " 'what': 116,\n",
       " 'am': 117,\n",
       " 'gonna': 118,\n",
       " 'do': 119,\n",
       " 'fvck': 120,\n",
       " '#flooding': 121,\n",
       " '#raining': 122,\n",
       " '#florida': 123,\n",
       " '#tampabay': 124,\n",
       " '#tampa': 125,\n",
       " '18': 126,\n",
       " '19': 127,\n",
       " 'days': 128,\n",
       " \"i've\": 129,\n",
       " 'lost': 130,\n",
       " 'count': 131,\n",
       " 'bago': 132,\n",
       " 'myanmar': 133,\n",
       " '#we': 134,\n",
       " 'arrived': 135,\n",
       " 'damage': 136,\n",
       " 'bus': 137,\n",
       " '80': 138,\n",
       " 'multi': 139,\n",
       " 'car': 140,\n",
       " 'crash': 141,\n",
       " '#breaking': 142,\n",
       " \"what's\": 143,\n",
       " 'up': 144,\n",
       " 'man?': 145,\n",
       " 'love': 146,\n",
       " 'fruits': 147,\n",
       " 'summer': 148,\n",
       " 'lovely': 149,\n",
       " 'my': 150,\n",
       " 'fast': 151,\n",
       " 'goooooooaaaaaal': 152,\n",
       " 'ridiculous': 153,\n",
       " 'london': 154,\n",
       " 'cool': 155,\n",
       " ';)': 156,\n",
       " 'skiing': 157,\n",
       " 'wonderful': 158,\n",
       " 'day!': 159,\n",
       " 'looooool': 160,\n",
       " 'wayi': 161,\n",
       " \"can't\": 162,\n",
       " 'eat': 163,\n",
       " 'shit': 164,\n",
       " 'was': 165,\n",
       " 'nyc': 166,\n",
       " 'last': 167,\n",
       " 'week!': 168,\n",
       " 'girlfriend': 169,\n",
       " 'cooool': 170,\n",
       " ':)': 171,\n",
       " 'you': 172,\n",
       " 'like': 173,\n",
       " 'pasta?': 174,\n",
       " 'end!': 175,\n",
       " 'wholesale': 176,\n",
       " 'markets': 177,\n",
       " 'ablaze': 178,\n",
       " 'we': 179,\n",
       " 'always': 180,\n",
       " 'try': 181,\n",
       " 'bring': 182,\n",
       " '#metal': 183,\n",
       " '#rt': 184,\n",
       " '#africanbaze:': 185,\n",
       " 'breaking': 186,\n",
       " 'news:nigeria': 187,\n",
       " 'flag': 188,\n",
       " 'set': 189,\n",
       " 'aba': 190,\n",
       " 'crying': 191,\n",
       " 'out': 192,\n",
       " 'for': 193,\n",
       " 'more!': 194,\n",
       " 'me': 195,\n",
       " 'plus': 196,\n",
       " 'side': 197,\n",
       " 'look': 198,\n",
       " 'at': 199,\n",
       " 'sky': 200,\n",
       " 'night': 201,\n",
       " 'it': 202,\n",
       " '#mufc': 203,\n",
       " \"they've\": 204,\n",
       " 'built': 205,\n",
       " 'much': 206,\n",
       " 'hype': 207,\n",
       " 'around': 208,\n",
       " 'new': 209,\n",
       " 'acquisitions': 210,\n",
       " 'but': 211,\n",
       " 'doubt': 212,\n",
       " 'they': 213,\n",
       " 'will': 214,\n",
       " 'epl': 215,\n",
       " 'season': 216,\n",
       " 'inec': 217,\n",
       " 'office': 218,\n",
       " 'abia': 219,\n",
       " 'barbados': 220,\n",
       " '#bridgetown': 221,\n",
       " 'jamaica': 222,\n",
       " '\\x89ûò': 223,\n",
       " 'two': 224,\n",
       " 'cars': 225,\n",
       " 'ablaze:': 226,\n",
       " 'santa': 227,\n",
       " 'cruz': 228,\n",
       " '\\x89ûó': 229,\n",
       " 'head': 230,\n",
       " 'st': 231,\n",
       " 'elizabeth': 232,\n",
       " 'police': 233,\n",
       " 'superintende': 234,\n",
       " 'lord': 235,\n",
       " ':d': 236,\n",
       " 'check': 237,\n",
       " 'these': 238,\n",
       " 'out:': 239,\n",
       " '#nsfw': 240,\n",
       " 'outside': 241,\n",
       " \"you're\": 242,\n",
       " 'alivebut': 243,\n",
       " 'dead': 244,\n",
       " 'inside': 245,\n",
       " 'had': 246,\n",
       " 'awesome': 247,\n",
       " 'time': 248,\n",
       " 'visiting': 249,\n",
       " 'cfc': 250,\n",
       " 'ancop': 251,\n",
       " 'site': 252,\n",
       " 'thanks': 253,\n",
       " 'tita': 254,\n",
       " 'vida': 255,\n",
       " 'taking': 256,\n",
       " 'care': 257,\n",
       " '??': 258,\n",
       " 'soooo': 259,\n",
       " 'pumped': 260,\n",
       " '????': 261,\n",
       " 'wanted': 262,\n",
       " 'chicago': 263,\n",
       " 'with': 264,\n",
       " 'preaching': 265,\n",
       " 'not': 266,\n",
       " 'hotel!': 267,\n",
       " 'gained': 268,\n",
       " '3': 269,\n",
       " 'followers': 270,\n",
       " 'week': 271,\n",
       " 'you?': 272,\n",
       " 'know': 273,\n",
       " 'your': 274,\n",
       " 'stats': 275,\n",
       " 'grow': 276,\n",
       " 'how': 277,\n",
       " 'west': 278,\n",
       " 'burned:': 279,\n",
       " 'thousands': 280,\n",
       " 'wildfires': 281,\n",
       " 'alone': 282,\n",
       " 'perfect': 283,\n",
       " 'tracklist': 284,\n",
       " 'life': 285,\n",
       " 'leave': 286,\n",
       " 'first': 287,\n",
       " 'retainers': 288,\n",
       " \"it's\": 289,\n",
       " 'quite': 290,\n",
       " 'weird': 291,\n",
       " 'better': 292,\n",
       " 'get': 293,\n",
       " 'used': 294,\n",
       " 'it;': 295,\n",
       " 'have': 296,\n",
       " 'wear': 297,\n",
       " 'them': 298,\n",
       " 'every': 299,\n",
       " 'single': 300,\n",
       " 'next': 301,\n",
       " 'year': 302,\n",
       " 'least': 303,\n",
       " 'deputies:': 304,\n",
       " 'man': 305,\n",
       " 'shot': 306,\n",
       " 'before': 307,\n",
       " 'brighton': 308,\n",
       " 'home': 309,\n",
       " 'wife': 310,\n",
       " 'six': 311,\n",
       " 'years': 312,\n",
       " 'jail': 313,\n",
       " 'setting': 314,\n",
       " 'niece': 315,\n",
       " 'superintendent': 316,\n",
       " 'lanford': 317,\n",
       " 'salmon': 318,\n",
       " 'has': 319,\n",
       " 'r': 320,\n",
       " 'police:': 321,\n",
       " 'arsonist': 322,\n",
       " 'deliberately': 323,\n",
       " 'black': 324,\n",
       " 'church': 325,\n",
       " 'north': 326,\n",
       " 'carolinaåêablaze': 327,\n",
       " 'noches': 328,\n",
       " 'el-bestia': 329,\n",
       " ':': 330,\n",
       " 'happy': 331,\n",
       " 'teammates': 332,\n",
       " 'training': 333,\n",
       " 'hard': 334,\n",
       " 'goodnight': 335,\n",
       " 'gunners??????': 336,\n",
       " '': 337,\n",
       " '#kurds': 338,\n",
       " 'trampling': 339,\n",
       " 'turkmen': 340,\n",
       " 'later': 341,\n",
       " 'while': 342,\n",
       " 'others': 343,\n",
       " 'vandalized': 344,\n",
       " 'offices': 345,\n",
       " 'front': 346,\n",
       " '#diyala': 347,\n",
       " 'truck': 348,\n",
       " 'r21': 349,\n",
       " 'voortrekker': 350,\n",
       " 'ave': 351,\n",
       " 'tambo': 352,\n",
       " 'intl': 353,\n",
       " 'cargo': 354,\n",
       " 'section': 355,\n",
       " 'hearts': 356,\n",
       " 'city': 357,\n",
       " 'gift': 358,\n",
       " 'skyline': 359,\n",
       " 'kiss': 360,\n",
       " 'upon': 361,\n",
       " 'lips': 362,\n",
       " '\\x89û_': 363,\n",
       " 'tonight': 364,\n",
       " 'los': 365,\n",
       " 'angeles': 366,\n",
       " 'expecting': 367,\n",
       " 'ig': 368,\n",
       " 'fb': 369,\n",
       " 'be': 370,\n",
       " 'filled': 371,\n",
       " 'sunset': 372,\n",
       " 'shots': 373,\n",
       " 'if': 374,\n",
       " 'peeps': 375,\n",
       " '#california': 376,\n",
       " '#climate': 377,\n",
       " '#energy': 378,\n",
       " 'revel': 379,\n",
       " 'yours': 380,\n",
       " 'wmv': 381,\n",
       " 'videos': 382,\n",
       " 'means': 383,\n",
       " 'mac': 384,\n",
       " 'farewell': 385,\n",
       " 'en': 386,\n",
       " 'route': 387,\n",
       " 'dvd:': 388,\n",
       " 'gtxrwm': 389,\n",
       " 'progressive': 390,\n",
       " 'greetings!in': 391,\n",
       " 'about': 392,\n",
       " 'month': 393,\n",
       " 'students': 394,\n",
       " 'would': 395,\n",
       " 'their': 396,\n",
       " 'pens': 397,\n",
       " 'torch': 398,\n",
       " 'publications': 399,\n",
       " 'rene': 400,\n",
       " '&amp;': 401,\n",
       " 'jacinta': 402,\n",
       " 'secret': 403,\n",
       " '2k13': 404,\n",
       " '(fallen': 405,\n",
       " 'skies': 406,\n",
       " 'edit)': 407,\n",
       " 'mar': 408,\n",
       " '30': 409,\n",
       " '2013': 410,\n",
       " 'steve': 411,\n",
       " 'fires': 412,\n",
       " 'here': 413,\n",
       " 'something': 414,\n",
       " 'else!': 415,\n",
       " 'tinderbox': 416,\n",
       " 'clown': 417,\n",
       " 'hood': 418,\n",
       " '#nowplaying:': 419,\n",
       " 'ian': 420,\n",
       " 'buff': 421,\n",
       " 'magnitude': 422,\n",
       " '#edm': 423,\n",
       " 'huge': 424,\n",
       " 'does': 425,\n",
       " 'talk': 426,\n",
       " 'go': 427,\n",
       " 'until?': 428,\n",
       " \"don't\": 429,\n",
       " 'make': 430,\n",
       " 'work': 431,\n",
       " 'kids': 432,\n",
       " 'cuz': 433,\n",
       " 'bicycle': 434,\n",
       " 'accident': 435,\n",
       " 'split': 436,\n",
       " 'testicles': 437,\n",
       " 'impossible': 438,\n",
       " 'michael': 439,\n",
       " 'father': 440,\n",
       " 'i-24': 441,\n",
       " 'w': 442,\n",
       " '#nashvilletraffic': 443,\n",
       " 'traffic': 444,\n",
       " 'moving': 445,\n",
       " '8m': 446,\n",
       " 'slower': 447,\n",
       " 'than': 448,\n",
       " 'usual': 449,\n",
       " 'center': 450,\n",
       " 'lane': 451,\n",
       " 'blocked': 452,\n",
       " '#santaclara': 453,\n",
       " 'us-101': 454,\n",
       " 'nb': 455,\n",
       " 'great': 456,\n",
       " 'america': 457,\n",
       " 'pkwy': 458,\n",
       " '#bayarea': 459,\n",
       " '#traffic': 460,\n",
       " '#personalinjury': 461,\n",
       " 'summer?': 462,\n",
       " 'read': 463,\n",
       " 'advice': 464,\n",
       " '#solicitor': 465,\n",
       " 'help': 466,\n",
       " '#otleyhour': 467,\n",
       " '#stlouis': 468,\n",
       " '#caraccidentlawyer': 469,\n",
       " 'speeding': 470,\n",
       " 'among': 471,\n",
       " 'teen': 472,\n",
       " 'accidents': 473,\n",
       " 'tee\\x89û_': 474,\n",
       " 'reported': 475,\n",
       " 'motor': 476,\n",
       " 'vehicle': 477,\n",
       " 'curry': 478,\n",
       " 'herman': 479,\n",
       " 'rd': 480,\n",
       " 'stephenson': 481,\n",
       " 'involving': 482,\n",
       " 'overturned': 483,\n",
       " 'please': 484,\n",
       " 'use': 485,\n",
       " 'bigrigradio': 486,\n",
       " 'awareness': 487,\n",
       " 'i-77': 488,\n",
       " 'mile': 489,\n",
       " 'marker': 490,\n",
       " '31': 491,\n",
       " 'mooresville': 492,\n",
       " 'iredell': 493,\n",
       " 'ramp': 494,\n",
       " '8/6': 495,\n",
       " '1:18': 496,\n",
       " 'pm': 497,\n",
       " 'rt': 498,\n",
       " 'sleeping': 499,\n",
       " 'pills': 500,\n",
       " 'double': 501,\n",
       " 'risk': 502,\n",
       " 'knew': 503,\n",
       " 'gon': 504,\n",
       " 'happen': 505,\n",
       " 'n': 506,\n",
       " 'cabrillo': 507,\n",
       " 'hwy/magellan': 508,\n",
       " 'av': 509,\n",
       " 'mir': 510,\n",
       " '(08/06/15': 511,\n",
       " '11:03:58)': 512,\n",
       " '40': 513,\n",
       " 'congestion': 514,\n",
       " 'pastor': 515,\n",
       " 'scene': 516,\n",
       " 'accidentwho': 517,\n",
       " 'owner': 518,\n",
       " 'range': 519,\n",
       " 'rover': 520,\n",
       " '?': 521,\n",
       " 'mom:': 522,\n",
       " \"didn't\": 523,\n",
       " 'wished': 524,\n",
       " 'me:': 525,\n",
       " 'why': 526,\n",
       " \"that?'mom:\": 527,\n",
       " 'there': 528,\n",
       " 'some': 529,\n",
       " 'spilt': 530,\n",
       " 'mayonnaise': 531,\n",
       " 'over': 532,\n",
       " '??????': 533,\n",
       " 'horrible': 534,\n",
       " 'past': 535,\n",
       " 'sunday': 536,\n",
       " 'finally': 537,\n",
       " 'able': 538,\n",
       " 'thank': 539,\n",
       " 'god??': 540,\n",
       " 'pissed': 541,\n",
       " 'donnie': 542,\n",
       " 'when': 543,\n",
       " 'tell': 544,\n",
       " 'him': 545,\n",
       " 'another': 546,\n",
       " 'accident??': 547,\n",
       " '#truckcrash': 548,\n",
       " 'overturns': 549,\n",
       " '#fortworth': 550,\n",
       " 'interstate': 551,\n",
       " 'click': 552,\n",
       " \"you've\": 553,\n",
       " 'been': 554,\n",
       " 'crash&gt;': 555,\n",
       " '#ashville': 556,\n",
       " '23': 557,\n",
       " 'sb': 558,\n",
       " 'sr': 559,\n",
       " '752': 560,\n",
       " 'carolina': 561,\n",
       " 'accident:': 562,\n",
       " 'motorcyclist': 563,\n",
       " 'dies': 564,\n",
       " 'i-540': 565,\n",
       " 'crossed': 566,\n",
       " 'median:': 567,\n",
       " 'motorcycle': 568,\n",
       " 'rider': 569,\n",
       " 'traveling': 570,\n",
       " 'fyi': 571,\n",
       " 'cad:fyi:': 572,\n",
       " ';accident': 573,\n",
       " 'property': 574,\n",
       " 'damage;nhs;999': 575,\n",
       " 'piner': 576,\n",
       " 'rd/horndale': 577,\n",
       " 'dr': 578,\n",
       " 'naayf:': 579,\n",
       " 'turning': 580,\n",
       " 'onto': 581,\n",
       " 'chandanee': 582,\n",
       " 'magu': 583,\n",
       " 'mma': 584,\n",
       " 'taxi': 585,\n",
       " 'rammed': 586,\n",
       " 'halfway': 587,\n",
       " 'turned': 588,\n",
       " 'everyone': 589,\n",
       " 'conf\\x89û_': 590,\n",
       " 'left': 591,\n",
       " '#manchester': 592,\n",
       " '293': 593,\n",
       " 'eddy': 594,\n",
       " 'stop': 595,\n",
       " 'back': 596,\n",
       " 'nh-3a': 597,\n",
       " 'delay': 598,\n",
       " '4': 599,\n",
       " 'mins': 600,\n",
       " 'damage;': 601,\n",
       " 'damage;wpd;1600': 602,\n",
       " 's': 603,\n",
       " '17th': 604,\n",
       " '8/6/2015:09': 605,\n",
       " 'pm:': 606,\n",
       " 'injury': 607,\n",
       " '2781': 608,\n",
       " 'willis': 609,\n",
       " 'foreman': 610,\n",
       " 'aashiqui': 611,\n",
       " 'actress': 612,\n",
       " 'anu': 613,\n",
       " 'aggarwal': 614,\n",
       " 'her': 615,\n",
       " 'near-fatal': 616,\n",
       " 'suffield': 617,\n",
       " 'alberta': 618,\n",
       " '9': 619,\n",
       " 'backup': 620,\n",
       " 'southaccident': 621,\n",
       " 'blocking': 622,\n",
       " 'right': 623,\n",
       " '2': 624,\n",
       " 'lanes': 625,\n",
       " 'exit': 626,\n",
       " 'langtree': 627,\n",
       " 'rdconsider': 628,\n",
       " 'nc': 629,\n",
       " '115': 630,\n",
       " '150': 631,\n",
       " '16': 632,\n",
       " 'alternate': 633,\n",
       " 'changed': 634,\n",
       " 'life?': 635,\n",
       " 'determine': 636,\n",
       " 'options': 637,\n",
       " 'financially': 638,\n",
       " 'support': 639,\n",
       " 'plans': 640,\n",
       " 'on-going': 641,\n",
       " 'treatment': 642,\n",
       " '#breaking:': 643,\n",
       " 'deadly': 644,\n",
       " 'happened': 645,\n",
       " '#hagerstown': 646,\n",
       " 'today': 647,\n",
       " \"i'll\": 648,\n",
       " 'more': 649,\n",
       " 'details': 650,\n",
       " '5': 651,\n",
       " '#whag': 652,\n",
       " 'were': 653,\n",
       " 'marinading': 654,\n",
       " 'accident?': 655,\n",
       " 'only': 656,\n",
       " 'even': 657,\n",
       " 'fucking': 658,\n",
       " 'mfs': 659,\n",
       " 'drive': 660,\n",
       " '#bahrain': 661,\n",
       " 'previously': 662,\n",
       " 'road': 663,\n",
       " 'killed': 664,\n",
       " 'explosion': 665,\n",
       " 'still': 666,\n",
       " 'heard': 667,\n",
       " 'leaders': 668,\n",
       " 'kenya': 669,\n",
       " 'forward': 670,\n",
       " 'comment': 671,\n",
       " 'issue': 672,\n",
       " 'disciplinary': 673,\n",
       " 'measures#arrestpastornganga': 674,\n",
       " 'scuf': 675,\n",
       " 'ps': 676,\n",
       " 'game': 677,\n",
       " 'cya': 678,\n",
       " 'who': 679,\n",
       " 'himself': 680,\n",
       " 'further': 681,\n",
       " 'once': 682,\n",
       " 'effort': 683,\n",
       " 'gets': 684,\n",
       " 'painful': 685,\n",
       " 'win': 686,\n",
       " 'roger': 687,\n",
       " 'bannister': 688,\n",
       " '320': 689,\n",
       " '[ir]': 690,\n",
       " 'icemoon': 691,\n",
       " '[aftershock]': 692,\n",
       " '|': 693,\n",
       " '#dubstep': 694,\n",
       " '#trapmusic': 695,\n",
       " '#dnb': 696,\n",
       " '#dance': 697,\n",
       " '#ices\\x89û_': 698,\n",
       " 'victory': 699,\n",
       " 'bargain': 700,\n",
       " 'basement': 701,\n",
       " 'prices': 702,\n",
       " 'dwight': 703,\n",
       " 'david': 704,\n",
       " 'eisenhower': 705,\n",
       " 'nobody': 706,\n",
       " 'remembers': 707,\n",
       " 'came': 708,\n",
       " 'charles': 709,\n",
       " 'schulz': 710,\n",
       " 'im': 711,\n",
       " 'speaking': 712,\n",
       " 'someone': 713,\n",
       " 'using': 714,\n",
       " 'xb1': 715,\n",
       " 'most': 716,\n",
       " 'end': 717,\n",
       " 'also': 718,\n",
       " 'harder': 719,\n",
       " 'conflict': 720,\n",
       " 'glorious': 721,\n",
       " 'triumph': 722,\n",
       " 'thomas': 723,\n",
       " 'paine': 724,\n",
       " '#growingupspoiled': 725,\n",
       " 'going': 726,\n",
       " 'clay': 727,\n",
       " 'pigeon': 728,\n",
       " 'shooting': 729,\n",
       " 'because': 730,\n",
       " \"aftershock'\": 731,\n",
       " 'guess': 732,\n",
       " 'one': 733,\n",
       " 'actually': 734,\n",
       " 'wants': 735,\n",
       " 'any': 736,\n",
       " 'free': 737,\n",
       " 'aftershock': 738,\n",
       " 'tc': 739,\n",
       " 'terrifying': 740,\n",
       " 'best': 741,\n",
       " 'roller': 742,\n",
       " 'coaster': 743,\n",
       " 'ever': 744,\n",
       " 'disclaimer': 745,\n",
       " 'very': 746,\n",
       " 'few': 747,\n",
       " 'seeing': 748,\n",
       " 'issues': 749,\n",
       " '#wisdomwed': 750,\n",
       " 'bonus': 751,\n",
       " 'minute': 752,\n",
       " 'daily': 753,\n",
       " 'habits': 754,\n",
       " 'could': 755,\n",
       " 'really': 756,\n",
       " 'improve': 757,\n",
       " 'many': 758,\n",
       " 'already': 759,\n",
       " 'do?': 760,\n",
       " '#lifehacks': 761,\n",
       " 'aftershock:': 762,\n",
       " 'protect': 763,\n",
       " 'yourself': 764,\n",
       " 'profit': 765,\n",
       " 'global': 766,\n",
       " 'financial': 767,\n",
       " 'meltdown': 768,\n",
       " 'wiedemer': 769,\n",
       " 'http': 770,\n",
       " 'moment': 771,\n",
       " 'scary': 772,\n",
       " 'guy': 773,\n",
       " 'behind': 774,\n",
       " 'screaming': 775,\n",
       " 'bloody': 776,\n",
       " 'murder': 777,\n",
       " '#silverwood': 778,\n",
       " '#aftershock': 779,\n",
       " '\\x89ã¢': 780,\n",
       " '(2010)': 781,\n",
       " 'full\\x89ã¢': 782,\n",
       " 'streaming': 783,\n",
       " 'youtube': 784,\n",
       " '&gt;&gt;': 785,\n",
       " '$15': 786,\n",
       " 'book': 787,\n",
       " 'sometimes': 788,\n",
       " 'face': 789,\n",
       " 'difficulties': 790,\n",
       " 'doing': 791,\n",
       " 'wrong': 792,\n",
       " 'joel': 793,\n",
       " 'osteen': 794,\n",
       " 'thing': 795,\n",
       " 'stands': 796,\n",
       " 'between': 797,\n",
       " 'dream': 798,\n",
       " 'belief': 799,\n",
       " 'possible': 800,\n",
       " 'brown': 801,\n",
       " 'praise': 802,\n",
       " 'god': 803,\n",
       " 'ministry': 804,\n",
       " 'tells': 805,\n",
       " '#now': 806,\n",
       " '#wdyouth': 807,\n",
       " '#biblestudy': 808,\n",
       " 'remembering': 809,\n",
       " 'die': 810,\n",
       " 'way': 811,\n",
       " 'avoid': 812,\n",
       " 'trap': 813,\n",
       " 'thinking': 814,\n",
       " 'lose': 815,\n",
       " 'jobs': 816,\n",
       " 'tried': 817,\n",
       " 'orange': 818,\n",
       " 'never': 819,\n",
       " 'same': 820,\n",
       " 'bb': 821,\n",
       " 'kick': 822,\n",
       " 'off': 823,\n",
       " 'want': 824,\n",
       " 'making': 825,\n",
       " 'say': 826,\n",
       " 'cannot': 827,\n",
       " 'done': 828,\n",
       " 'should': 829,\n",
       " 'interrupt': 830,\n",
       " 'those': 831,\n",
       " 'george': 832,\n",
       " 'bernard': 833,\n",
       " 'shaw': 834,\n",
       " 'oyster': 835,\n",
       " 'shell': 836,\n",
       " 'andrew': 837,\n",
       " 'carnegie': 838,\n",
       " 'anyone': 839,\n",
       " 'need': 840,\n",
       " 'p/u': 841,\n",
       " 'tonight?': 842,\n",
       " 'play': 843,\n",
       " 'hybrid': 844,\n",
       " 'slayer': 845,\n",
       " 'ps4': 846,\n",
       " 'eu': 847,\n",
       " 'hmu': 848,\n",
       " 'experts': 849,\n",
       " 'france': 850,\n",
       " 'begin': 851,\n",
       " 'examining': 852,\n",
       " 'airplane': 853,\n",
       " 'debris': 854,\n",
       " 'found': 855,\n",
       " 'reunion': 856,\n",
       " 'island:': 857,\n",
       " 'french': 858,\n",
       " 'air': 859,\n",
       " 'o': 860,\n",
       " '#news': 861,\n",
       " 'strict': 862,\n",
       " 'liability': 863,\n",
       " 'context': 864,\n",
       " 'pilot': 865,\n",
       " 'error': 866,\n",
       " 'common': 867,\n",
       " 'component': 868,\n",
       " 'aviation': 869,\n",
       " 'cr': 870,\n",
       " 'lifetime': 871,\n",
       " 'odds': 872,\n",
       " 'dying': 873,\n",
       " '1': 874,\n",
       " '8015': 875,\n",
       " 'wedn': 876,\n",
       " 'awwww': 877,\n",
       " \"they're\": 878,\n",
       " 'cuties': 879,\n",
       " 'good': 880,\n",
       " 'job!': 881,\n",
       " 'family': 882,\n",
       " 'members': 883,\n",
       " 'osama': 884,\n",
       " 'bin': 885,\n",
       " 'laden': 886,\n",
       " 'ironic': 887,\n",
       " 'mhmmm': 888,\n",
       " 'gov': 889,\n",
       " 'suspect': 890,\n",
       " 'goes': 891,\n",
       " 'engine': 892,\n",
       " 'via': 893,\n",
       " 'wings': 894,\n",
       " '(29-07-2015)': 895,\n",
       " 'cessna': 896,\n",
       " 'ocampo': 897,\n",
       " 'coahuila': 898,\n",
       " 'mexico': 899,\n",
       " 'july': 900,\n",
       " '29': 901,\n",
       " '2015': 902,\n",
       " 'four': 903,\n",
       " 'men': 904,\n",
       " 'including': 905,\n",
       " 'state': 906,\n",
       " 'government': 907,\n",
       " 'official': 908,\n",
       " '#horrible': 909,\n",
       " '#accident': 910,\n",
       " '#watchthevideo': 911,\n",
       " 'island': 912,\n",
       " 'wednesday\\x89û_': 913,\n",
       " 'wednesday': 914,\n",
       " 'began': 915,\n",
       " 't': 916,\n",
       " '#kca': 917,\n",
       " '#votejkt48id': 918,\n",
       " 'mbataweel:': 919,\n",
       " '#rip': 920,\n",
       " '#binladen': 921,\n",
       " \"airplane's\": 922,\n",
       " 'almost': 923,\n",
       " 'coworker': 924,\n",
       " 'nudes': 925,\n",
       " 'mode': 926,\n",
       " 'might': 927,\n",
       " 'wreck!': 928,\n",
       " 'politics': 929,\n",
       " '#mlb': 930,\n",
       " 'unbelievably': 931,\n",
       " 'insane#man': 932,\n",
       " '#airport': 933,\n",
       " '#airplane': 934,\n",
       " '#aircraft': 935,\n",
       " '#aeroplane': 936,\n",
       " '#runway': 937,\n",
       " '#freaky\\x89û_': 938,\n",
       " 'airplaneåê(29-07-2015)': 939,\n",
       " 'usama': 940,\n",
       " 'ladins': 941,\n",
       " 'naturally': 942,\n",
       " 'plane': 943,\n",
       " 'festival': 944,\n",
       " '#crash': 945,\n",
       " '#pilot': 946,\n",
       " '#death': 947,\n",
       " '#carfest': 948,\n",
       " 'dtn': 949,\n",
       " 'brazil:': 950,\n",
       " 'exp': 951,\n",
       " '\\x89ûïairplane\\x89û\\x9d': 952,\n",
       " '29-07-2015': 953,\n",
       " 'wtf': 954,\n",
       " 'can\\x89ûªt': 955,\n",
       " 'believe': 956,\n",
       " 'eyes': 957,\n",
       " '+': 958,\n",
       " 'nicole': 959,\n",
       " 'fletcher': 960,\n",
       " 'victim': 961,\n",
       " 'crashed': 962,\n",
       " 'times': 963,\n",
       " 'ago': 964,\n",
       " 'little': 965,\n",
       " 'bit': 966,\n",
       " 'trauma': 967,\n",
       " 'although': 968,\n",
       " \"she's\": 969,\n",
       " 'omg': 970,\n",
       " '#omg!': 971,\n",
       " 'bro#airplane': 972,\n",
       " '#jetengine': 973,\n",
       " '#turbojet': 974,\n",
       " '#boing': 975,\n",
       " '#g90': 976,\n",
       " 'phone': 977,\n",
       " 'looks': 978,\n",
       " 'ship': 979,\n",
       " 'terrible': 980,\n",
       " 'statistically': 981,\n",
       " 'cop': 982,\n",
       " 'crashes': 983,\n",
       " 'house': 984,\n",
       " 'colombia': 985,\n",
       " '12': 986,\n",
       " 'drone': 987,\n",
       " 'cause': 988,\n",
       " 'pilots': 989,\n",
       " 'worried': 990,\n",
       " 'drones': 991,\n",
       " 'esp': 992,\n",
       " 'close': 993,\n",
       " 'vicinity': 994,\n",
       " 'airports': 995,\n",
       " '#': 996,\n",
       " 'early': 997,\n",
       " 'wake': 998,\n",
       " 'call': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {}\n",
    "id_to_word = {}\n",
    "for i,word in enumerate(vocab_dict.keys()):\n",
    "    word_dict[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_pre = data['text']\n",
    "y_data_pre = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    the out of control wild fires in california ev...\n",
       "7610             m194 [01:04 utc]?5km s of volcano hawaii\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest: more homes razed by northern calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = X_data_pre\n",
    "y_data = y_data_pre\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
       "1                            [13, 14, 15, 16, 17, 18, 19]\n",
       "2       [12, 20, 21, 22, 23, 24, 25, 2, 26, 27, 28, 29...\n",
       "3                        [36, 37, 38, 39, 32, 34, 24, 40]\n",
       "4       [41, 42, 43, 6, 44, 45, 46, 47, 48, 49, 45, 39...\n",
       "                              ...                        \n",
       "7608    [224, 4221, 4602, 4603, 52, 4561, 1732, 51, 46...\n",
       "7609    [3, 192, 5, 1873, 1319, 412, 24, 40, 657, 24, ...\n",
       "7610             [9937, 18106, 18107, 603, 5, 9939, 9940]\n",
       "7611    [233, 4270, 1392, 88, 6252, 6234, 264, 52, 140...\n",
       "7612    [3, 13788, 649, 4605, 15386, 28, 1904, 40, 290...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert each tweet into the corresponding integer representation and make the training set\n",
    "\n",
    "# print(X_data_new.shape)\n",
    "def entry_convert(entry):\n",
    "    # print(type(entry))\n",
    "    entry_split = entry.split()\n",
    "    # print(entry_split)\n",
    "    for i,word in enumerate(entry_split):\n",
    "        entry_split[i] = word_dict[word]\n",
    "    return np.array(entry_split, dtype = np.int64)\n",
    "X_data = X_data.apply(lambda v: entry_convert(v))\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = torch.tensor(y_data.astype(np.int64))\n",
    "\n",
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = torch.zeros(y_data.shape[0], 2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data[range(len(y_data)),y_data] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_data = y_data.clone().view(1,-1)\n",
    "# y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1D():\n",
    "    \"\"\" This class creates a Batch Normalization layer\"\"\"\n",
    "    def __init__(self, dim, epsilon = 1e-5, momentum = 0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Initializing learnable paramters\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "        # Buffers trained with a momentum update\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_variance = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        if X.ndim == 2:\n",
    "            dim = 0\n",
    "        if X.ndim == 3:\n",
    "            dim = (0,1)\n",
    "        # Calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = X.mean(dim, keepdim=True)\n",
    "            xvar = X.var(dim,keepdim = True, unbiased = True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_variance\n",
    "\n",
    "        xhat = (X-xmean)/ torch.sqrt(xvar+self.epsilon)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # Now update the running mean buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_variance = (1-self.momentum)* self.running_variance + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisasterModel(\n",
       "  (embedding): Embedding(18874, 20)\n",
       "  (l1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (l2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (l3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (l4): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DisasterModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(word_dict), 20)\n",
    "        self.l1 = nn.Linear(20,128)\n",
    "        self.bn1 = BatchNorm1D(128)\n",
    "        # self.tanh1 = nn.Tanh()\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.bn2 = BatchNorm1D(128)\n",
    "        # self.tanh2 = nn.Tanh()\n",
    "        self.l3 = nn.Linear(128,64)\n",
    "        self.bn3 = BatchNorm1D(64)\n",
    "        # self.tanh3 = nn.Tanh()\n",
    "        self.l4 = nn.Linear(64,2)\n",
    "    \n",
    "    def forward(self, xix):\n",
    "        x = torch.zeros(32,20)\n",
    "        for i in range(len(xix)):\n",
    "            # print(xix[i])\n",
    "            ix_entry = torch.from_numpy(xix[i])\n",
    "            # print(ix_entry)\n",
    "            embedding = torch.sum(self.embedding.weight[ix_entry], dim = 0, keepdim = True)\n",
    "            # print(embedding.shape)\n",
    "            x[i] = embedding\n",
    "\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.tanh(self.bn1(self.l1(x)))\n",
    "        # x = torch.tanh(self.bn2(self.l2(x)))\n",
    "        x = torch.tanh(self.bn3(self.l3(x)))\n",
    "        self.out = self.l4(x)\n",
    "        return self.out\n",
    "\n",
    "model = DisasterModel()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3834,  1.0620,  0.4433,  1.2012],\n",
       "        [ 0.9260,  0.5059,  0.5502,  1.5115],\n",
       "        [ 0.5053, -1.1794,  0.4182,  0.6096]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b  = torch.randn((5,4))\n",
    "x = torch.tensor([1,2,3], dtype=torch.int64)\n",
    "b[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405066"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = sum([x.nelement() for x in model.parameters()])\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_data = X_data[:int(0.9*len(X_data))]\n",
    "test_x_data = X_data[int(0.9*len(X_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313444018363953\n",
      "0.261134535074234\n",
      "0.34552183747291565\n",
      "0.27206602692604065\n",
      "0.2340467870235443\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for i in range((len(train_x_data.values))//32):\n",
    "        batch_data = X_data.values[i*32:(i+1)*32]\n",
    "        batch_labels = target_data[i*32:(i+1)*32].view(32,-1,)\n",
    "\n",
    "        # print(batch_labels.shape)\n",
    "        logits = model(batch_data)\n",
    "        loss = F.cross_entropy(logits, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range((len(test_x_data.values))//32):\n",
    "        batch_data = X_data.values[i*32:(i+1)*32]\n",
    "        batch_labels = target_data[i*32:(i+1)*32].view(32,-1,)\n",
    "\n",
    "        # print(batch_labels.shape)\n",
    "        logits = model(batch_data)\n",
    "        # logits = torch.exp(logits)\n",
    "        # print(logits)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, batch_labels)\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        print(loss)\n",
    "        break\n",
    "        \n",
    "    # if epoch % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dev_data, dev_labels = next(iter(dev_dataloader))\n",
    "\n",
    "    dev_logits = model(dev_data)\n",
    "\n",
    "    dev_probs = torch.softmax(dev_logits, dim = 1)\n",
    "    print(len(dev_probs))\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6707e-04, 9.9983e-01],\n",
      "        [7.6953e-04, 9.9923e-01],\n",
      "        [1.6092e-04, 9.9984e-01],\n",
      "        [1.4233e-04, 9.9986e-01],\n",
      "        [7.6953e-04, 9.9923e-01],\n",
      "        [4.4240e-01, 5.5760e-01],\n",
      "        [5.4918e-06, 9.9999e-01],\n",
      "        [3.9603e-04, 9.9960e-01],\n",
      "        [2.0885e-03, 9.9791e-01],\n",
      "        [1.5038e-01, 8.4962e-01],\n",
      "        [4.2081e-05, 9.9996e-01],\n",
      "        [6.5492e-03, 9.9345e-01],\n",
      "        [1.9650e-03, 9.9804e-01],\n",
      "        [7.6953e-04, 9.9923e-01],\n",
      "        [9.7717e-06, 9.9999e-01],\n",
      "        [5.8694e-03, 9.9413e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(dev_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dev_probs.max(dim = 1).indices == dev_labels).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
