{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11 : Training a Classification model on our data\n",
    "1. Using the pytorch dataloader to load the data.\n",
    "2. Using a classification model to classify the data.\n",
    "3. Setting up a basic skeleton for our project app.\n",
    "4. Calculating, logging and displaying of various different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do two things in this chapter. We are going to set a basic training loop and a basic model which clssifies our data. We will use the `Ct` and `LunaDataset` classes which we have defined in chapter 10 to feed into a dataloader which feeds the data into our model through training and validation data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by the next step after cleaning our data i.e the classification part. We would build a clssifier which would classify the candidates as nodules or non-nodules first. Defining the malignancy of the tumor comes in later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic structure that we are going to implement is as follows\n",
    "1. Initialize our model and data loading.\n",
    "2. Loop over the epochs\n",
    "    1. Loop over each batch of the data.\n",
    "    2. Calculate the error between the prediction and ground truth of the batch.\n",
    "    3. The background worker processes load the next batch of the data.\n",
    "    4. Now backpropagate to get the gradients.\n",
    "    5. Update the weights.\n",
    "    6. Record the metrics of the training into a seperate data structure.\n",
    "    7. Load the validation data, classify it and get the error between the prediction and the ground truth on the validation data.\n",
    "    8. The background worker processes load the validation data in the background.\n",
    "    9. Record the metrics of the validation step in a data structure.\n",
    "    10. Print out the progress and performance information about this epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approch to this part of the project will be more structured than the training loops that we implemented in part 1. We will have our main training application have more well-contained function and for things like dataset loading we will have them as seperate python modules.\\\n",
    "\\\n",
    "When working on your own projects make sure that the structure of the project is according to the complexity of the project. Too little structure and you cannot weed our errors and run training effectively while too much structure can take your mind off from devising the solution to the problem and keep it focussed only on setting up on the infrastructure, which can also be a great procrastination tactic.\\\n",
    "\\\n",
    "We would also focus on the correct logging and collection of metrics pertaining to the task at hand as the training progresses so that we can determine the effect of changes on training. We would lay the infrastructure of those metrics in this chapter and we would use them in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main entry point of our application\n",
    "1. The first and the foremost thing that we will do is that we will set up our application so that we can use the command line to run the script and parse the arguments in the command line. This will make it easy to run it in a wide variety of environments.\n",
    "2. Our application functinality will be implemented via a class so that we can istantiate and pass it aroung for testing, debugging and running. We can invoke the application without spinning up a second os-level process(We won't do unit testing but the structure that we create will be useful if unit testing is required).\n",
    "3. One way to take advantage of being able to invoke our training by either function call or OS level process is to wrap up our function invocations into a jupyter notebook so that it can be called from a command line or a browser.(OS level process means to run the script from the CLI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workings of the application\n",
    "1. lets get some semi-standard boilerplate code out of the way. The first part invokes the if statement which creates the application object and invokes the main statement of that object.\n",
    "2.  Next we would create the skeleton of the LunaTrainingApp with the `__init__` and the `main` methods.\n",
    "3. As we are trying to parse arguments from the CLI we would need the standard `argeparse` library in the apps `__init__` method. We can also pass custom arguments to our init method should we wish to do so.\n",
    "4. The main method will be the primary entry point to our application.\n",
    "5. Parsing arguments in the init method allows us to configure the application seperately from invoking it.\n",
    "\n",
    "    ##### Pretraining setup and initialization\n",
    "    Before we can begin training our model some of the initialization needs to happen.\n",
    "    1. First we need to initialize our `dataset` and `Dataloader` instances. The class LunaDataset will provide randomised samples of data which will be loaded per epoch and our dataloader will perform the work of loading the data and providing it to our application.\n",
    "    2. This will be implemented by the training and the validation scripts in our code.\n",
    "\n",
    "    ##### Initializing the model and the optimizer\n",
    "    For this chapter we will take the inner workings of the model as a blackbox. We will improve on the model architecture in the next chapter.\n",
    "\n",
    "    ##### Care and feeding of the dataloaders.\n",
    "    1. The LunaDataset class that we built earlier transfroms the data from real-world to the form of tensors expected by pytorch.\n",
    "    2. For eg. torch.nn.conv3D expects an input of `N x C x D x H x W` which is quite different from the native 3D dataout CT provides.\n",
    "    3. CT scans only have a single channel, but other types of data have more than one channel(color images have 3 channels but astronimical data will have more than 3 upto 12 channels)\n",
    "    4. We will not have to do the batching of the data. The pytorch dataloader handles the batching of the data for us.\n",
    "    5. In addition to batching, dataloaders can also provide parallel loading of the data. Just add the number of worker processes in  `num_workers = ` and the parallelization is handled behind the scenes.\n",
    "    6. Using data loading routines of pytorch can help speed up many projects, because we can overlap data loading and processing with GPU calculations.\n",
    "\n",
    "    ##### Our first pass Neural Network Model design\n",
    "    1. We can use a 3D CNN for recognition of our tumors.\n",
    "    2. We will base our network design on the model that we used in Chapter 8. Although there we used a 2D model here we will modify it to use a 3D model.\n",
    "    3. The model consists of \n",
    "        1. A batchNormalization layer. [TAIL]\n",
    "        2. Four repeated blocks of convolution operation.[BACKBONE]\n",
    "        3. A Linear layer followed by a softmax layer. [Head]\n",
    "\n",
    "    \n",
    "    4. ##### The Core Convolutions\n",
    "        1. CNN's always have some of format of the core 3 parts\n",
    "            1. A tail which is responsible for consuming the inputs and converting it into the form expected by the backbone. Here we have used a simple batchnormalization though often the tails contain Convolutional layers also which are used to downsample the size of the image. Here we do not need them because our size is already very small.\n",
    "            2. A backbone which performs the convolution operations which is the bulk of the layers. They are usually arranged in a series of blocks. Each block has the same or similar set of layers though often the blocks have different expected input size and the different number of filters. \n",
    "            3. We will use a block that contains two 3x3 convolutions followed by an activation and max-pooling layer.\n",
    "            4. Finally we define a head which has a linear layer followed by a softmax layer.\n",
    "\n",
    "    ##### The convolution Block\n",
    "    1. We are defining a convolution block called as LunaBlock which when stacked 4 times would make up the backbone of our Model.\n",
    "    2. This convolution block consists of:\n",
    "        1. `Convolution Layer 1`\n",
    "        2. `Relu Non-linearity`\n",
    "        3. `Convolution Layer 2`\n",
    "        4. `Relu Non-linearity`\n",
    "        5. `Max Pooling Layer`\n",
    "    3. What the above architecture is doing is that it is stacking two convolution layers side one after another to increase the receptive field so that it has the receptive field of a larger kernel_size(5x5) while having lesser parameters and performing lesser number of calculations than a 5x5 kernel. Lets suppose we have a 6x6 image, now after othe first convolution it reduces to 4x4 and after the second convolution it reduces to 2x2 (relu does not reduce the image size) so both the convolutions combined have a receptive field of 6x6 but doing one 5x5 convolution also produces the same output size with a receptive field of 6x6 but the number of calculations increase.\n",
    "    4. Though we have taken the example of the receptive field shrinking we have actualy used padded convolutions. So the size of the image will be the same.\n",
    "    \n",
    "    ##### The full model\n",
    "    1. We would stack 4 of the convolution blocks along with a batchnorm tail and a Linear + softmax head to form the full model.\n",
    "    2. The way the backbone is structured is that due to the maxpool layer at the end of each block it takes the original input image and halves it in dimensions. So by the time the image reaches the end of the backbone it has been reduced to 2^4 = 16 times. So a 32x48x48 image gets reduced to 2x3x3 image.\n",
    "    3. Finally our last layer is a `nn.Softmax` layer. This is useful because for single label clssification because it nicely modifies the input between 0 and 1 and it is relatively insensitive to the absolute range of the inputs(only relative values of the inputs matter). It also allows our model to express the degree on uncertainity it has in the answer.\n",
    "    4. After the convolution backbone we have the image size as `2 x 3 x 3 x 64` but our Linear layer expects the images to be in the format of a batch of 1D vecotrs. So we will need to convert it to that shape in the forward pass.\n",
    "    5. The head portion will be similar in a wide variety of problems that use convolutions and produces classification, regression or any other non-image output.\n",
    "    6. For the return values of the forward function we would return the `Logits (Raw values produced by the model before they are fed into softmax and probabilities are calculated)` and we would also return the `softmax outputs`. `We will use the logits when we calculate the nn.CrossEntropyLoss during training and we will use the probabilities when we want to actually classify the samples.` This kind of slight difference between what is used for training and what is used for production is fairly common, especially when the output of the network is a stateless function like softmax. This approach can also provide numerical benefits also. Propogating gradients from an exponential function using 32 bit floats can be a bit problematic.\n",
    "\n",
    "    ##### Initialization\n",
    "    1. Lets talk about the initialization of the weights to a predefined value. If we imagine a case where the weights are initialized to 1 then the weights would get substantially large when the activations move from initial layers to later layers leading to larger and larger weights. Conversely the weights would get smaller and smaller in subsequent calculations leading as the activation travel further and further.\n",
    "    2. Many normalization techniques would alleviate the problem but the simplest is to just initialize the weights so that they are not very large or very small. \n",
    "    3. Pytorch does not come with predefined initialization fucntionality so we will have to define the weights oureselves.\n",
    "    4. We can treat the init_weights function as boilerplate code without needing much to understand it.\n",
    "\n",
    "    ##### Training and Validating the model\n",
    "    1. Now it's the time to take the various pieces we have been working on and assemble them into something we can actually execute.\n",
    "    2. In the training loop in the main function : \n",
    "        1. We can see that the trnMetrics_g tensor collects per class metrics during training. This type of insight is very useful in larger projects.\n",
    "        2. We do not directly iterate over the train_dl dataloader, we iterate with an estimated time of completion. This is a stylistic choice.\n",
    "        3. The actual loss computation is done by the computeBatchLoss method this is not nescessary but it is good pratice to split it up in functions.\n",
    "    3. The trnMetrics_g tensor is to transport information about how the model is behaving on a per sample basis from the computeBatchLoss function to the LogMetrics function\n",
    "\n",
    "        ##### The `computeBatchLoss` function:\n",
    "        1. This function is responsible for computing the actal loss and is called in by both training and validation loops. \n",
    "        2. It computes the loss and also records the per sample output that our model is producing. This lets us compute the % of correct answers per class.\n",
    "        3. In this function we are not using the stadard loss value of the average loss of the batch, instead we get a tensor of loss values, one per sample and we can aggregate them as we according to our problem. In projects where you do not want to keep the individual loss values per sample, it is perfectly fine to use the averaged loss over the batch as the output.\n",
    "        5. If we leave the function as it is we have accomplished the most of the backpropagation and the weight update parts. But we also need to record per sample stats for psterity and later analysis. We'll use the metrics_g parameter passed in to accomplish it.\n",
    "\n",
    "        ##### The validation loop\n",
    "        1. The validation loop is somewhat similar to the training loop w.r.t the metrics calculated on a per sample basis. And we will also not calculate the gradients and update the weights.\n",
    "\n",
    "    ##### Outputting the performance metrics\n",
    "    \n",
    "    1. One last thing that we do per epoch is to log in out performance metrics for this epoch. When we have logged in the metrics we need to return to the next epoch. Logging results and progress as we go is important  because if the training does not converge we can easily notice it and stop training the model. \n",
    "    2. Ealier we were collecting our information in trainMetrics_g and logMetrics_g for logging per epoch. Each of these tensors now needs to compute our % correct and average loss per class for the training and validation runs.\n",
    "    3. Logging of the metrics per epoch is somewhat of a common choice but the logs get reasonably big if we are training for many epochs. If future we can do that after a set number of epochs.\n",
    "    4. ##### The logMetircs function\n",
    "        1. The function takes in the epochs_ndx argument which is used to keep track of the number of epochs.\n",
    "        2. The `mode_str` argument tells us which metrics is used, training or validation.\n",
    "        3. The trainMetrics_g or valMetrics_g is the tensor that is passed in the function which keeps the metrics of the training or validation. This has been computed from the computBatchLoss function and transferred to the CPU after being computed in the CPU. Both these tensors have 3 rows and as many columns as we have samples.\n",
    "    5. ##### Tensor masking and Boolean Indexing\n",
    "        1. Masks are common usage pattern used in arrays.\n",
    "        2. We will use masks to get the number of nodules and non-nodules in the data. Will find the total samples per class as well as how many did we classify correctly.\n",
    "        3. First we calculate and track the overall loss, since this is the metric which should be optimized, we would always keep track of it.\n",
    "        4. Then we limit the loss averaging to only those samples with the negative labels using the negLabel_mask. We do the same for the positive class.\n",
    "        5. Calculating the loss per class is useful if we have a class that is consistently harder to classify since that can help drive investigation and improvements.\n",
    "        6. We'll close out the calculations with determining the fraction of samples we calssified correctly as well  as the fraction correct from each label. Since we will display these numbers as percentages we will also multiply the values by 100. \n",
    "        7. After the calculations we log our results with 3 calls to log.info.\n",
    "        \n",
    "    ##### Running the training script\n",
    "    1. Now that we have completed the training script we need to run it. This will intialize and train and run our model while printing the stats of how well the model is performing.\n",
    "    2. \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "from logging import log\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataset import LunaDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels, conv_channels, kernel_size=3, padding=1, bias = True)\n",
    "\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv3D(in_channels, conv_channels, kernel_size = 3, padding = 1, bias = True)\n",
    "\n",
    "        self.relu2 = nn.ReLU(inplace = True)\n",
    "\n",
    "        self.max_pool = nn.MaxPool3d(2,2)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)     # this could be implemented as calls to functional API instead\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)     # this could be implemented as calls to functional API instead\n",
    "        block_out = self.max_pool(block_out)  # this could be implemented as calls to functional API instead\n",
    "\n",
    "        return block_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels = 1 , conv_channels = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
    "\n",
    "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels*2)\n",
    "        self.block3 = LunaBlock(conv_channels*2, conv_channels*4)\n",
    "        self.block4 = LunaBlock(conv_channels*4, conv_channels*8)\n",
    "\n",
    "        self.head_linear = nn.Linear(1152, 2)\n",
    "        self.head_softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules:\n",
    "            if type(m) in {nn.Linear, nn.Conv3d}:\n",
    "                nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                bound = 1 / math.sqrt(fan_out)\n",
    "                nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "\n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "\n",
    "        block_1_out = self.block1(bn_output)\n",
    "        block_2_out = self.block2(block_1_out)\n",
    "        block_3_out = self.block3(block_2_out)\n",
    "        block_4_out = self.block4(block_3_out)\n",
    "\n",
    "        conv_flat = block_4_out.view(block_4_out.size(0), -1)  # Flattening to (batch_size, -1)\n",
    "\n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "        softmax_output = self.head_softmax(linear_output)\n",
    "\n",
    "        return linear_output, softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num-workers NUM_WORKERS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: c:\\Users\\Anant\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"f938c452-538d-451e-acb1-1280e1d17eed\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\Anant\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-18608OGI2p1xdQlZW.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anant\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "METRICS_LABEL_NDX = 0    # These are the named array indices which are declared at the module level.\n",
    "METRICS_PRED_NDX = 1     # These are the named array indices which are declared at the module level.\n",
    "METRICS_LOSS_NDX = 2    # These are the named array indices which are declared at the module level.\n",
    "METRICS_SIZE = 3        # These are the named array indices which are declared at the module level.\n",
    "\n",
    "def enumerateWithEstimate(\n",
    "        iter,\n",
    "        desc_str,\n",
    "        start_ndx=0,\n",
    "        print_ndx=4,\n",
    "        backoff=None,\n",
    "        iter_len=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    In terms of behavior, `enumerateWithEstimate` is almost identical\n",
    "    to the standard `enumerate` (the differences are things like how\n",
    "    our function returns a generator, while `enumerate` returns a\n",
    "    specialized `<enumerate object at 0x...>`).\n",
    "    However, the side effects (logging, specifically) are what make the\n",
    "    function interesting.\n",
    "    :param iter: `iter` is the iterable that will be passed into\n",
    "        `enumerate`. Required.\n",
    "    :param desc_str: This is a human-readable string that describes\n",
    "        what the loop is doing. The value is arbitrary, but should be\n",
    "        kept reasonably short. Things like `\"epoch 4 training\"` or\n",
    "        `\"deleting temp files\"` or similar would all make sense.\n",
    "    :param start_ndx: This parameter defines how many iterations of the\n",
    "        loop should be skipped before timing actually starts. Skipping\n",
    "        a few iterations can be useful if there are startup costs like\n",
    "        caching that are only paid early on, resulting in a skewed\n",
    "        average when those early iterations dominate the average time\n",
    "        per iteration.\n",
    "        NOTE: Using `start_ndx` to skip some iterations makes the time\n",
    "        spent performing those iterations not be included in the\n",
    "        displayed duration. Please account for this if you use the\n",
    "        displayed duration for anything formal.\n",
    "        This parameter defaults to `0`.\n",
    "    :param print_ndx: determines which loop interation that the timing\n",
    "        logging will start on. The intent is that we don't start\n",
    "        logging until we've given the loop a few iterations to let the\n",
    "        average time-per-iteration a chance to stablize a bit. We\n",
    "        require that `print_ndx` not be less than `start_ndx` times\n",
    "        `backoff`, since `start_ndx` greater than `0` implies that the\n",
    "        early N iterations are unstable from a timing perspective.\n",
    "        `print_ndx` defaults to `4`.\n",
    "    :param backoff: This is used to how many iterations to skip before\n",
    "        logging again. Frequent logging is less interesting later on,\n",
    "        so by default we double the gap between logging messages each\n",
    "        time after the first.\n",
    "        `backoff` defaults to `2` unless iter_len is > 1000, in which\n",
    "        case it defaults to `4`.\n",
    "    :param iter_len: Since we need to know the number of items to\n",
    "        estimate when the loop will finish, that can be provided by\n",
    "        passing in a value for `iter_len`. If a value isn't provided,\n",
    "        then it will be set by using the value of `len(iter)`.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if iter_len is None:\n",
    "        iter_len = len(iter)\n",
    "\n",
    "    if backoff is None:\n",
    "        backoff = 2\n",
    "        while backoff ** 7 < iter_len:\n",
    "            backoff *= 2\n",
    "\n",
    "    assert backoff >= 2\n",
    "    while print_ndx < start_ndx * backoff:\n",
    "        print_ndx *= backoff\n",
    "\n",
    "    log.warning(\"{} ----/{}, starting\".format(\n",
    "        desc_str,\n",
    "        iter_len,\n",
    "    ))\n",
    "    start_ts = time.time()\n",
    "    for (current_ndx, item) in enumerate(iter):\n",
    "        yield (current_ndx, item)\n",
    "        if current_ndx == print_ndx:\n",
    "            # ... <1>\n",
    "            duration_sec = ((time.time() - start_ts)\n",
    "                            / (current_ndx - start_ndx + 1)\n",
    "                            * (iter_len-start_ndx)\n",
    "                            )\n",
    "\n",
    "            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n",
    "            done_td = datetime.timedelta(seconds=duration_sec)\n",
    "\n",
    "            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n",
    "                desc_str,\n",
    "                current_ndx,\n",
    "                iter_len,\n",
    "                str(done_dt).rsplit('.', 1)[0],\n",
    "                str(done_td).rsplit('.', 1)[0],\n",
    "            ))\n",
    "\n",
    "            print_ndx *= backoff\n",
    "\n",
    "        if current_ndx + 1 == start_ndx:\n",
    "            start_ts = time.time()\n",
    "\n",
    "    log.warning(\"{} ----/{}, done at {}\".format(\n",
    "        desc_str,\n",
    "        iter_len,\n",
    "        str(datetime.datetime.now()).rsplit('.', 1)[0],\n",
    "    ))\n",
    "\n",
    "class LunaTrainingApp():\n",
    "\n",
    "    def __init__(self, sys_argv = None):\n",
    "        # Here we are checking of arguments are provided by the user in the CLI. If not then we use default system arguments\n",
    "        if sys_argv == None: # check \n",
    "            sys_argv = sys.argv    # using the defualt system arguments\n",
    "\n",
    "        # Then we instantiate the argument parser object.\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "        # Now we add an argument --num-workers which lets us specify how many backgroung workers would be utilized for data loading.\n",
    "        parser.add_argument('--num-workers',\n",
    "                                help = 'number of worker processes for backgroung data loading',\n",
    "                                type = int,\n",
    "                                default = 8)\n",
    "\n",
    "        # Then we parse arguments provided in the CLI and assign them to an attribute cli_args\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "\n",
    "        # We instantiate a datetime.now object and assign it to the time_str attribute.\n",
    "        self.time_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "        # Now we will initialize the model and the optimizer\n",
    "\n",
    "        # First we will check if the GPU is available\n",
    "        self.use_cude = torch.cuda.is_available()\n",
    "        # If GPU is available then use GPU else use CPU\n",
    "        self.device = torch.device('cuda' if self.use_cuda else 'cpu')\n",
    "\n",
    "        # Initialize the model \n",
    "        self.model = self.initModel()\n",
    "\n",
    "        #Initialize the optimizer\n",
    "        self.optimizer = self.initOptimizer()\n",
    "\n",
    "    def initModel():\n",
    "        \"\"\" This function initialzes the model and transfers the model and parameters to the GPU.\n",
    "            If multiple GPU's are available then execute the model computations in paraller and sync and return the results \"\"\"\n",
    "\n",
    "        # initialize the model\n",
    "        model = LunaModel()\n",
    "\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            log.info(f\"Using CUDA : {torch.cuda.device_count()} devices\")\n",
    "            # If multiple GPU's are available then execute the computations in parallel\n",
    "            if torch.cuda.device_count()>1:\n",
    "                model = nn.DataParallel(model)\n",
    "            # move the model to the device\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def initOptimizer(self):\n",
    "        return optim.SGD(self.model.parameters(), lr= 0.001,momentum = 0.99)\n",
    "\n",
    "    # Now lets put the training data into the dataloader.\n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(val_stride= 10, is_val_set_bool = False)\n",
    "\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            self.batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_dl = DataLoader(train_ds,\n",
    "                                batch_size = batch_size,\n",
    "                                num_workers = self.cli_args.num_workers,\n",
    "                                pin_memory = self.use_cuda)\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(val_stride = 10, is_val_set_bool=True)\n",
    "        batch_size = self.cli_args.batch_size\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.batch_size *= torch.cuda.device_count()\n",
    "        \n",
    "        val_dl = DataLoader(val_ds,\n",
    "                            batch_size = batch_size,\n",
    "                            num_workers= self.cli_args.num_workers,\n",
    "                            pin_memory = self.use_cuda)\n",
    "\n",
    "        return val_dl\n",
    "    \n",
    "    def logMetrics(self, epochs_ndx, mode_str, metrics_t, classification_threshold):\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classification_threshold   # Non Nodules\n",
    "        negPred_mask = metrics_t[METRICS_LABEL_NDX] >= classification_threshold    # Nodules\n",
    "\n",
    "\n",
    "        posLabel_mask = ~negLabel_mask\n",
    "        posPred_mask = ~posLabel_mask\n",
    "\n",
    "\n",
    "        # Next we  would use the masks to calculate some per label statistics and use them to store in a dictionary metrics_dict\n",
    "        metrics_dict = {}\n",
    "        neg_count = int(negLabel_mask.sum())\n",
    "        pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "        neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "        pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "\n",
    "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "        metrics_dict['correct/all'] = metrics_t(pos_correct + neg_correct)/ np.float32(metrics_t.shape[1]) * 100\n",
    "        metrics_dict['correct/neg'] = neg_correct/ np.float32(neg_count) * 100\n",
    "        metrics_dict['correct/pos'] = pos_correct/ np.float32(pos_count) * 100\n",
    "\n",
    "\n",
    "        log.info(f\"E{epochs_ndx}, {mode_str[:8]}, {metrics_dict['loss/all']:.4f} LOSS, {metrics_dict['correct/all']:-5.1f}% CORRECT\")\n",
    "        log.info(f\"E{epochs_ndx}, {mode_str[:8]}, {metrics_dict['loss/neg']:.4f} LOSS, {metrics_dict['correct/neg']:-5.1f}% CORRECT {neg_correct} of {neg_count}\")\n",
    "        log.info(f\"E{epochs_ndx}, {mode_str[:8]}, {metrics_dict['loss/pos']:.4f} LOSS, {metrics_dict['correct/pos']:-5.1f}% CORRECT {pos_correct} of {pos_count}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size,  metrics_g):\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking = True)\n",
    "        label_g = label_t.to(self.device, non_blocking = True)\n",
    "\n",
    "\n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "        loss_fn = nn.CrossEntropyLoss(reduction = 'none')  # reduction = 'none' gives the loss per sample\n",
    "\n",
    "        loss_g = loss_fn(logits_g, label_g[:,1])\n",
    "\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "\n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1].detach()            # We use detach since none of them need to hold gradients.\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:, 1].detach()\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g[:,1].detach()\n",
    "\n",
    "\n",
    "        return loss_g.mean()  # recombines the loss per sample to a single value averaged over the entire batch \n",
    "\n",
    "\n",
    "    def doTraining(self, epochs_ndx, train_dl):\n",
    "            self.model.train()\n",
    "            trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device = self.device)  # Initializes an empty array\n",
    "\n",
    "            batch_iter = enumerateWithEstimate(train_dl, f\"E{epochs_ndx} Training\")  # Sets up batch looping with time estimate\n",
    "            start_ndx = train_dl.num_workers\n",
    "\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.optimizer.zero_grad()   # Frees up any leftover gradient tensors\n",
    "\n",
    "                loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
    "\n",
    "                loss_var.backward()     # Backpropagates\n",
    "\n",
    "                self.optimizer.step()   # Updates the model weights\n",
    "\n",
    "            self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "\n",
    "            return trnMetrics_g.to('cpu')\n",
    "            \n",
    "\n",
    "    def doValidation(self, epochs_ndx, val_dl):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "\n",
    "                valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device = self.device)\n",
    "\n",
    "                batch_iter = enumerateWithEstimate(val_dl, f\"E{epochs_ndx} Validation\", start_ndx = val_dl.num_workers)\n",
    "                \n",
    "                for batch_ndx, batch_tup in batch_iter:\n",
    "                    self.computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "            return valMetrics_g.to('cpu')   # We would need to measure the validation statistics for each sample\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        \n",
    "        log.info(f\"Starting {type(self).__name__}, {self.cli_args}\")\n",
    "        train_dl = self.initTrainDL()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        for epochs_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            trnMetrics_t = self.doTraining(epochs_ndx, train_dl)\n",
    "            self.logMetrics(epochs_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            valMetrics_t = self.doValidation(epochs_ndx, val_dl)\n",
    "            self.logMetrics(epochs_ndx, 'val', valMetrics_t)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LunaTrainingApp().main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Anant\\\\anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"cef03c5f-6114-44be-b09b-9f9ff47f28fd\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=c:\\\\Users\\\\Anant\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-16508q7E1hkay3Uch.json']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RootLogger root (WARNING)>\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "print(logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
