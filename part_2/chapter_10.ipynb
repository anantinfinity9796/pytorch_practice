{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data sources in unified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading and processing raw data files\n",
    "* Implementing a Python class to represent our data\n",
    "* Converting our data format into a format usable by pytorch\n",
    "* Visualizing the training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now that we have discussed the high level goals of part2 its time to discuss the specifics in this chapter. Now it's time to implement the basic data-loading and data-processing routines for our raw data. Basically evry significant project that we work on will need something analogous to what we work on here.\n",
    "2. Our goal is to be able to produce a training sample given our inputs of our raw CT scan data and a list of annotations for those CTs. This might sound simple but quite a bit needs to happen before we can load, process and extract the data we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw CT data files\n",
    "1. Our CT data comes in two files, a .mhd data which contains the metadata header information and a .raw file containing the raw files that make up the 3D array.\n",
    "2. Each files name starts with an identifier called as a `Series UID` for the CT scan in question.\n",
    "3. Our CT class will consume those two files and produce the 3D array, as well as the transformation matrix to convert from the patient coordinate system (discussed later) to index, row, column coordinates needed by the array. We just have some coordinate system conversion to do before we can apply these coordinates to the CT data. The trasformation matric is contained in the .mhd file.\n",
    "4. We will also load the annotation data provided by LUNA, which will give us a list of nodule coordinates, each with a malignancy flag, along with the series UID of the relevant CT Scan. By combining the nodule coordinate with the coordinate system transformation information, we get the index, row and column of the coordinates of the voxel at the center of our nodule.\n",
    "5. Using the I,R,C data we can crop a small 3D slice of our CT Scan to use as an input to our model. Along with this 3D sample of our array we must construct the rest of our training sample tuple which will have\n",
    "    * Sample array\n",
    "    * Nodule Status Flag\n",
    "    * Series UID\n",
    "    * The index of this sample in the CT list of nodule candidates.\\\n",
    "    This sample tuple is exactly what pytorch expects from our Dataset subclass and represents the last section of our bridge from our original raw data to the standard structure of Pytorch Tensors.\n",
    "6. Limiting or cropping our data so as not to drown our model with noise is important. But we need to make sure that the cropping is not so agressive that our signal gets cropped out of the input. This is known as feature engineering, but here we would not do traditional feature engineering rather we would let the model do the heavy lifting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing LUNA's annotation data\n",
    "The first thing we need to do is to load the data and see what the data looks like. We could try loading and parsing individual CT scans but it would be better to parse the csv files that luna provides, which contains points of interest in each CT Scan. The information that we will get is:\n",
    "* The coordinates\n",
    "* An indication whether the coordinate is a nodule\n",
    "* A unique identifier for the CT Scan\n",
    "Since there are fewer types of information in the CSV files, they are easier to parse and will give us some clues about what to look for, if we start loading the CT's.\n",
    "* The candidate.csv contains information about all lumps that look like nodules, whether those lumps are malignant, benign tumors or something else altogether. We will use this a basis for a complete list of candidates that can be split into training and validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_uid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>-56.08</td>\n",
       "      <td>-67.85</td>\n",
       "      <td>-311.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>53.21</td>\n",
       "      <td>-244.41</td>\n",
       "      <td>-245.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>103.66</td>\n",
       "      <td>-121.80</td>\n",
       "      <td>-286.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>-33.66</td>\n",
       "      <td>-72.75</td>\n",
       "      <td>-308.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>-32.25</td>\n",
       "      <td>-85.36</td>\n",
       "      <td>-362.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551060</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>-55.66</td>\n",
       "      <td>37.24</td>\n",
       "      <td>-110.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551061</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>68.40</td>\n",
       "      <td>70.18</td>\n",
       "      <td>-109.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551062</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>-82.29</td>\n",
       "      <td>-27.94</td>\n",
       "      <td>-106.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551063</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>38.26</td>\n",
       "      <td>83.50</td>\n",
       "      <td>-102.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551064</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>-63.37</td>\n",
       "      <td>-9.71</td>\n",
       "      <td>-111.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551065 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               series_uid       x       y  \\\n",
       "0       1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -56.08  -67.85   \n",
       "1       1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...   53.21 -244.41   \n",
       "2       1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  103.66 -121.80   \n",
       "3       1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -33.66  -72.75   \n",
       "4       1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -32.25  -85.36   \n",
       "...                                                   ...     ...     ...   \n",
       "551060  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...  -55.66   37.24   \n",
       "551061  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...   68.40   70.18   \n",
       "551062  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...  -82.29  -27.94   \n",
       "551063  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...   38.26   83.50   \n",
       "551064  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...  -63.37   -9.71   \n",
       "\n",
       "             z  class  \n",
       "0      -311.92      0  \n",
       "1      -245.17      0  \n",
       "2      -286.62      0  \n",
       "3      -308.41      0  \n",
       "4      -362.51      0  \n",
       "...        ...    ...  \n",
       "551060 -110.42      0  \n",
       "551061 -109.72      0  \n",
       "551062 -106.92      0  \n",
       "551063 -102.71      0  \n",
       "551064 -111.12      0  \n",
       "\n",
       "[551065 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing candidates.csv file, we can use pandas to parse the files\n",
    "\n",
    "# reading the csv file\n",
    "candidate = pd.read_csv(\"E:\\data\\data-unversioned/candidates.csv\")\n",
    "\n",
    "# renaming the columns\n",
    "candidate.rename(columns={\"seriesuid\":\"series_uid\", \"coordX\":\"x\",\"coordY\":\"y\",\"coordZ\":\"z\"}, inplace=True)\n",
    "\n",
    "# displaying the dataframe\n",
    "candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_uid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>diameter_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>-128.699421</td>\n",
       "      <td>-175.319272</td>\n",
       "      <td>-298.387506</td>\n",
       "      <td>5.651471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>103.783651</td>\n",
       "      <td>-211.925149</td>\n",
       "      <td>-227.121250</td>\n",
       "      <td>4.224708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...</td>\n",
       "      <td>69.639017</td>\n",
       "      <td>-140.944586</td>\n",
       "      <td>876.374496</td>\n",
       "      <td>5.786348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "      <td>-24.013824</td>\n",
       "      <td>192.102405</td>\n",
       "      <td>-391.081276</td>\n",
       "      <td>8.143262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "      <td>2.441547</td>\n",
       "      <td>172.464881</td>\n",
       "      <td>-405.493732</td>\n",
       "      <td>18.545150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-160.856298</td>\n",
       "      <td>-28.560349</td>\n",
       "      <td>-269.168728</td>\n",
       "      <td>5.053694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-102.189570</td>\n",
       "      <td>-73.865766</td>\n",
       "      <td>-220.536241</td>\n",
       "      <td>4.556101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-37.535409</td>\n",
       "      <td>64.041949</td>\n",
       "      <td>-127.687101</td>\n",
       "      <td>4.357368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>43.196112</td>\n",
       "      <td>74.438486</td>\n",
       "      <td>-200.523314</td>\n",
       "      <td>4.277203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>-21.958478</td>\n",
       "      <td>33.486096</td>\n",
       "      <td>-155.292026</td>\n",
       "      <td>23.802913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             series_uid           x  \\\n",
       "0     1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222... -128.699421   \n",
       "1     1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  103.783651   \n",
       "2     1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...   69.639017   \n",
       "3     1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...  -24.013824   \n",
       "4     1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...    2.441547   \n",
       "...                                                 ...         ...   \n",
       "1181  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950... -160.856298   \n",
       "1182  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950... -102.189570   \n",
       "1183  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...  -37.535409   \n",
       "1184  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...   43.196112   \n",
       "1185  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...  -21.958478   \n",
       "\n",
       "               y           z  diameter_mm  \n",
       "0    -175.319272 -298.387506     5.651471  \n",
       "1    -211.925149 -227.121250     4.224708  \n",
       "2    -140.944586  876.374496     5.786348  \n",
       "3     192.102405 -391.081276     8.143262  \n",
       "4     172.464881 -405.493732    18.545150  \n",
       "...          ...         ...          ...  \n",
       "1181  -28.560349 -269.168728     5.053694  \n",
       "1182  -73.865766 -220.536241     4.556101  \n",
       "1183   64.041949 -127.687101     4.357368  \n",
       "1184   74.438486 -200.523314     4.277203  \n",
       "1185   33.486096 -155.292026    23.802913  \n",
       "\n",
       "[1186 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The annotations.csv contains information about the candidates that have been flagged as nodules\n",
    "# We are particularly interested in the diameter_mm feature in particular\n",
    "annotations = pd.read_csv(\"E:\\data\\data-unversioned/annotations.csv\")\n",
    "annotations.rename(columns={\"seriesuid\":\"series_uid\", \"coordX\":\"x\",\"coordY\":\"y\",\"coordZ\":\"z\"}, inplace = True)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get item from object for given key (ex: DataFrame column).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.339142594937666268384335506819    1468\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.174692377730646477496286081479    1425\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.279953669991076107785464313394    1414\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.113697708991260454310623082679    1408\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.258220324170977900491673635112    1400\n",
       "                                                                    ... \n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.608029415915051219877530734559     172\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.397202838387416555106806022938      65\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.225515255547637437801620523312      57\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.935683764293840351008008793409      52\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630      32\n",
       "Name: series_uid, Length: 888, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate['series_uid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549714\n",
       "1      1351\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts the number of examples which are nodules(0) or non_nodules(1)\n",
    "candidate['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_uid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>diameter_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>-128.699421</td>\n",
       "      <td>-175.319272</td>\n",
       "      <td>-298.387506</td>\n",
       "      <td>5.651471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>103.783651</td>\n",
       "      <td>-211.925149</td>\n",
       "      <td>-227.121250</td>\n",
       "      <td>4.224708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...</td>\n",
       "      <td>69.639017</td>\n",
       "      <td>-140.944586</td>\n",
       "      <td>876.374496</td>\n",
       "      <td>5.786348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "      <td>-24.013824</td>\n",
       "      <td>192.102405</td>\n",
       "      <td>-391.081276</td>\n",
       "      <td>8.143262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "      <td>2.441547</td>\n",
       "      <td>172.464881</td>\n",
       "      <td>-405.493732</td>\n",
       "      <td>18.545150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-160.856298</td>\n",
       "      <td>-28.560349</td>\n",
       "      <td>-269.168728</td>\n",
       "      <td>5.053694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-102.189570</td>\n",
       "      <td>-73.865766</td>\n",
       "      <td>-220.536241</td>\n",
       "      <td>4.556101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...</td>\n",
       "      <td>-37.535409</td>\n",
       "      <td>64.041949</td>\n",
       "      <td>-127.687101</td>\n",
       "      <td>4.357368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>43.196112</td>\n",
       "      <td>74.438486</td>\n",
       "      <td>-200.523314</td>\n",
       "      <td>4.277203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...</td>\n",
       "      <td>-21.958478</td>\n",
       "      <td>33.486096</td>\n",
       "      <td>-155.292026</td>\n",
       "      <td>23.802913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             series_uid           x  \\\n",
       "0     1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222... -128.699421   \n",
       "1     1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  103.783651   \n",
       "2     1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...   69.639017   \n",
       "3     1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...  -24.013824   \n",
       "4     1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...    2.441547   \n",
       "...                                                 ...         ...   \n",
       "1181  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950... -160.856298   \n",
       "1182  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950... -102.189570   \n",
       "1183  1.3.6.1.4.1.14519.5.2.1.6279.6001.994459772950...  -37.535409   \n",
       "1184  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...   43.196112   \n",
       "1185  1.3.6.1.4.1.14519.5.2.1.6279.6001.997611074084...  -21.958478   \n",
       "\n",
       "               y           z  diameter_mm  \n",
       "0    -175.319272 -298.387506     5.651471  \n",
       "1    -211.925149 -227.121250     4.224708  \n",
       "2    -140.944586  876.374496     5.786348  \n",
       "3     192.102405 -391.081276     8.143262  \n",
       "4     172.464881 -405.493732    18.545150  \n",
       "...          ...         ...          ...  \n",
       "1181  -28.560349 -269.168728     5.053694  \n",
       "1182  -73.865766 -220.536241     4.556101  \n",
       "1183   64.041949 -127.687101     4.357368  \n",
       "1184   74.438486 -200.523314     4.277203  \n",
       "1185   33.486096 -155.292026    23.802913  \n",
       "\n",
       "[1186 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The annotations.csv contains information about the candidates that have been flagged as nodules\n",
    "# We are particularly interested in the diameter_mm feature in particular\n",
    "annotations = pd.read_csv(\"E:\\data\\data-unversioned/annotations.csv\")\n",
    "annotations.rename(columns={\"seriesuid\":\"series_uid\", \"coordX\":\"x\",\"coordY\":\"y\",\"coordZ\":\"z\"}, inplace = True)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.176030616406569931557298712518    12\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.328789598898469177563438457842     9\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.195557219224169985110295082004     9\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.202187810895588720702176009630     9\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.219428004988664846407984058588     9\n",
       "                                                                    ..\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.164988920331211858091402361989     1\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.202464973819273687476049035824     1\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.200841000324240313648595016964     1\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.167919147233131417984739058859     1\n",
       "1.3.6.1.4.1.14519.5.2.1.6279.6001.313605260055394498989743099991     1\n",
       "Name: series_uid, Length: 601, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['series_uid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have information about around 1200 nodules. This is useful bacause we can use it to make sure that our training and validation data includes a representative spread of nodule sizes. Without this it is possible that our validation set could end up with only extreme values making it seem as though our model is underperforming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Sets\n",
    "For any classification task we split our data in training and validation sets. We want to make sure that both the sets are representative of the range of real-world input data we are expecting to see and handle normally. If either set is meaningfully different from real-world use cases, it's pretty likely that our model would behave differently than we expect. All the statistics that we collect and the trainings that we compute will not be predictive once we transfer our model to production to use.\\\n",
    "Lets go back to our nodules in annotations.csv file:\n",
    "1. We are going to sort the nodules in ascending order and we are going to take every nth element of the data for our validation set. This should give us a sample representative of the data. But the problem is that sometimes the results of annotations.csv does not match with the candidates.csv.\n",
    "2. Real-world data sources are often imperfect we are going to have to do some work to make them line-up, this is a good-example of the kind of work that you would need to assemble data from disparate data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unifying our annotation and candidate data\n",
    "Now that we have seen what the data looks like lets, build a candidate infoList function which will stitch all these things together. We will use a named tuple to hold all of these information that is defined at the top of the file to hold the information of each nodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\n",
      "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860.mhd\n",
      "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "list_d = os.listdir(\"E://data/data-unversioned/subset0/\")\n",
    "filename = list_d[0]\n",
    "print(filename)\n",
    "print(os.path.split(filename)[-1])\n",
    "print(os.path.split(filename)[-1][:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tuple instances are not our training samples, as they are missing the chunks of CT data that we need. Instead they represent a sanitized, cleaned and unified interface to human-annotated data that we are using.\\\n",
    "`Note : Clearly seperate the code that is responsible for data sanitization from the rest of your project. Don't be afraid to rewrite your data once and save it to disk if needed.`\\\n",
    "Our candidate tuple would have:\n",
    "1. `nodule status`: what we are going to be training the model to classify.\n",
    "2. `Diameter` : useful for getting a good spread in training, since large and small nodules would not have the same features.\n",
    "3. `series_uid` : To locate the correct CT Scan\n",
    "4. `Candidate center` : To find the candidate in the larger CT scan\\\n",
    "The function that will build a list of these NoduleInfoTuple instances starts by using an in-memory caching decorator, followed by getting the list of files present on the disk.\n",
    "\n",
    "1. Since parsing the data files can be a slow process. We will be caching the results of this function call in memory. This will come in handy later because we will be calling this function more often in future chapters. Sppeding up our data pipleline by carefully applying in-memory or on disk caching will result in some pretty impressive gains in training speed. Keep an eye out on these opportunities when working on your projects.\n",
    "2. Earlier we said that we will run our program on a less than full set of training data, due to long download times and high disk space requirements. `The requireOnDisk_bool = True parameter makes good on this promise`, we are detecting the series_uid's that are actually present and ready to be loaded from disk and we are using that information to limit which entries that we use from the CSV file we are going to parse.\n",
    "3. Being able to run our loop on a subset of data is useful because it can help to see if the code is working as intended. Often the metrics are bad but excercising our metrics, logging, model-checking and similar functionality is beneficial. We can then load the full training set to improve the performance after we are satisfied with all the other criteria's.\n",
    "4. After we get our candidate information, we want to merge the diameter information from annotations.csv file. First we need to group our annotations by the series_uid, and that is the first key that we will use to cross-reference each row from the two files. \n",
    "5. For each candidate_entry for a given series_uid, we would loop through the annotations we collected earlier for the same series_uid and see of the nodules are close enough to be classified as the same nodule. If yes then we have found out the diameter of the nodule. If not then we will just treat the nodule as having 0.0 diameter. Since we are using this information to get a good spread of nodule sizes in our training and validation sets, having incorrect diameter sizes for the same nodules should not be a problem, but we should remember we are doing this in case our assumption is wrong.\n",
    "Doing this sort of fiddly code is to just converge on the nodule diameter is somewhat fuzzy. But this is required when we are dealing with raw data. Once we get to this point we just need to sort the data and return it.\n",
    "6. The ordering of tuple members in this nodule_info_list is driven by this sort. We are using this approach that when we use a slice of the data, that slice gets a representative chunk of the actual nodules, with a good spread of nodule diameters.\n",
    "\n",
    "\n",
    "#### Loading the individual CT Scans\n",
    "1. Next up we need to be able to take our raw CT scan data and transform it from bits on the disk to a python object from which we can extract 3D nodule density data.\n",
    "2. Our annotation information acts as a map to the interesting parts of the Raw data. Before we can follow that map to our data of interest we need to get the data into an adressable form.\\\n",
    "`Note : Having large amounts of raw data, most of which is uninteresting is common in machine learning. Look for ways to limit your scope to only relevant data when working on your own projects.`\n",
    "3. The DICOM format is a very old format, so LUNA has converted our data into a format known as meta.IO format which is a bit easier to use. Don't worry if you have never heard of the format before. We can treat the data format as a black box and use the SimpleITK library to load them into more familiar numpy arrays.\n",
    "4. For real projects you would want to know what type of information is contained in your data, but it's perfectly fine to use libraries like simpleITK to parse the bits on the disk. Just remember that we are more concerned about the data and not how it is represented on the disk.\n",
    "5. Being able to uniquely identify the sample is very useful for ourselves, clearly communicating which sample is causing trouble can help us isolate the case and look at it more closely.\n",
    "6. We identify the CT scans using series instance UID's (series_uid) when the CT scan was created. DICOM makes heavy useful of identifiers like UUID's but they are created in different ways and are formatted differently.\n",
    "7. At this point ct_a is a 3D array. All dimensions are spatial and the single intensity channel is implicit i.e the channle information is represented as the fourth dimension of size 1.\n",
    "\n",
    "##### Hounsefield Units\n",
    "1. Without understanding the nuances of our data values and range, it will hinder our models ability to learn well from the data\n",
    "2. The CT scans are typically stored as a signed 12 bit integer(shoved into 16 bit integers) which fits well with the level of precision CT scanners can provide.\n",
    "3. CT scans are expressed in `Hounsfield units` which are odd units. `Air` is expressed as `-1000 HU (close to 0g/cc)`, `Water` is expressed as `0 HU(1g/cc)` and `Bone` is expressed as `+1000 HU (2-3g/cc)`.\n",
    "4. CT scanners use negative HU values to correspond to negative densities to indicate that those voxels are out of the CT Scanners view. For our purposes every thing outside the patients body is irrelevant and we also don't want the exact densities of the bones. So clipping our values HU values from `-1000 to 1000` would be a good range but it would not be biologically sound for most of the cases.\n",
    "5. Values above 0 HU don't scale perfectly with density but as our tumors are arounf the range of 0 HU we do not need our values to perfectly map from HU to g/cc because we will be using HU units as the input.\n",
    "6. We want to remove all of these outliers from the data becuase they aren't directly relevant to our goal and having outliers can make the models job harder. This can happen in many ways like when the unclipped data is fed to the batch normalization and the statistics on how to best normalize the data are skewed.`Always be on the lookout for these ways to clean your data`.\n",
    "7. It is important to know that our data has values between -1000 to 1000, since in chapter 13 we end up adding channels of information to our samples. If we do not account for disparities between HU and our data, those new channels can easily be overshadowed by the raw HU data. We do not add those channels for the classification step of the project so we do not need to implement special handling right now.\n",
    "\n",
    "##### Locating a nodule using the patient coordinate system\n",
    "1. Deep Learning model expect fixed size inputs due to the fixed number of input neurons. we need to input a fixed size input which contains our candidate nodule so that it can used as an input to our classifier. We would like to train our model on a crop of the data which has the candidate nicely centered so that our model does not need to learn to identify nodules tucked in the corners. By reducing the variation of our inputs we make the models job easier.\n",
    "    ##### The patient coordinate system\n",
    "    1. Unfortunately all the candidates data we loaded earlier is in millimeters(x,y,z coordinate system) and not voxels(I,R,C) coordinate system.\n",
    "    2. So they need to be converted from the x,y,z coordinate system to the voxel based I,R,C coordinate system used to take array slices from our CT scan data. This is a classic example of how to handle units consistently.\n",
    "    3. As mentioned previously the patient when dealing with CT scans we define array dimensions x,y,z as index, row and column, because seperate meanings exists for the x,y and z axis.\n",
    "    4. `The patient coordinate system defines the postive X to be the patients left(Left), positive Y to be the patients behind (Posterior) and positive Z to be towards the patients head (Superior). This is sometimes also referred to as Left-Posterior-Superior(LPS).`\n",
    "    5. `The patient coordinate system is measured in millimeters and has an arbitrarily positioned origin, that does not correspond to the origin of the CT voxel array`.\n",
    "    6. The patient coordinate system is used to specify the locations of interesting anatomy in a way that is independent of any particular scan. The metadata that defines the relation between the CT array and the patient coordinate system is stored in the header of the DICOM files and that meta-image format preserves the data in its header itself. This metadata allows us to construct the information from X,Y,Z to I,R,C. The raw data contains many other similar metadata but as we do not have use for them right now we won't be looking into it.\n",
    "\n",
    "    ##### CT scan Shape and Voxel size\n",
    "    1. One of the most common variations between CT scans is the size of the voxels; typically they are not cubes. Usually the row and column voxel sizes are the same but the index dimension has a larger value, but other ratios can exist.\n",
    "    2. When plotted using square pixels the non-cubic voxels can look somewhat distorted. We will need to apply a scaling factor if we want the image to depict realistic proportions.\n",
    "    3. Knowing these kinds of details would help when we want to depict our data and interpret our results visually. without this information it would be easy to assume that something was wrong with our data loading, and that the data might be looking squat because we omitted some slices at the time of loading. It can be easy to waste a long time debugging something that was working all along and being familiar with your data can help that.\n",
    "    4. CT's are commonly 512x512 with the index dimension ranging from 100 - 250 slices(250 slices into 2.5 mm would be enough for the anatomical region of interest). This results in a lower bound size of about 2^25 pixels or about 32 million data points. Each CT specifies the voxel size in millimeters as part of the file metadata.\n",
    "\n",
    "    ##### Converting between millimeters and voxel adresses\n",
    "    1. We would define the code to assist between the conversion of patient coordinates(in millimeters) and the IRC corrdinate system.\n",
    "    2. We might also wonder that the SimpleITK library comes up with utility functions to convert these, and indeed the `Image` instance does feature two mehtods - `Transform-IndexToPhysicalPoint` and `TransformPhysicalPointToIndex` ---> to do just that(except shuffling from CRI (column, row, index) to IRC.) However, we want to be able to do this computation without keeping the Image object around, so we'll perform the math manually here.\n",
    "    3. Flipping the axes(and potentially a rotation and other transforms) is encoded in a 3x3 matrix returned as a tuple from `ct_mhd.GetDirections()`. To go from voxel indices to coordinates, we need to follow these 4 steps in order.\n",
    "        1. Flip the coordinates from IRC to CRI, to align with XYZ.\n",
    "        2. Scale the indices with the voxel size.\n",
    "        3. Matrix-Multiply with the directions matrix using @ in Python.\n",
    "        4. Add the offset for the origin.\n",
    "\n",
    "    4. To go back from XYZ to IRC, we need to perform the inverse of each step in the reverse order.\n",
    "    5. We keep the voxel sizes in named tuples, so we convert these into arrays.\n",
    "\n",
    "    ##### Extracting a Nodule from a CT Scan\n",
    "    1. As we mentioned that 99.9999% of the voxels in the CT Ct scan of the patient with a lung cancer nodule won't be a part of the actual nodule.\n",
    "    2. So what we would do is that we would we will extract an area around each candidate and let the model focus on one candidate at a time. Looking for ways to reduce the scope of our problem for our model can help, especiallly in the early stages of a project when we are trying to get our first working implementation up and running.\n",
    "    3. The getRawNodule() function takes the center expressed in the patient coordinate system(x,y,z), just as it is specified in the LUNA CSV data, as well width in voxels. It returns a cubic chunk of CT, as well as the center of the candidate coordinates.\n",
    "    4. The actual implementation will need to deal with situations where the combination of center and width puts the edges of the cropped areas outside of the array. But as noted earlier, we will skip complications that obscure the larger intent of the function.The full implementation can be found in the books github repo.\n",
    "\n",
    "\n",
    "#### A straightforward Dataset implementation\n",
    "1. Now to load the data and to conform it with pytorch we need to implement the dataset class. By subclassing the Dataset, we will take our arbitrary data and plug it into the rest of the Pytorch Ecosystem.\n",
    "2. Each CT instance represents hundereds of different samples that we can use to train our model or validate its effectiveness.\n",
    "3. Our lunaDataset class will normalize those samples, flattening each CT's nodules into a single collection from which samples can be retrieved without regard for which CT instance the sample originates from. This flattenning is often how we want to process the data(we will see different methods also).\n",
    "4. In terms of implementation, we are going to start with the requirements imposed from subclassing dataset and work backwards. This is different from the datasets we have encountered earlier; there we we using classes provided by external libraries, whereas here we need to implement and instantiate the class oureselves.\n",
    "5. Pytorch API only requires the dataset class to have two functions:\n",
    "    1. An implementation of the `__len__` that must return a single, constant value after initialization(the value ends up being cached in some cases)\n",
    "    2. The `__getitem__` method, which takes an index and returns a tuple with sample data to used for training(or validation, as the case may be). It should return a something valid for all inputs from 0 to N-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
    "XyzTuple = collections.namedtuple('XyzTuple', ['x','y','z'])\n",
    "\n",
    "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
    "    # Swipes the order of the coordinates from I,R,C to C,R,I while we convert to a numpy array of the voxels coordinates to x,y,z coordinates\n",
    "    cri_a = np.array(coord_irc)[::-1]\n",
    "    # Converts the origin to an array\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    # converts the voxel size to an array\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "\n",
    "    # Multiplies the indices with the voxel_szie to scale them and matrix multiply with the directions so that it is captured aacordingly in each axis and offset the origin.\n",
    "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
    "    return XyzTuple(*coords_xyz)  # Convert it into a named tuple and return it\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coord_a =  np.array(coord_xyz)\n",
    "\n",
    "    # Inverse  of the last 3 steps\n",
    "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a))/vxSize_a\n",
    "\n",
    "    # rounds off before converting to integers\n",
    "    cri_a = np.round(cri_a)\n",
    "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0])) # Converts from shape C,R,I to I,R,C and converts to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'series_uid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\pytorch_practice\\part_2\\chapter_10.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 215>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/pytorch_practice/part_2/chapter_10.ipynb#X25sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m x \u001b[39m=\u001b[39m GetCandidateInfoList(requireOnDisk_bool\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/pytorch_practice/part_2/chapter_10.ipynb#X25sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m new_dataset_instance \u001b[39m=\u001b[39m LunaDataset()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/pytorch_practice/part_2/chapter_10.ipynb#X25sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m new_ct \u001b[39m=\u001b[39m Ct()\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'series_uid'"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@functools.lru_cache(1)\n",
    "def GetCandidateInfoList(requireOnDisk_bool = True):\n",
    "    candidate_info_tuple = namedtuple('candidate_info_tuple', ['isNodule_bool', 'diameter_mm', 'series_uid','center_xyz'])\n",
    "    # get a list of all the .mhd files in the various subsets of the data.\n",
    "    mhd_list = glob.glob(\"E://data/data-unversioned/subset*/*.mhd\")\n",
    "\n",
    "    # We are splitting the whole filepath into its various pieces, taking only the filename(hence the -1) and removing the .mhd from it(hence the -4). \n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    # here we will define a diameter dict to keep track of distance between individual nodules.\n",
    "    # If they are really close, treat them as a single nodule and save their series_uid along with their diameter, else 0 diameter\n",
    "    diameter_dict = {}\n",
    "\n",
    "    with open(\"E://data/data-unversioned/annotations.csv\", 'r') as f:\n",
    "\n",
    "        for row in list(csv.reader(f))[1:]:  # starting from the first row becuse row 0 is just column names(headers)\n",
    "            # get the series_uid of the nodule which is the first value of the row\n",
    "            series_uid = row[0]\n",
    "\n",
    "            # get the x,y,z coordinates(which are the 2,3,4 values respectively) which mark the center of the nodules.\n",
    "            annotation_center_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            # Get the diameter of the nodules which is the 5th value of each row\n",
    "            annotation_diameter_mm = float(row[4])\n",
    "\n",
    "            # Set the default of the key i.e series_uid as an empty list and this also returns the value of that key.\n",
    "            # Then append a tuple of the x,y,z coordinates of the center of the nodule and the diameter of the annotated nodule\n",
    "            diameter_dict.setdefault(series_uid, []).append((annotation_center_xyz, annotation_diameter_mm))\n",
    "\n",
    "    # Now  we will build a full list of candidates nodules using the information in candidates.csv file\n",
    "    candidate_info_list = []\n",
    "    with open(\"E://data/data-unversioned/candidates.csv\", 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "\n",
    "            # Get the series_uid\n",
    "            series_uid = row[0]\n",
    "\n",
    "            # If the series_uid is not present on disk and the requireOnDisk attribute is set to True then skip the file\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "            \n",
    "            # Put if the candidate is a nodule or not into the isNodule_bool parameter. This is the 5th value of the row\n",
    "            isNodule_bool = bool(int(row[4]))\n",
    "\n",
    "            # Get the x,y,z coordinates of the center of the candidate\n",
    "            candidate_center_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            # set the candidate diameter to 0.0 \n",
    "            candidate_diameter_mm = 0.0\n",
    "\n",
    "            # loop over the annotation_tuple dictionary and get the annotation tuple of the matching series_uid of the candidate\n",
    "            for annotation_tuple in diameter_dict.get(series_uid, []):\n",
    "                # get the x,y,z center values of the dictionary and the annotation_diameter of the annotated tuple\n",
    "                # print(diameter_dict.get(series_uid, []), annotation_tuple)\n",
    "                # print(type(annotation_tuple), len(annotation_tuple))\n",
    "                annotation_center_xyz, annotation_diameter_mm = annotation_tuple\n",
    "\n",
    "                # Now this loops over the x,y,z coodinates and finds the absolute distance between the centers of annotated and candidate nodules.\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidate_center_xyz[i] - annotation_center_xyz[i])\n",
    "\n",
    "                    # if delta is > annotation_diameter/4\n",
    "                    # annotation_diameter/4 --> Divides it by 2 to get diameter, again divides it by 2 to get the radius and then compares.\n",
    "                    # This is done to make sure that the centers are not too far apart relative to the size of the nodule.\n",
    "                    # This is a type of a bounding box check and not a distance check.\n",
    "                    if delta_mm > annotation_diameter_mm /4:\n",
    "                        # If the candidates and the annotations are not close then break and add them to the candidate list as seperate nodules\n",
    "                        break\n",
    "                else:\n",
    "                    # If they are very close then we should see them as the same nodule\n",
    "                    candidate_diameter_mm = annotation_diameter_mm\n",
    "                    break\n",
    "\n",
    "            # Covert all the in information into a tuple and append it to the candidate list\n",
    "            candidate_info_list.append(candidate_info_tuple(isNodule_bool,\n",
    "                                                            candidate_diameter_mm,\n",
    "                                                            series_uid,\n",
    "                                                            candidate_center_xyz))\n",
    "\n",
    "    # Sort the list in ascending/descending order to make the sampling representative of the dataset.\n",
    "    candidate_info_list.sort(reverse=True)\n",
    "    return candidate_info_list\n",
    "\n",
    "class Ct():\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(f\"E://data/data-unversioned/subset*/{series_uid}.mhd\")[0]\n",
    "        \n",
    "        # The readImage automatically consumes the .raw file in addition to the .mhd file passed in\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "\n",
    "        # Recreates an np.array since we want to convert the value type to np.float32\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype = np.float32)\n",
    "        ct_a.clip(-1000,1000,ct_a)\n",
    "\n",
    "        # All of the values we have built above are now assigned to self to make them the attributes of the object\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "        \n",
    "        # Get the origin values from the .mhd file which would be used to convert from x,y,z coordinates to the i,r,c coordinates\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "\n",
    "        # Get the size of the voxel which needs to be multiplied to every axis to get the correct voxel length from xyz to irc of each axis\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "\n",
    "        # converts the directions to an array, and reshapes the nine-element array to 3,3\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3,3)\n",
    "        # These are the inputs we need to pass into our xyz2irc conversions In addition to the individual point to convert.\n",
    "        # with these attributes, our CT object implementation now has all the data needed to convert a candidate center from patient coordinates to array coordinates.\n",
    "    \n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        \"\"\" This function crops the relevant voxels(bounding box of the nodule) from the full CT scan array.\n",
    "            It does so by converting from x,y,z to i,r,c of the center of the nodule and then using the width to calculate the\n",
    "            start and end indexes of the crop box.  The width_irc is a fixed 3D width(which resembles our input shape) with which\n",
    "            the CT scan will need to be cropped so that the center of the cropped array and the nodule are aligned.\"\"\"\n",
    "        center_irc = xyz2irc(\n",
    "            coord_xyz = center_xyz, # center corrdinates of the ct_array\n",
    "            origin_xyz = self.origin_xyz, # origin information of the ct_array\n",
    "            vxSize_xyz = self.vxSize_xyz, # voxel_size of the ct array\n",
    "            direction_a = self.direction_a # direction matrix of the ct_array\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            # Find the start and the end indexes for every axis\n",
    "            start_ndx = int(round(center_val-width_irc[axis]/2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            # Append the start and the end indexes for every axis onto a list\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "        \n",
    "        # Now crop the CT scan with the start and end indexes list to get the cropped 3D CT array which has the nodule clearly centered\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, center_irc   # Return the CT chunk and the I,R,C corrdinates of the center.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "@functools.lru_cache(1, typed=True)\n",
    "def getCt(series_uid):\n",
    "    return Ct(series_uid)\n",
    "\n",
    "@functools.lru_cache(typed=True)\n",
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    return ct_chunk, center_irc\n",
    "\n",
    "class LunaDataset(Dataset):\n",
    "    def __init__(self, val_stride = 0, is_val_set_bool = None, series_uid = None):\n",
    "        self.candidateInfo_list = copy.copy(GetCandidateInfoList())\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [x for x in self.candidateInfo_list if x.series_uid == series_uid]\n",
    "\n",
    "        if is_val_set_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        candidate_info_tup = self.candidateInfo_list[ndx]\n",
    "        print(candidate_info_tup)\n",
    "        # assert isinstance(candidate_info_tup, collections.namedtuple)\n",
    "        width_irc = (32,48,48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(\n",
    "            candidate_info_tup.series_uid, \n",
    "            candidate_info_tup.center_xyz,\n",
    "            width_irc\n",
    "        )\n",
    "\n",
    "        candidate_t = torch.from_numpy(candidate_a)\n",
    "        candidate_t = candidate_t.to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0)\n",
    "\n",
    "        pos_t = torch.tensor([\n",
    "            not candidate_info_tup.isNodule_bool,\n",
    "            candidate_info_tup.isNodule_bool\n",
    "        ],\n",
    "        dtype = torch.long)\n",
    "\n",
    "        return (candidate_t, \n",
    "                pos_t,\n",
    "                candidate_info_tup.series_uid,\n",
    "                torch.tensor(center_irc))\n",
    "x = GetCandidateInfoList(requireOnDisk_bool=False)\n",
    "new_dataset_instance = LunaDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the whole code for the util.py file in the github repo.\n",
    "This will be a bit heavy on the logic side. Just remember that we need to convert and use the functions as a black box. The metadata we need to convert from patient coordinates(_xyz) to array_coordinates(_irc) is contained within the MetaIO file alongside the CT data itself. We pull the voxel sizing and positioning metadata out of the .mhd file at the same time we get the ct_a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_info_tuple(isNodule_bool=True, diameter_mm=11.36963426, series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.249404938669582150398726875826', center_xyz=(131.415682197, 32.7486662007, -174.02278535))\n",
      "(tensor([[[[-848., -828., -780.,  ...,  -20.,   61.,   79.],\n",
      "          [-797., -817., -816.,  ...,    7.,   68.,   95.],\n",
      "          [-793., -823., -832.,  ...,   45.,   73.,   71.],\n",
      "          ...,\n",
      "          [-820., -814., -805.,  ...,    7.,   16.,    8.],\n",
      "          [-820., -836., -839.,  ...,   -2.,    5.,  -31.],\n",
      "          [-852., -838., -840.,  ...,  -10.,  -30.,  -54.]],\n",
      "\n",
      "         [[-787., -799., -815.,  ...,  -53.,   14.,   46.],\n",
      "          [-729., -771., -802.,  ...,  -32.,   29.,   52.],\n",
      "          [-732., -780., -845.,  ...,  -45.,   21.,   37.],\n",
      "          ...,\n",
      "          [-793., -846., -828.,  ...,  -14.,   -1.,   29.],\n",
      "          [-768., -805., -842.,  ...,  -60.,  -40.,  -26.],\n",
      "          [-766., -798., -807.,  ...,  -38.,    1.,  -39.]],\n",
      "\n",
      "         [[-811., -801., -826.,  ...,  -84.,  -39.,   24.],\n",
      "          [-818., -817., -824.,  ...,  -69.,  -10.,   27.],\n",
      "          [-825., -834., -853.,  ...,  -77.,   -2.,   29.],\n",
      "          ...,\n",
      "          [-860., -843., -831.,  ...,  -20.,  -44.,  -10.],\n",
      "          [-851., -868., -825.,  ...,  -28.,  -35.,    9.],\n",
      "          [-848., -850., -821.,  ...,  -16.,  -23.,  -20.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-885., -856., -822.,  ...,   80.,   78.,  143.],\n",
      "          [-888., -857., -827.,  ...,   37.,   18.,   40.],\n",
      "          [-875., -848., -817.,  ...,    4.,  -27.,  -24.],\n",
      "          ...,\n",
      "          [-833., -825., -847.,  ...,    2.,    6.,   35.],\n",
      "          [-842., -844., -834.,  ...,  -18.,    7.,   11.],\n",
      "          [-859., -855., -848.,  ...,   -2.,   33.,   18.]],\n",
      "\n",
      "         [[-828., -806., -788.,  ...,  583.,  423.,  478.],\n",
      "          [-834., -803., -758.,  ...,  480.,  379.,  469.],\n",
      "          [-862., -844., -806.,  ...,  353.,  324.,  408.],\n",
      "          ...,\n",
      "          [-809., -808., -827.,  ...,   -6.,   27.,  106.],\n",
      "          [-759., -790., -790.,  ...,  -37.,  -20.,   22.],\n",
      "          [-755., -752., -765.,  ...,  -12.,   31.,   29.]],\n",
      "\n",
      "         [[-348., -455., -564.,  ...,  276.,  190.,  245.],\n",
      "          [-464., -618., -737.,  ...,  324.,  262.,  325.],\n",
      "          [-720., -830., -876.,  ...,  391.,  307.,  380.],\n",
      "          ...,\n",
      "          [-837., -835., -846.,  ...,   18.,   74.,   72.],\n",
      "          [-784., -792., -799.,  ...,  -24.,   24.,   51.],\n",
      "          [-781., -811., -818.,  ...,   28.,   52.,   44.]]]]), tensor([0, 1]), '1.3.6.1.4.1.14519.5.2.1.6279.6001.249404938669582150398726875826', tensor([ 68, 300, 405]))\n",
      "55107\n"
     ]
    }
   ],
   "source": [
    "c = LunaDataset(is_val_set_bool= True, val_stride=10)\n",
    "print(c.__getitem__(25))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_info_tuple(isNodule_bool=True, diameter_mm=23.57156231, series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.220596530836092324070084384692', center_xyz=(-83.80648889, 247.1206328, -508.1281928))\n",
      "551065\n"
     ]
    }
   ],
   "source": [
    "p = new_dataset_instance.__getitem__(25)\n",
    "p\n",
    "print(len(new_dataset_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 32.27003025\n",
      "100 : 17.74608206\n",
      "200 : 12.98553245\n",
      "300 : 9.953169615\n",
      "400 : 8.222297439\n",
      "500 : 7.013205598\n",
      "600 : 6.256102138\n",
      "700 : 5.687246746\n",
      "800 : 5.118324575\n",
      "900 : 4.66199491\n",
      "1000 : 3.973281304\n",
      "1100 : 0.0\n",
      "1200 : 0.0\n",
      "1300 : 0.0\n"
     ]
    }
   ],
   "source": [
    "positive_info_list = [y for y in x if y[0] == True]\n",
    "# print(positive_info_list)\n",
    "positive_diameter = [x[1] for x in positive_info_list]\n",
    "for i, diameter in  enumerate(positive_diameter):\n",
    "    if i%100 == 0:\n",
    "        print(f\"{i} : {diameter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_info_tuple(isNodule_bool=True, diameter_mm=23.57156231, series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.220596530836092324070084384692', center_xyz=(-83.80648889, 247.1206328, -508.1281928))\n",
      "(tensor([[[[-766., -774., -788.,  ..., -811., -866., -854.],\n",
      "          [-769., -816., -819.,  ..., -847., -805., -802.],\n",
      "          [-754., -762., -815.,  ..., -847., -810., -796.],\n",
      "          ...,\n",
      "          [ 251.,  211.,  199.,  ..., -707., -736., -697.],\n",
      "          [ 637.,  521.,  527.,  ..., -755., -693., -689.],\n",
      "          [ 726.,  773.,  793.,  ..., -643., -600., -540.]],\n",
      "\n",
      "         [[-747., -727., -739.,  ..., -801., -849., -805.],\n",
      "          [-768., -776., -772.,  ..., -830., -830., -770.],\n",
      "          [-758., -775., -778.,  ..., -815., -794., -784.],\n",
      "          ...,\n",
      "          [  44.,   12.,   21.,  ..., -720., -755., -719.],\n",
      "          [ 291.,  177.,  181.,  ..., -777., -769., -708.],\n",
      "          [ 444.,  483.,  449.,  ..., -589., -511., -426.]],\n",
      "\n",
      "         [[-798., -782., -741.,  ..., -756., -798., -799.],\n",
      "          [-809., -786., -760.,  ..., -814., -818., -775.],\n",
      "          [-804., -793., -773.,  ..., -749., -761., -768.],\n",
      "          ...,\n",
      "          [  15.,  -14.,  -47.,  ..., -724., -729., -721.],\n",
      "          [  95.,   58.,   67.,  ..., -711., -745., -663.],\n",
      "          [ 133.,  186.,  196.,  ..., -547., -457., -336.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-781., -751., -744.,  ..., -808., -796., -776.],\n",
      "          [-726., -664., -668.,  ..., -722., -725., -783.],\n",
      "          [-743., -709., -691.,  ..., -747., -752., -816.],\n",
      "          ...,\n",
      "          [ -25.,  -27.,   -6.,  ...,  251.,  151.,   10.],\n",
      "          [ -74.,  -78.,  -42.,  ...,  119.,  119.,   90.],\n",
      "          [ -27.,    7.,   11.,  ...,   87.,  108.,  136.]],\n",
      "\n",
      "         [[-763., -786., -789.,  ..., -791., -769., -806.],\n",
      "          [-762., -753., -782.,  ..., -754., -761., -804.],\n",
      "          [-788., -782., -802.,  ..., -787., -789., -811.],\n",
      "          ...,\n",
      "          [ -38.,  -81., -104.,  ...,  195.,  163.,  109.],\n",
      "          [ -45.,  -66.,  -79.,  ...,   94.,   82.,   49.],\n",
      "          [  -3.,   15.,   20.,  ...,  200.,  164.,  108.]],\n",
      "\n",
      "         [[-784., -786., -802.,  ..., -773., -770., -802.],\n",
      "          [-783., -799., -823.,  ..., -810., -770., -739.],\n",
      "          [-820., -837., -838.,  ..., -825., -796., -743.],\n",
      "          ...,\n",
      "          [ -20.,  -28.,  -37.,  ...,  308.,  259.,  268.],\n",
      "          [ -28.,  -36.,  -31.,  ...,  327.,  254.,  219.],\n",
      "          [ -31.,    3.,  -22.,  ...,  607.,  502.,  383.]]]]), tensor([0, 1]), '1.3.6.1.4.1.14519.5.2.1.6279.6001.220596530836092324070084384692', tensor([245, 355, 154]))\n"
     ]
    }
   ],
   "source": [
    "from dataset import LunaDataset\n",
    "b = LunaDataset()\n",
    "print(b.__getitem__(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_info_tuple(isNodule_bool=True, diameter_mm=11.36963426, series_uid='1.3.6.1.4.1.14519.5.2.1.6279.6001.249404938669582150398726875826', center_xyz=(131.415682197, 32.7486662007, -174.02278535))\n",
      "(tensor([[[[-848., -828., -780.,  ...,  -20.,   61.,   79.],\n",
      "          [-797., -817., -816.,  ...,    7.,   68.,   95.],\n",
      "          [-793., -823., -832.,  ...,   45.,   73.,   71.],\n",
      "          ...,\n",
      "          [-820., -814., -805.,  ...,    7.,   16.,    8.],\n",
      "          [-820., -836., -839.,  ...,   -2.,    5.,  -31.],\n",
      "          [-852., -838., -840.,  ...,  -10.,  -30.,  -54.]],\n",
      "\n",
      "         [[-787., -799., -815.,  ...,  -53.,   14.,   46.],\n",
      "          [-729., -771., -802.,  ...,  -32.,   29.,   52.],\n",
      "          [-732., -780., -845.,  ...,  -45.,   21.,   37.],\n",
      "          ...,\n",
      "          [-793., -846., -828.,  ...,  -14.,   -1.,   29.],\n",
      "          [-768., -805., -842.,  ...,  -60.,  -40.,  -26.],\n",
      "          [-766., -798., -807.,  ...,  -38.,    1.,  -39.]],\n",
      "\n",
      "         [[-811., -801., -826.,  ...,  -84.,  -39.,   24.],\n",
      "          [-818., -817., -824.,  ...,  -69.,  -10.,   27.],\n",
      "          [-825., -834., -853.,  ...,  -77.,   -2.,   29.],\n",
      "          ...,\n",
      "          [-860., -843., -831.,  ...,  -20.,  -44.,  -10.],\n",
      "          [-851., -868., -825.,  ...,  -28.,  -35.,    9.],\n",
      "          [-848., -850., -821.,  ...,  -16.,  -23.,  -20.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-885., -856., -822.,  ...,   80.,   78.,  143.],\n",
      "          [-888., -857., -827.,  ...,   37.,   18.,   40.],\n",
      "          [-875., -848., -817.,  ...,    4.,  -27.,  -24.],\n",
      "          ...,\n",
      "          [-833., -825., -847.,  ...,    2.,    6.,   35.],\n",
      "          [-842., -844., -834.,  ...,  -18.,    7.,   11.],\n",
      "          [-859., -855., -848.,  ...,   -2.,   33.,   18.]],\n",
      "\n",
      "         [[-828., -806., -788.,  ...,  583.,  423.,  478.],\n",
      "          [-834., -803., -758.,  ...,  480.,  379.,  469.],\n",
      "          [-862., -844., -806.,  ...,  353.,  324.,  408.],\n",
      "          ...,\n",
      "          [-809., -808., -827.,  ...,   -6.,   27.,  106.],\n",
      "          [-759., -790., -790.,  ...,  -37.,  -20.,   22.],\n",
      "          [-755., -752., -765.,  ...,  -12.,   31.,   29.]],\n",
      "\n",
      "         [[-348., -455., -564.,  ...,  276.,  190.,  245.],\n",
      "          [-464., -618., -737.,  ...,  324.,  262.,  325.],\n",
      "          [-720., -830., -876.,  ...,  391.,  307.,  380.],\n",
      "          ...,\n",
      "          [-837., -835., -846.,  ...,   18.,   74.,   72.],\n",
      "          [-784., -792., -799.,  ...,  -24.,   24.,   51.],\n",
      "          [-781., -811., -818.,  ...,   28.,   52.,   44.]]]]), tensor([0, 1]), '1.3.6.1.4.1.14519.5.2.1.6279.6001.249404938669582150398726875826', tensor([ 68, 300, 405]))\n"
     ]
    }
   ],
   "source": [
    "c = LunaDataset(is_val_set_bool=True, val_stride = 10)\n",
    "print(c.__getitem__(25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
