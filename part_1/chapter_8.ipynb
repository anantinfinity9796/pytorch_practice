{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8 : Using Convolutions to Generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutions in action\n",
    "Convolution operations can be used to for solving the of translational variance limitation of fully connected networks because they provide two things\n",
    "1. They provide a method of keeping the localised relationships betweem the pixels of an image. Just like a fully connected network.\n",
    "2. They also can detect if the same localized realtionships occur in different parts of the image, without needing to relearn the relationship i.e `translational invariance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will se some convolutions in action. The torch.nn module provides convolutions in -->\n",
    "1. 1D: Time-Series\n",
    "2. 2D: For images\n",
    "3. 3D: For volumes or videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our CIFAR10 we will resort to nn.Conv2D, at minimum we will provide to the Conv2D function are\n",
    "1. Number of input features(or channels since we are dealing with multichannel images i.e more that one value per pixel)\n",
    "2. The number of output features.\n",
    "3. The size of the kernel.\n",
    "For instance in the first convolutional module we would have 3 input features per pixel (or channels) for our input image of RGB channels and an arbitrary number of channels in the output(lets day 16). The more channels in the output the more the capcity of the network and the network can learn more features. Also because we are randomly initializing, some of the features that we will get will be useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets define a Conv2D layer with 3 input channels and 16 output channels with a kernel size of 3x3.\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size = 3)  # if we give a kerne_size=3 it is the same as a 3x3 kernel\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what should be the shape of the weight tensor. As our kernel_size = 3 so the weight would be a 3x3 for a single input channel. As the number of input channels is 3 the weight would be `in_ch*3*3`. An then the number of these individual `in_ch*3*3` weights is the number of output channels i.e `16*in_ch*3*3`. The bias would be a 16 valued vector that conforms to the number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets test out the weight vector and the bias vectors\n",
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see how convolutional Networks are good for images, we have smaller models which look for local patterns whose weights are optimized for the whole image.\n",
    "2. A 2D convolution produces a 2D image as an output whose pixels are the locally weighted sum pixels of the original image.\n",
    "3. Because the weights are initialized randomly we will not see a meaningful output image.\n",
    "4. As usual we need to add a zeroth batch dimension in the input data because pytorch expects a batch of images with `B x C x H x W` shaped tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get an image from the cifar 10 dataset\n",
    "\n",
    "# download the cifar10 dataset from torchvision.datasets\n",
    "cifar10 = datasets.CIFAR10(\"./datasets/CIFAR_10/\", download = False, transform=transforms.ToTensor(), train = True)\n",
    "cifar10_val = datasets.CIFAR10(\"./datasets/CIFAR_10/\", download = False, transform = transforms.ToTensor(), train = False)\n",
    "\n",
    "# stack the images to get the mean and the standard deviations of all channels\n",
    "images = torch.stack([img for img,_ in cifar10], dim = 3)\n",
    "image_reshape = images.view(3, -1) # We give the first dimension here and the rest are figured out according to the shape.\n",
    "images_mean = image_reshape.mean(dim = 1)  # The dim=1 means that we are taking the mean along the first dimension i.e adding every column value\n",
    "images_std = image_reshape.std(dim=1)\n",
    "\n",
    "# get the tranformed cifar10 dataset with totensor() and normalize transformation applied\n",
    "transformed_cifar10 = datasets.CIFAR10(\"./datasets/CIFAR_10/\",\n",
    "                                        download= False,\n",
    "                                        train = True,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(images_mean, images_std)]))\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(\"./datasets/CIFAR_10/\",\n",
    "                                        download= False,\n",
    "                                        train = False,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(images_mean, images_std)]))\n",
    "\n",
    "# get only the classes of birds and planes\n",
    "label_map = {0:0, 2:1}  # Birds are the zero class in  the dataset and 2 is the class number for planes.\n",
    "class_names = ['airplanes', 'birds']\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label == 0 or label == 2]\n",
    "cifar_2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label == 0 or label == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32]) torch.Size([1, 16, 30, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25b43e74850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXM0lEQVR4nO2dXYyeZZnHf9eUFkuLQlv6Qb+/RJYiiJVsAm7YGA1LTNADjRwYNjFbDyTRxMQ17oEcko0f8YCY1IWIG9ePRI0c6K5KTIgnQMFSWkoRSqHTTjuUFpjWaaHttQfz1ox17v89vDPzvrN7/3/JZGbea57nvt/7ff7zvM/7f67riszEGPP/n4F+T8AY0xssdmMawWI3phEsdmMawWI3phEsdmMa4ZKpbBwRtwPfAeYA/5GZ96m/X7hwYS5atGjC2JkzZ+RYZ8+eLcbmzJlTjJ0/f74YGx0dlWO+9dZbXY35rne9q6vtACKiGDt37lwxpuaqtgMYGCj/z58pa7bb51lj/vz5xZg6htSYap+g1089TxVT+wS45JKJpXvixAlOnTo14Y67FntEzAHuBz4KDAJPRMTDmflsaZtFixbxla98ZcLY/v375XjDw8PF2BVXXFGM/fnPfy7Gdu3aJcc8ePBgMfbud7+7GLvmmmuKMTVXgLlz5xZjb7zxRjGm5vr666/LMRcuXFiMKYGof6S1+KWXXlqMqedZ++dzww03FGPHjh3raswtW7bIMdVzUa/nvHnzirHLLrtMjrl48eIJH7///vuL20zlbfzNwAuZuT8z3wJ+DNw5hf0ZY2aQqYh9JTD+dDLYecwYMwuZitgnui74m/dYEbEtInZExI6TJ09OYThjzFSYitgHgdXjfl8FHL74jzJze2Zuzcyt6trQGDOzTEXsTwCbI2J9RMwDPgM8PD3TMsZMN11/Gp+ZZyPiHuB/GLPeHszMPWqbU6dO8fjjj08YW758uRxv06ZNxZiyndR26pNSGJtvibfffrsYU59gK3egNubp06eLMWXplWyaC6hP69W2CxYskPsdHBwsxtT6XXnllcXYqlWr5JjXXXddMabcF2X91j4ZVw7L5ZdfXoyp17NkUV9A2XYlpuSzZ+avgF9NZR/GmN7gO+iMaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhGmJL19k45ffo0e/ZMbMXXMqjWrFlTjB05cqQYe8973lOMXX/99XLMEydOFGMqy0z5pyMjI3JMlUGlUL5rzWdX6Z0qXbeW+qn2q3xt5fvXst66vS+glEUGeg1A3zOg0qiVl66yKkFn6ZXwmd2YRrDYjWkEi92YRrDYjWkEi92YRrDYjWmEnlpvc+bMKVphteqyqiClsp1UdZxly5bJMVV6rKr+qcZ88cUX5ZjKOlq3bl0xpiyeWoWgmaqUq9KW1fopG/a5556TY/76178uxpQlunnz5mKsZl2qtVdp1Cqm0rahvvYT4TO7MY1gsRvTCBa7MY1gsRvTCBa7MY1gsRvTCD213ubNm8fq1asnjNUqeKpMKFXNdd++fcWYquQK2nZSNpjKBlu6dKkcU1lzyuJRVozqKQbd93OrVThVVVdVBdm1a9cWY7WKtuq5qPVTVX9rvfLUcaLWYCrNLWtZohPhM7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIU7LeIuIAMAKcA85m5lb19wMDA0XrRBWGBF2AT2UzKeutVPzyAsrOUg37NmzYUIypTDrQVs2hQ4eKMVWAUM0VtI2j1r1mvamsLpVxeNtttxVjan1A22CqWKXKDKxloHU7H2UTdjumeo7T4bP/Y2Yem4b9GGNmEL+NN6YRpir2BH4TEU9GxLbpmJAxZmaY6tv4WzLzcEQsBX4bEc9l5qPj/6DzT2AbwMKFC6c4nDGmW6Z0Zs/Mw53vw8AvgJsn+Jvtmbk1M7fWOogYY2aOrsUeEQsi4vILPwMfA3ZP18SMMdPLVN7GLwN+0bFfLgH+KzP/e1pmZYyZdroWe2buB254J9sMDAwUPd+aZ6s8b5Vu+uabbxZjqnEj6NTGoaGhYmx4eLgYK6X4XkDdb6AaAdbSdRXK81ZVYFXKKOiGh91WrVXeNMDGjRuLMXUZefTo0WKs1tjx1Vdf7WpM9Vxqr2et4u1E2HozphEsdmMawWI3phEsdmMawWI3phEsdmMaoafVZQcGBopVZGsVPFWaproNV9lytUqlq1atKsZUNVKVcqssO4CXX365GFMVWUdGRoqxY8d0UqKq7NttyijoZp1q7VXzxt/97ndyTGVdqjGVfabsvNqYykZU9lrtbtNSXI3nM7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIPbXe5syZU7Q/jh8/LrdVVpeKKVuk1jxPZSVdffXVxZjKtHvttdfkmOq5qMxAZV2qfYK2ClWmXS3zSq2Daja5f//+YuzZZ5+VY15//fUyXkLZhMrOAp01qI4/te61TLtSXB0jPrMb0wgWuzGNYLEb0wgWuzGNYLEb0wgWuzGNMGuy3pSVBdqyUlldixcvLsZqWVvKzlK2nLJUajaOsmqU1aUy/2o2jrLBVLZhbb+qOaF6PV966aViTDWaBH0sqMw29ZrVUIU1VXNL1VCzVoC1ZBWqY9pndmMawWI3phEsdmMawWI3phEsdmMawWI3phEsdmMaoeqzR8SDwMeB4czc0nlsEfATYB1wAPh0ZuouiYx5gCV/+oorrpDbKi9Y+bnK17700kvlmMqzVF6w8pBr3rRqpKiqkap03VoV3auuuqoYU/cpqLmC9v7V660871qlV/WaquNEeeWHDh2SY6pGnuo4Ua+Lur8BytWEVTrzZM7s3wduv+ixrwKPZOZm4JHO78aYWUxV7Jn5KHBxZYk7gYc6Pz8EfGJ6p2WMmW66vWZflplDAJ3vS0t/GBHbImJHROw4efJkl8MZY6bKjH9Al5nbM3NrZm5V13DGmJmlW7EfjYgVAJ3v5U8ojDGzgm7F/jBwd+fnu4FfTs90jDEzxWSstx8BtwFLImIQ+DpwH/DTiPgc8ArwqckOWErdq6X0KctKNSZUlp2KgbYxDh8+XIyp5o01602lzqrPPJT1pqw16N4Gq6Fsuy1bthRjqrFjrTqvsktVFVi17jWUvaZi6nWpXfKWjjF1jFTFnpl3FUIfqW1rjJk9+A46YxrBYjemESx2YxrBYjemESx2Yxqhp9VloWyNjI6Oyu2UNaeqdCorZv78+XJMVclUWWgqG0w1O4Tu7TVVxbR2m7KquqqywWqZWWvWrCnG1q9fX4ypdVdNHwGWL19ejKnKver4q1m06lhQlp7KeqtZb6XsPnXs+cxuTCNY7MY0gsVuTCNY7MY0gsVuTCNY7MY0Qs+ttxKlAnoXqBWHLKGaIaoChKBtMpUN1m1xTNA2oloDlflXszUHBweLMdUoccmSJXK/a9euLcbUc1GvWS1rUKFsRGXt1gprKktPWWjqNas1AC1lFKosO5/ZjWkEi92YRrDYjWkEi92YRrDYjWkEi92YRrDYjWmEnvrsAwMDRd+x5j+rVEHl2arKqTUvUzVSPHPmTDG2dGmxQY70kEH77Cr1U3m2tRRXtfavv/56MVZL1125cmUxtm/fvmJMpfLWfPZuXzPlhx8/fnH3s79GvaZXXnllMXbiRLkX6tGjR+WYJdS9BD6zG9MIFrsxjWCxG9MIFrsxjWCxG9MIFrsxjTCZxo4PAh8HhjNzS+exe4F/AS6UJf1aZv6qtq/z588XmyUqywR0SqlKT1Tph7XqqMrmUZaeagiptgNdcVTZa6q6rFoD0M9TPRdlHdW2PXjwYDGmKgIraxJ0Oq9qpKhsudrzVHaXer2VJaoq/kJ5HZRFPZkz+/eB2yd4/NuZeWPnqyp0Y0x/qYo9Mx8F9F0FxphZz1Su2e+JiF0R8WBElG8TMsbMCroV+3eBjcCNwBDwzdIfRsS2iNgRETtqt20aY2aOrsSemUcz81xmnge+B9ws/nZ7Zm7NzK21ljbGmJmjK7FHxIpxv34S2D090zHGzBSTsd5+BNwGLImIQeDrwG0RcSOQwAHg85MZbHR0lKeffnrCWM16U/aG2lZlitUy0FTjR/UuRVUjVTYh6Ay0bq3A2vNUa6SsQJURB/q5KttTPZea9aZsRlXtVu23Vl320KFDxZg6blW2psqWg7I1p2zLqtgz864JHn6gtp0xZnbhO+iMaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhG6Gl12dHRUXbv7u7+G+UfqttwlUeqqsCCrp46PDxcjG3atKkYq90yvH///mJMpYyqFNfVq1fLMVXqrOpIu2jRIrlfte2yZcuKMVVZVW0H+p4BtfbXXnttMVY7TlSlXOWzqyq6pS6tFyhVRlb3nPjMbkwjWOzGNILFbkwjWOzGNILFbkwjWOzGNEJPrbezZ89y7NixrrZVaZrdphGuWbNGjqmqiioL6IYbbijGatabSmNVqZQqnbTWwFKlm6pYLd1UWW/KdlJ26apVq+SY6lhQ1uVrr71WjKmKraCPIzWmmusbb7whxyytg6ra6zO7MY1gsRvTCBa7MY1gsRvTCBa7MY1gsRvTCD213qCcvVarutptxdapWFIq005lkq1fv74YqzXsUzZPtzZYrYGlqua6fPnyYkytD2h77fnnny/GVNZbLQNNWbuqWrCyRF988UU5ppqTyl5Ta3vkyBE5Zmlt1WviM7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIk2nsuBr4AbAcOA9sz8zvRMQi4CfAOsaaO346M8tpYozZAiVrQGV7gW50p2w5lVk0ODgox1RWzfve975iTDV9rNl9qjHhkiVLijGVtTWVIo2jo6PFWM3SUwUpn3nmmWLs+PHjxZhaA9DNJlXmpGqoqQqPQvf2rnpdahmFpeei5jKZM/tZ4MuZeS3w98AXIuLvgK8Cj2TmZuCRzu/GmFlKVeyZOZSZT3V+HgH2AiuBO4GHOn/2EPCJGZqjMWYaeEfX7BGxDvgA8BiwLDOHYOwfAqBvbTLG9JVJiz0iFgI/A76Umfoi5q+32xYROyJiR+06xBgzc0xK7BExlzGh/zAzf955+GhErOjEVwATtkjJzO2ZuTUzt6oP0owxM0tVfTH28d4DwN7M/Na40MPA3Z2f7wZ+Of3TM8ZMF5PJersF+CzwTETs7Dz2NeA+4KcR8TngFeBTMzJDY8y0UBV7Zv4BKJl3H3mnA5au26dSAVVVMVWoVErQlUGvueaaYkxdrigPHmDz5s3F2CuvvFKMPfbYY8VYzZtW/vOuXbuKsfe///1yv+o+BVU9VaWbjoyMyDFPnz5djKl7GFRqrLpXA7S3r/x7te7q2IOyR68afPoi2phGsNiNaQSL3ZhGsNiNaQSL3ZhGsNiNaYT/M9VlVdVVlR6rGjvWxtyzZ08xpiygDRs2FGO1JoG33nprMaYsHhVT6cEAL730UjGmKqtu2rRJ7nf37t3FmLIRlVU4PDzhjZp/QaX6LliwoBjrtqpvbU7K3lWVhlXFXzUnZfv6zG5MI1jsxjSCxW5MI1jsxjSCxW5MI1jsxjRCT623gYGBov2hMqRAZ4upBnnKkrrqqqu6HvPQoUPF2IkT5SK7Bw8elGOqaq6qCqzKzFLZVQBDQ0PFmMoorFmXjz/+eDGmLKKbbrqpq32CzjJT2XTqta41sFRVYlUW3hNPPFGMrV27Vo6pqhuX8JndmEaw2I1pBIvdmEaw2I1pBIvdmEaw2I1phJ5ab3PmzCnaR7XMrJUrVxZjqvjj4cOHizFliwB8+MMfLsZUdtWpU6eKsVrxwgMHDhRjqsDj+vXri7Fa8UKVmaXWXVmBoIsfrlixohi7+uqri7Ga5aQy5lRRU5UdqY4h0FlxytJTWYFHjhyRY5aOI2UT+sxuTCNY7MY0gsVuTCNY7MY0gsVuTCNY7MY0wmS6uK6OiN9HxN6I2BMRX+w8fm9EHIqInZ2vO2Z+usaYbpmMz34W+HJmPhURlwNPRsRvO7FvZ+Y3Jj3YJZewePHiCWPKdwXtn3brrdbGVCmwl112WVexmjf9wgsvFGPKC1b7ffPNN+WYqrrs6tWri7HnnntO7lc1UlTVU9V9CrV003Xr1hVjqoKsus+j1oxTNalUz0U18azdA9INk+niOgQMdX4eiYi9QPlOC2PMrOQdXbNHxDrgA8CF/sD3RMSuiHgwIvQtcMaYvjJpsUfEQuBnwJcy803gu8BG4EbGzvzfLGy3LSJ2RMQOVUXEGDOzTErsETGXMaH/MDN/DpCZRzPzXGaeB74H3DzRtpm5PTO3ZuZWVeLIGDOzTObT+AAeAPZm5rfGPT7+061PAuW7+o0xfWcyn8bfAnwWeCYidnYe+xpwV0TcCCRwAPj8DMzPGDNNTObT+D8AE5UR/dU7HWzu3LnF9EVVIRa0DabstY0bNxZjJRvwAqpKrLJjlK1Us8FUsz9lO6nmlioGOgVWVWTdu3ev3K9ah1WrVhVjZ86cKcaUlQXaXlPrp46/D33oQ3JMVWn4j3/8YzGmrN9z587JMUvH0fnz54vb+A46YxrBYjemESx2YxrBYjemESx2YxrBYjemEXre2LFkA9WyfJQ9pGwKZUWopo+1uLJ4lNVVqxqqMtCefvrpYkxZjB/84AflmLfccksxptZWVaUFik08ayibVWW1AYyMjBRjL7/8cjGmmlted911ckx1jCn7VlWlrVW0LVU3Vq+Xz+zGNILFbkwjWOzGNILFbkwjWOzGNILFbkwj9NR6g7GikxNRs8GUNafsBlUMUGXL1bZVGV1qv7VMO9XUUDWTVOunbCXQDRhVwRG1PqDnq6zLtWvXFmO14o+l4wvg4MGDxZiaa81iVM9FWaLKYlSvCZSt6IGB8vnbZ3ZjGsFiN6YRLHZjGsFiN6YRLHZjGsFiN6YRLHZjGmHWpLjWUD6oSmtUaYSqaSHo1EVV5VSl4ypvFWDDhg3FmFo75bOrCqeg72FYs2ZNMTY8PCz3qzoAqRTOnTt3FmOqKi3Ae9/73mJMNQBVVX/37dsnx5w/f35XY6oGoKrRJJSbfI61eZgYn9mNaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhGCNXsbtoHi3gVGF/icwlwrGcTqOP5aGbbfGD2zanf81mbmRP6uz0V+98MHrEjM7f2bQIX4floZtt8YPbNabbNZzx+G29MI1jsxjRCv8W+vc/jX4zno5lt84HZN6fZNp+/0NdrdmNM7+j3md0Y0yP6IvaIuD0i9kXECxHx1X7M4aL5HIiIZyJiZ0Ts6NMcHoyI4YjYPe6xRRHx24j4U+e7ToWa+fncGxGHOuu0MyLu6OF8VkfE7yNib0TsiYgvdh7vyxqJ+fRtjWr0/G18RMwBngc+CgwCTwB3ZeazPZ3IX8/pALA1M/vmj0bEPwAngR9k5pbOY/8OHM/M+zr/FK/MzH/t43zuBU5m5jd6MYeL5rMCWJGZT0XE5cCTwCeAf6YPayTm82n6tEY1+nFmvxl4ITP3Z+ZbwI+BO/swj1lFZj4KHL/o4TuBhzo/P8TYwdTP+fSNzBzKzKc6P48Ae4GV9GmNxHxmLf0Q+0pgfLX+Qfq/SAn8JiKejIhtfZ7LeJZl5hCMHVzA0j7PB+CeiNjVeZvfs8uK8UTEOuADwGPMgjW6aD4wC9ZoIvoh9olKafTbErglM28C/gn4QuctrPlbvgtsBG4EhoBv9noCEbEQ+Bnwpcwsl5fp33z6vkYl+iH2QWB8PahVQLlGUQ/IzMOd78PALxi71JgNHO1cG164RtR1oGaYzDyamecy8zzwPXq8ThExlzFh/TAzf955uG9rNNF8+r1Gin6I/Qlgc0Ssj4h5wGeAh/swDwAiYkHnAxYiYgHwMWC33qpnPAzc3fn5buCXfZzLBTFd4JP0cJ1irLjaA8DezPzWuFBf1qg0n36uUZXM7PkXcAdjn8i/CPxbP+Ywbi4bgKc7X3v6NR/gR4y97XubsXc/nwMWA48Af+p8X9Tn+fwn8AywizGRrejhfG5l7HJvF7Cz83VHv9ZIzKdva1T78h10xjSC76AzphEsdmMawWI3phEsdmMawWI3phEsdmMawWI3phEsdmMa4X8B+0Ot4ElrX40AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now lets get one image from the cifar2 dataset\n",
    "import plotly.express as px\n",
    "image, _ = cifar2[0]\n",
    "\n",
    "# Now add a batch dimension and input it in the conv2d transformation\n",
    "output = conv(image.unsqueeze(0))\n",
    "\n",
    "# Check the size of the original and output image and plot the images\n",
    "print(image.unsqueeze(0).shape, output.shape)\n",
    "\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see in the output shape that we have lost a few pixels, how did this happen:\n",
    "1. This is the effect of doing the convolution operation at the boundary, applying the convolution kernel at the position 0,0 at the image means that there is no pixel at the left side of the kernel. We only have pixels at the right and the bottom of the image.\n",
    "2. By default pytorch will slide the kernel at the position `width-kernel_width+1` horizontal and vertical positions. For odd sized kernels it results in images that are (one/half) smaller of the kernel size(3/2=1). This explains the missing two pixels in each side.\n",
    "3. However pytorch gives us the utility of padding the image with ghost pixels (which has 0 value) around the border that have a zero output value as far as convolution operation is concerned. In our case specifying the padding as 1 means that our kernel has one extra pixel to compute the output on the left side corner of the image. The net result is that the output now has the same size as the input.\n",
    "4. For even sized kernels we would have to pad with different numbers on all 4 sides. Pytorch does not offer this functionality in the convolution operation itself, but we have this funcitonality in `torch.nn.functional.pad`, it is best to stay with odd-sized kernels. Even sized kernels are just odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32]) torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25b43f6ae80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now would see the same operation with padding\n",
    "conv = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
    "\n",
    "output = conv(image.unsqueeze(0))\n",
    "print(output.shape, image.shape)\n",
    "\n",
    "plt.imshow(output[0][0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting features with convolutions\n",
    "We know that weights and biases are parameters that are learned through backpropagation. But we can play around and set our own weights and biases and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25b43fe57c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets start by making the bias as zero and removing an unescessary factor.\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "# Now lets set the weigths by initializing to a value.\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0/9.0)\n",
    "\n",
    "# we could have gone with conv.one_() which would have resulted in the output pixel value to be sum of all the neighbouring values.\n",
    "\n",
    "# Now lets see the effect this has on our convolution output image\n",
    "output = conv(image.unsqueeze(0))\n",
    "\n",
    "print(output.shape)\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now lets try anothe new kernel\n",
    "conv = nn.Conv2d(3,1, kernel_size=3, padding = 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] =  torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "\n",
    "output_vertical = conv(image.unsqueeze(0))\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] =  torch.tensor([[-1.0, -1.0, 1.0],\n",
    "                                    [0.0, 0.0, 0.0],\n",
    "                                    [1.0, 1.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "output_horizontal = conv(image.unsqueeze(0))\n",
    "\n",
    "# plot the images\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(nrows = 1,ncols = 3, sharey = False)\n",
    "ax3.imshow(output_horizontal[0,0].detach(), cmap = 'gray')\n",
    "ax3.set_title('horizontal filter')\n",
    "ax2.imshow(output_vertical[0,0].detach(), cmap='gray')\n",
    "ax2.set_title('vertical filter')\n",
    "ax1.imshow(image[0], cmap = 'gray')\n",
    "ax1.set_title('Before')\n",
    "fig.set_dpi(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by using the above kernel weights we only can make out vertical lines, similarly we have also created a filter which can sperate all the horizontal lines. By generating and applying these types of filters we can manually detect features in our image and the job of a computer vision reasercher has been to research a combination of these filters so that the features are highlighted and objects are recognized.\n",
    "##### But where does deep learning come into the picture in identifying kernels.\n",
    "The deep learning algorithm estimates these kernel weights in the manner which is the most effective(make the loss least). For e.g the successive filters from the filterbank that are applied to an image, the neural network estimates the kernel weights of the filters and the different channels of the output image correspond to the different features(one channel for horizontal, vertical edges and diagonal edges, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking further with Depth and Pooling\n",
    "We have achieved locality and translation invariance. We are using kernels which are 3x3 and 5x5, this takes care of the locality but the problem is what about the big picture. How do we know that all the objects in our picture are 3x3 or 5x5 pixels wide, we don't know because they do. This is something we will need to look into if we want to solve our problem since our images are small the objects have a span of several pixels across. There are two solutions for this problem:\n",
    "1. Using a bigger kernel the max limit for a convolutional kernel for a 32x32 image is 32x32, but using this kernel we would be like making the covolution oepration the same as the fully connected network with the same problems.\n",
    "2. The second option here is to stack one convolution after another and at the same time downsampling the image between successive convolutions.\n",
    "\n",
    "#### From large to small: Downsampling\n",
    "Downsampling could occur in multiple different ways. Scaling the image in half is equivalent to taking the four neighbouring pixels as an input and producing one pixel as the output. How we compute the values of the output is upto us. We could:\n",
    "1. Take the average of the neighbouring pixels to cumpute the output pixel. This is called as `Average Pooling`.\n",
    "2. Take the maximum of the four pixels. This approach is called as `Max Pooling` and the downside of this is that it discards 3/4 of the data.\n",
    "3. Perform a strided convolution where every nth pixel is calculated. A 3x4 convolution with a stride of 2 still incorporates the inputs of all pixels from the previous layer. It is a good approach but it still has not surpassed max pooling.\n",
    "We will focus on maxpooling. Intuitively what is happening here is that after the convolution operation the outputs are passed through the activation function which further highlights the features captured by the kernels in the image. Then what happens is that we keep the maximum of those 2x2 pixel values and ensure that the feature which was highlighted the most survives the downsampling step at the expense of other steps.\\\n",
    "Max pooling is provided by nn.Maxpool2d().It takes the input size of the neighbourhood on which to operate the pooling operation. If we wish to downsample our image by half we should use a size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) torch.Size([1, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Lets see Maxpool2d in action\n",
    "maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "output = maxpool(image.unsqueeze(0))\n",
    "print(image.shape, output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happens when we combine these two features is that, the first set of kernels which operates on a 8x8 image captures small features like edges, vertical lines etc, and then the maxpooling happens which downsamples the image so that now only the highlighted features of the image are in focus. The as we are using the same kernel size but the size of the image has been reduced the next kernel now captures more high level features of the image which are a combination of the previous features. `This combination allows the convolution network to see very complex scenes and can be used in a wide variety of tasks like time-series forecasting, NLP etc and not only in Image recognition and it can also figure out much compelx images than a 32x32 cifar10 image.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting it altogether for our Network\n",
    "Now that we have the building blocks we can put these building blocks to use and make our network which can classify the images of the birds and the planes. We will make a model that has both the convolution operation and the linear layers because the convolution operation outputs the image as multi channel 2D image but to give the input for the log-likelihood loss function as plane or a bird we need the output to be a 1D vector of probabilites.\\\n",
    "So now we need to take this multi-channel image and convert it to a 1D vector of probabilities for it to be input to the log-likelihood loss function and complete our network with a set of fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv2d_1',nn.Conv2d(3, 16, kernel_size=3, padding=1)),\n",
    "    ('Activation_1', nn.Tanh()),\n",
    "    (\"maxpooling2d_1\", nn.MaxPool2d(2)),\n",
    "    ('conv2d_2', nn.Conv2d(16,8, kernel_size = 3, padding = 1)),\n",
    "    ('Activation_2', nn.Tanh()),\n",
    "    ('Maxpooling2d_2', nn.MaxPool2d(2)),\n",
    "    # This model also has one layer here which convert the 2D image with multiple channels to a 1D vector required by the linear layer\n",
    "    (\"linear Layer_1\", nn.Linear(8*8*8, 32)),\n",
    "    (\"Activation_3\", nn.Tanh()),\n",
    "    (\"Output_layer\", nn.Linear(32,2))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the size of the first linear layer is dependent upon the output of the Maxpool2d layer i.e `8*8*8 = 512`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090 [432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "# lets check the number of parameters in the mode\n",
    "parameters = [p.numel() for p in model.parameters()]\n",
    "print(sum(parameters), parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters of the model is 18090 which is very reasonable for such small dataset of 32x32 images. In order to increase the capacity of the model we would increase the number of output channels of the convolutional layers, which would increase the linear layer size as well.\n",
    "`We put the warning text, for a reason. This model has zero chance of running`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two lines were used to run the model defined above but the model will not run because it is missing some parts which is explained below.\n",
    "# output = model(image.unsqueeze(0))\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is missing here is the conversion of the `8*8*8` image into a 512 element 1D vector. This could be achieved by calling view on the last nn.Maxpool2D, but unfortunately we do not have explicit visibilty of the output of each module when we use nn.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subclassing the nn.module\n",
    "Sometimes when building a Neural Network we stumble across a problem where we want to compute something that the premade modules do not cover. Here it is a very simple thing  like reshaping, but further along we would like to implement residual connections. So in this section we would build our own nn.Module subclass so that we can use them like the prebuilt ones with added functionality or in nn.Sequential.\n",
    "1. When we want to build models that do many other things other than only applying many layers one after another, we need to leave nn.Sequential for something that gives us additional stability. Pytorch allows us to use any computation in our model by subcalssing the nn.Module.\n",
    "2. In order to subclass the nn.module we need to define both the forward pass of the network which takes in the inputs and return the outputs, and typically we need to define the backward pass also but if we use the standard pytorch operation we do not need to define the backwards function. Pytorch takes care of the backward pass.\n",
    "3. Typically our defined computation would make use of other premade modules like the convolution or other modules. To use these submodules we typically define them in the constructor `__init__` and assign them to self for use in the forward function. Remember you need to call `super().__init__()` before you can do that(you need to import the constructor of the nn.Submodule use its predefined functionality and add your own to it. Super takes care of importing of all the functionality of the base class and you can just add the enhancement and get it working)\\\n",
    "`Note : ommiting the flatten layer was a design choice. Recently pytorch gained the nn.flatter layer(version 1.3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our network as an nn.Module\n",
    "# To do this we define all the layers we defined in our model in the constructor and then use their instances one after another in forward\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # define the layers of your custom model in the constructor\n",
    "        super().__init__()  # Makes all the other methods of nn.Module avaible for use\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding = 1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.linear1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.output = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.maxpool1(self.act1(self.conv1(x)))\n",
    "        out = self.maxpool2(self.act2(self.conv2(x)))\n",
    "        out = out.view(-1, 8*8*8)  # This is the reshaping step that we were missing earlier. reshape into a row vecotr with shape (1,8*8*8)\n",
    "        out = self.act3(self.linerar1(out))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Net model is equivalent to our model that we defined earlier in our nn.Sequential class with the adden flatten step(which was not provided in the Sequential model) but here we can manipulate the output of maxpool2 directly and can call the out.view() method on the tensor to change its shape into a BxN vector so that it can be inputted to the first fully connected layer. We leave the batch dimension as -1 because we do not know the size of the batch.\\\n",
    "Recall that the goal of the classification network is to typically compress information in the sense that we start with a sizable number of pixels and we reach an output vector of probabilites. Our goal is reflected in the design of the architecture.\n",
    "1. It is done by reducing the number of channels in the convolution operation.\n",
    "2. It is done by reducing the size of the pixels in the pooling operation.\n",
    "3. It is done by reducing the number of hidden neurons in the fully connected layers.\\\n",
    "This is being done to compress the information and make predictions on only the most important information in the whole image, and this is a common trait of classification networks, but in some other architectures only the resolution is reduces while increasing the number of channels(still resulting in a reduction in size). It seems that the pattern of fast information reduction works well with shallow networks and smaller images but is not suitable for deeper networks, but the first layer is exceptional in such a way that it greatly increases the overall dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How pytorch keeps track of parametes of user defined submodules.\n",
    "Interestingly in pytroch, `assigning an instance of the nn.Module(the linear and conv layers) to the attribute of nn.Module automatically registers the module as submodule`.\\\n",
    "`Note : The attributes of the nn.Module should be a top-level attribute, not a list or a dict of submodule instances, otherwise the optimizer will not be able to locate the paramters of the instances. For instances where your model needs a list/dict of submodules, pytorch provides nn.ModuleList and nn.ModuleDict classes`.\\\n",
    "We can define and call some arbitrary methods of nn.Module subclass. For eg. if the model has a different functionality when predciting then it will make sense to define a predict method and use that to predict. But doing so is like calling the forward method instead of the calling the module as a function by the call method(it runs without intiating the accessory methods such as hooks and the JIt does not check the model structure because we do not have `__call__`).\\\n",
    "This allows the Net to have access to the parameters of its submodules without further action by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090 [432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "# lets define the model\n",
    "model = Net()\n",
    "\n",
    "# lets get the parameters to check of our subclass model if we have the same number of parameters\n",
    "paramters = [p.numel() for p in model.parameters()]\n",
    "print(sum(paramters), parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by comparing both the models that the number of parameters is the same (which means that pytorch is discovering the parameters of all the layers) as the previous model that we defined(because we just added a flatten layer which just reshapes the tensor we did not introduce any extra parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is happening is here is that we call the parameters on the instance of the Net() class, it recursively calls the parameters() on all the instances of the submodules defined as attributes of the net class(no matter how nested), and in the same way for all the submodules the grad attribute (which has been populated by autograd) is also found and updated to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`By now we know the utility of creating our own subclass of the nn.Module and looking into the constructor of the subclass we can see that we have defined submodules as attributes so that their parameters can be accessed by pytorch at runtime, but we are also defining those submodules which do not have parameters of their own like nn.Tanh() and nn.Maxpool2d(). It would be good if instead of defining the submodules in constructor we could just call them in the forward method just as we called the view() method.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The functional API: \n",
    "The solution to the problem defined above is the pytorch `Functional API`. Every nn.Module has its functional counterpart, and by functional we mean `having no internal state (whose output is solely determined by the value of it's inputs)`. Indeed `torch.nn.functinal` provides many functions that work like modules we find in nn. But instead of working on inputs and parameters stored like the module counterparts they accept inputs and paramters as arguments to the function call. For instance `nn.Linear` has `nn.functional.linear` which has a function signature as `linear(inputs, weight, bias = None)` which indicates that it accepts the inputs and weights and biases as arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the model it seems to make sense to define the conv2d and linear layers as modules because pytorch can keep track of parameters on its own, however we can safely switch to functional counterparts for  pooling and activations since they have no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define our new model using the mix of fucntional api and the subclassing module\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        # tanh will be defined by the functional API in forward\n",
    "        # maxpool will be defined by the functional API in the forward method\n",
    "        self.conv2 = nn.Conv2d(16,8, kernel_size = 3, padding = 1)\n",
    "        # tanh will be defined by the functional API in forward\n",
    "        # maxpool will be defined by the functional API in the forward method\n",
    "        self.fc1  = nn.Linear(8*8*8, 32)\n",
    "        # tanh will be defined by the functional API in forward\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Here we would define the steps of the computation and also the tanh and maxpool layers. x is the input\"\"\"\n",
    "\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way more concise than the full definition of the class. But if the layer requires more arguments then it would be best to define it in the constructor. Thus the functional module also sheds light on what the nn.Module API is all about. A module is a container for state in the forms of parameters and submodules combined with the instructions to do a forward.\\\n",
    "Using either of the functional API's or modular API's is a matter of prefernce according to the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2459, -0.0383]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets check if out model run \n",
    "model = Net()\n",
    "image, _  = cifar2[88]\n",
    "output = model(image.unsqueeze(0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got two numbers. Then we are certain that the information flows correctly and we have got the correct number of outputs. We might realize it right now but in more complex models the getting the size of the first linear layer is sometimes a source of frustration, even famous practitioners rely on putting arbitrary values and backtracking from the output to get to the correct input shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training our Convnet\n",
    "We are not a point where we can train our convnet and see the results that it produces as compared to our linear classifier. The training loop stays the same as our linear classifier with the addition of accuracy in the mix to monitor the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the core of our convnet has two loop, one for epochs and another for the dataloader for going over the batches. Now lets define our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-10 16:28:43.519214 --> Epoch : 1 | Train Loss : 0.5349392931872663\n",
      "2022-10-10 16:29:48.247175 --> Epoch : 20 | Train Loss : 0.25915760461229104\n",
      "2022-10-10 16:31:00.092121 --> Epoch : 40 | Train Loss : 0.19767980106151142\n",
      "2022-10-10 16:32:11.808920 --> Epoch : 60 | Train Loss : 0.14398290847722714\n",
      "2022-10-10 16:33:23.212315 --> Epoch : 80 | Train Loss : 0.09548568038549572\n",
      "2022-10-10 16:34:34.248517 --> Epoch : 100 | Train Loss : 0.058353324399921844\n",
      "2022-10-10 16:35:45.325411 --> Epoch : 120 | Train Loss : 0.03499921111571498\n",
      "2022-10-10 16:36:56.069209 --> Epoch : 140 | Train Loss : 0.022086896718637394\n",
      "2022-10-10 16:38:07.206457 --> Epoch : 160 | Train Loss : 0.01486979045432073\n",
      "2022-10-10 16:39:18.780834 --> Epoch : 180 | Train Loss : 0.010584756161440343\n",
      "2022-10-10 16:40:29.568234 --> Epoch : 200 | Train Loss : 0.007932323896989654\n"
     ]
    }
   ],
   "source": [
    "# import datetime for monitoring time\n",
    "import datetime\n",
    "\n",
    "# Instantiate the model\n",
    "convnet = Net()\n",
    "\n",
    "# Initlaize the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar2, batch_size= 32, shuffle = False)\n",
    "val_dataloader = torch.utils.data.DataLoader(cifar_2_val, batch_size = 32, shuffle = False)\n",
    "\n",
    "# Intialize loss function\n",
    "cross_ent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initlaize the optimizer\n",
    "optimizer_sgd = optim.SGD(convnet.parameters(), lr = 1e-2)\n",
    "\n",
    "# define the training loop\n",
    "def training_loop(n_epochs, model, optimizer, loss_fn, train_data, val_data):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0  # Initialize the loss for each epoch\n",
    "        loss_val = 0.0\n",
    "        for image, label in train_data:\n",
    "            # carry out the forward pass\n",
    "            # This does not require us to reshape the image with batch_size because the train_loader already give batches.\n",
    "            pred = model(image)\n",
    "\n",
    "            # compute the training loss per batch\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            # compute the validation loss and the accuracy\n",
    "            # with torch.no_grad():\n",
    "            #     for val_image, val_label in val_data:\n",
    "            #         val_pred = model(val_image)\n",
    "            #         val_loss = loss_fn(val_pred, val_label)\n",
    "            #          # This transforms the val_loss from a tensor to a python number which is nescessary to escape gradients.\n",
    "            #         loss_val += val_loss.item()\n",
    "\n",
    "            # zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # carry out backward pass\n",
    "            loss.backward()\n",
    "            # carry out the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # This transforms the loss from a tensor to a python number which is nescessary to escape gradients\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        # print the training loss, validation loss, accuracy per epoch\n",
    "        if epoch == 1 or epoch %20 == 0:\n",
    "            print(f\"{datetime.datetime.now()} --> Epoch : {epoch} | Train Loss : {loss_train/len(train_data)}\")\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "training_loop(n_epochs = 200,\n",
    "             model = convnet,\n",
    "             optimizer = optimizer_sgd, \n",
    "             loss_fn=cross_ent_loss_fn, \n",
    "             train_data= train_dataloader, \n",
    "             val_data=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now we can see from the above output that the network has been trained to a very low loss, but that nescessarily does not mean that the accuracy of the network on unseen images is high also.`\\\n",
    "So we could find out the accuracy of our model by running the model against the training and the validation sets and see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy : 0.9992\n",
      "validation Accuracy : 0.8955\n"
     ]
    }
   ],
   "source": [
    "def validate(model, train_data, val_data):\n",
    "    for name, loader in [(\"train\",train_data), ('validation', val_data)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label in loader:\n",
    "                pred = model(image)\n",
    "\n",
    "                _, predicted = torch.max(pred, dim=1)\n",
    "                total += label.shape[0]\n",
    "                correct += int((predicted==label).sum())\n",
    "\n",
    "        print(f\"{name} Accuracy : {correct/total}\")\n",
    "\n",
    "validate(convnet, train_data= train_dataloader, val_data = val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats quite better than our linear model which could only achieve an accuracy of around 81% on the validation set. This achieves better performance with lesser number of parameters. Lets run it for a few more epochs and see what performance we could squeeze out.\\\n",
    "Model trained till 100 epochs --> 88.15 % val accuracy and 96.03 % train accuracy\\\n",
    "Model trained till 200 epochs --> 89.55 % val accuracy and 99.92% train accuracy\\\n",
    "Anlyzing the above sentences we can see that the model trained on 200 epochs has overfitted the data and could not generalize better even though it's loss is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading our model\n",
    "Since we are satisfied with our progress we can save our model, so that we can load the weights for inference without having to train the model again. This is very useful if we have a very large model which takes a long time to train.\n",
    "But pytorch only save the weights of the model and not the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./saved_models/\"\n",
    "torch.save(convnet.state_dict(), path+\"birds_vs_airplanes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch just saves the weights of the convnet. If we want to use the model again we would need to define the model architecture, instantiate a convnet object and load the weights into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets load the saved model\n",
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(path+\"birds_vs_airplanes.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all keys matched succesfully message shows that our model has been loaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the GPU\n",
    "We have trained the model and saved it the memory, now whenever we want to use our model we can just use the saved weights to perform inference.\n",
    "1. Training on the GPU makes the model training faster. We can use the .to() method to move our tensors of the dataloader to the GPU, after which the computation will automatically take place. But we also need to move our parameters() tensor to the GPU. The nn.Module impelements a .to() function which transfers its parameters to the GPU.\n",
    "2. There is subtle difference between `nn.module.to()` and `tensor.to()` methods. The module.to() method modifies the module instance inplace, but the tensor.to() returns a new tensor. \n",
    "3. `One good practice is to create the optimizer after moving the parameter tensors to the appropriate device.`\n",
    "4. `It is good practice to use a GPU if one is available. A good pattern is to use a GPU or CPU depending upon torch.cuda.is_available()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can update our code with the line which transfer the dataloader to the GPU using the tensor.to() method. It is a very small change, just two lines of code. We would also need to move our model instance to the GPU also and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-12 07:06:45.962364 --> Epoch : 1 | Train Loss : 0.537982549911109 | Val Loss : 0.46123253494974165\n",
      "2022-10-12 07:07:07.496024 --> Epoch : 20 | Train Loss : 0.26882070379135325 | Val Loss : 0.2892750169313143\n",
      "2022-10-12 07:07:29.329909 --> Epoch : 40 | Train Loss : 0.2077372375601968 | Val Loss : 0.2704237648655498\n",
      "2022-10-12 07:07:50.313408 --> Epoch : 60 | Train Loss : 0.15647608473801766 | Val Loss : 0.27687373851972913\n",
      "2022-10-12 07:08:11.531732 --> Epoch : 80 | Train Loss : 0.10731856414554314 | Val Loss : 0.3035459849569533\n",
      "2022-10-12 07:08:32.837458 --> Epoch : 100 | Train Loss : 0.0660671279661524 | Val Loss : 0.336388751036591\n"
     ]
    }
   ],
   "source": [
    "# import datetime for monitoring time\n",
    "import datetime\n",
    "\n",
    "# One important thing to realize is to instantiate the optimizer and the model afterdefining the training loop and moving the data to the tensor\n",
    "\n",
    "# define the training loop\n",
    "def training_loop(n_epochs, model, optimizer, loss_fn, train_data, val_data):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0  # Initialize the loss for each epoch\n",
    "        loss_val = 0.0\n",
    "        for image, label in train_data:\n",
    "            # Move the images and labels to the GPU. This is the only step which makes the model train on the GPU. \n",
    "            image = image.to(device=device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "\n",
    "            # carry out the forward pass\n",
    "            # This does not require us to reshape the image with batch_size because the train_loader already give batches.\n",
    "            pred = model(image)\n",
    "\n",
    "            # compute the training loss per batch\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            # zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # carry out backward pass\n",
    "            loss.backward()\n",
    "            # carry out the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # This transforms the loss from a tensor to a python number which is nescessary to escape gradients\n",
    "            loss_train += loss.item() \n",
    "        # compute the validation loss and the accuracy\n",
    "        with torch.no_grad():\n",
    "            for val_image, val_label in val_data:\n",
    "                # move the tensors of the loader to the GPU.\n",
    "                # This step was added after we trained once on the GPU without the validation step with a training time of 3m.\n",
    "                val_image = val_image.to(device = device)\n",
    "                val_label = val_label.to(device = device)\n",
    "                val_pred = model(val_image)\n",
    "                val_loss = loss_fn(val_pred, val_label)\n",
    "                # This transforms the val_loss from a tensor to a python number which is nescessary to escape gradients.\n",
    "                loss_val += val_loss.item()\n",
    "        # print the training loss, validation loss, accuracy per epoch\n",
    "        if epoch == 1 or epoch %20 == 0:\n",
    "            print(f\"{datetime.datetime.now()} --> Epoch : {epoch} | Train Loss : {loss_train/len(train_data)} | Val Loss : {loss_val/len(val_data)}\")\n",
    "\n",
    "\n",
    "# This step of defining the model and optimizer after the data_loader has been moved to the GPU is only applicable when using a GPU\n",
    "# Instantiate the model\n",
    "convnet = Net().to(device=device)  # The .to(device = 'cuda') move the model to the GPU.\n",
    "\n",
    "# Initlaize the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar2, batch_size= 32, shuffle = False)\n",
    "val_dataloader = torch.utils.data.DataLoader(cifar_2_val, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Intialize loss function\n",
    "cross_ent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initlaize the optimizer\n",
    "optimizer_sgd = optim.SGD(convnet.parameters(), lr = 1e-2)\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "training_loop(n_epochs = 100,\n",
    "             model = convnet,\n",
    "             optimizer = optimizer_sgd, \n",
    "             loss_fn=cross_ent_loss_fn, \n",
    "             train_data= train_dataloader, \n",
    "             val_data=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training on the GPU took around 3 min(without the validation step) while training the same thing on the CPU took around 12 min(without the validation step). This difference might not be very significant because we have a very small dataset, but it is very significant when we have a larger model and a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training with the validation set for 100 epochs was done in 3 min on the GPU. This shows how much a GPU is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy : 0.9643\n",
      "validation Accuracy : 0.8885\n"
     ]
    }
   ],
   "source": [
    "# lets check the training and the validation accuracy of the model by performing the computations on the GPU\n",
    "def validate(model, train_data, val_data):\n",
    "    for name, loader in [(\"train\",train_data), ('validation', val_data)]:\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label in loader:\n",
    "                image = image.to(device=device)\n",
    "                label = label.to(device=device)\n",
    "                pred = model(image)\n",
    "\n",
    "                _, predicted = torch.max(pred, dim=1)\n",
    "                # predicted = predicted.to(device=device)\n",
    "                total += label.shape[0]\n",
    "                correct += (predicted==label).sum().item()\n",
    "\n",
    "        print(f\"{name} Accuracy : {correct/total}\")\n",
    "\n",
    "validate(convnet, train_data= train_dataloader, val_data = val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the weights of a trained model on the GPU\n",
    "There is a slight complication when trying to load the weights of the model on the GPU. By default pytorch will load the weights on the same device that the weights were saved from i.e. `If the weights were saved from the GPU pytorch will load the weights to the GPU.` As we do not know that the GPU would be available at all times, we could move the parameters to the CPU before saving it and move them back to the GPU after loading them back.\n",
    "It is super easy to instruct pytorch to load chose the device while loading the weights. This is done by passing the `map_location` keyword argument to the torch.load()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to load the model and the parameters of our saved model directly to the GPU\n",
    "path = \"./saved_models/\"\n",
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(path+\"birds_vs_airplanes.pt\", map_location = device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Design\n",
    "We have now developed a good understanding on how to design the a CNN model by subclassing the nn.Module and implementing the forward pass of the model(pytorch takes care of the backward pass). Now we will see how to design models for more complex data in varied fields and learn how to get the basics of models and other tricks for better models in the real-world. This would help us to implement research papers from scratch and read other peoples codes.\\\n",
    "We would learn some real-world techniques of sqeezing performance like\n",
    "1. Batch-Normalization\n",
    "2. Dropout\n",
    "3. Regulariation\n",
    "4. Adding  `Capcaity(fitting more complex data)` and `Depth(better feature extraction)` to our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Memory Capacity : Width\n",
    "Given our feedforward architecture there is a number of things that we would like to look upon. The first is the `width` of the network or `the number of hidden neurons` or `The number of channels per convolution`. This can be done very easily, we just specify a greater number of channels and adjust the later layers accoringly and also change the forward function to take care of the larger vector when we switch to fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define a wider model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class WideNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32,16, kernel_size = 3, padding = 1)\n",
    "        self.fc1  = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "        out = out.view(-1, 16*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not want to hardcode the numbers we can easily pass in the argument to init to parameterize the width, taking care to also parameterize the view of the flattening of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38386\n"
     ]
    }
   ],
   "source": [
    "# now we can check the number of parameters a model has with the numel function.\n",
    "wide_model = WideNet()\n",
    "paramters = [p.numel() for p in wide_model.parameters()]\n",
    "print(sum(paramters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the wide model has greater number of parameters(38386) while the previous one had only 18090. A bigger model can fit more complex images(images with more variablity) but it is more likely to overfit the data since it uses the greater number of parameters to memorize the unessential things of the model. There are a few other tricks that we can use to tackle overiftting (like regularization, dropout)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helping the model converge and generalize : Regularization\n",
    "Training involves two step, the first is `optimization(decreasing the loss)` and the second is `generalization(making correct predictions on unseen data)`. Combining these steps is called as `Regularization`.\n",
    "##### Keeping the parameters in check : Weight Penalties\n",
    "The first way to regularize is to add the regularization term to the loss. This makes it so that the weights of the model tend to be small on their own, limiting to how much they can grow i.e it penalises larger weight values. This makes for a smoother loss function topography and there is relatively less to gain from individual samples. The most common of the regularization techniques is \n",
    "1. L2 Regularizatin : This is sum of the squares of all the weights of the network.\n",
    "2. L1 Regulariation : This is sum of the absolute values all the weights of the network. It is also scaled(by a small number) by a hyperparameter which is decided prior to the training.\\\n",
    "We will focus on L2 regularization here, L1 regularization has the property of training very sparse weights.\\\n",
    "L2 regularization is also referred to as weight decay in the sense that the negative gradient of l2 regularization w.r.t a term w_i is shown as (`-2*lambda*w_i`) where lambda is the aforementioned hyperparameter simply referred to as weight decay in Pytorch. So adding L2 regularization to the loss function is equivalent to dcreasing the value of the weight that is proportional to its current value(hence weight decay). This weight decay applies to all the parameters of the network including baiases.\\\n",
    "We can apply L2 regularization pretty easily. After we calculate the loss, we need to sum the squares of all the parameters of the model and then we multiply the sum with lambda and add it to the loss and then backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-12 07:10:13.475735 --> Epoch : 1 | Train Loss : 0.7754923813640119 | Val Loss : 0.43704588451082743\n",
      "2022-10-12 07:10:43.491314 --> Epoch : 20 | Train Loss : 0.378885542765593 | Val Loss : 0.2921984415678751\n",
      "2022-10-12 07:11:14.855100 --> Epoch : 40 | Train Loss : 0.3467022910856972 | Val Loss : 0.2734919696573227\n",
      "2022-10-12 07:11:46.170373 --> Epoch : 60 | Train Loss : 0.33922280971044166 | Val Loss : 0.26473655705414123\n",
      "2022-10-12 07:12:17.384496 --> Epoch : 80 | Train Loss : 0.3366683330684424 | Val Loss : 0.26577883935163893\n",
      "2022-10-12 07:12:48.440634 --> Epoch : 100 | Train Loss : 0.33521127100950615 | Val Loss : 0.26356141269207\n",
      "2022-10-12 07:13:19.605600 --> Epoch : 120 | Train Loss : 0.3343719965257584 | Val Loss : 0.2633783607965424\n",
      "2022-10-12 07:13:50.604433 --> Epoch : 140 | Train Loss : 0.3337654546141244 | Val Loss : 0.26338753872920595\n",
      "2022-10-12 07:14:22.250655 --> Epoch : 160 | Train Loss : 0.333340052550974 | Val Loss : 0.26554145344666075\n",
      "2022-10-12 07:14:53.342542 --> Epoch : 180 | Train Loss : 0.33296903896446045 | Val Loss : 0.26410984992980957\n",
      "2022-10-12 07:15:24.306701 --> Epoch : 200 | Train Loss : 0.33262583465812307 | Val Loss : 0.26472317581138916\n"
     ]
    }
   ],
   "source": [
    "# lets add L2 regularization to our training loop\n",
    "# import datetime for monitoring time\n",
    "import datetime\n",
    "\n",
    "# One important thing to realize is to instantiate the optimizer and the model afterdefining the training loop and moving the data to the tensor\n",
    "\n",
    "# define the training loop\n",
    "def training_loop(n_epochs, model, optimizer, loss_fn, train_data, val_data):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0  # Initialize the loss for each epoch\n",
    "        loss_val = 0.0\n",
    "        for image, label in train_data:\n",
    "            # Move the images and labels to the GPU. This is the only step which makes the model train on the GPU. \n",
    "            image = image.to(device=device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "\n",
    "            # carry out the forward pass\n",
    "            # This does not require us to reshape the image with batch_size because the train_loader already give batches.\n",
    "            pred = model(image)\n",
    "\n",
    "            # compute the training loss per batch\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            # ADD L2 regularization to the loss function\n",
    "            l2_rate  = 0.01    # initialzing lambda\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # summing the squares of the parameters\n",
    "\n",
    "            loss = loss + l2_rate * l2_norm             # Doing the L2 regularization\n",
    "\n",
    "            # zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # carry out backward pass\n",
    "            loss.backward()\n",
    "            # carry out the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # This transforms the loss from a tensor to a python number which is nescessary to escape gradients\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        # compute the validation loss and the accuracy\n",
    "        with torch.no_grad():\n",
    "            for val_image, val_label in val_data:\n",
    "                # move the tensors of the loader to the GPU.\n",
    "                # This step was added after we trained once on the GPU without the validation step with a training time of 3m.\n",
    "                val_image = val_image.to(device = device)\n",
    "                val_label = val_label.to(device = device)\n",
    "                val_pred = model(val_image)\n",
    "                val_loss = loss_fn(val_pred, val_label)\n",
    "                # This transforms the val_loss from a tensor to a python number which is nescessary to escape gradients.\n",
    "                loss_val += val_loss.item()\n",
    "        # print the training loss, validation loss, accuracy per epoch\n",
    "        if epoch == 1 or epoch %20 == 0:\n",
    "            print(f\"{datetime.datetime.now()} --> Epoch : {epoch} | Train Loss : {loss_train/len(train_data)} | Val Loss : {loss_val/len(val_data)}\")\n",
    "\n",
    "\n",
    "# This step of defining the model and optimizer after the data_loader has been moved to the GPU is only applicable when using a GPU\n",
    "# Instantiate the model\n",
    "# convnet = Net().to(device=device)  # The .to(device = 'cuda') move the model to the GPU.\n",
    "wide_net = WideNet().to(device = device)  # Transferring the model to the GPU\n",
    "\n",
    "# Initlaize the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar2, batch_size= 32, shuffle = False)\n",
    "val_dataloader = torch.utils.data.DataLoader(cifar_2_val, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Intialize loss function\n",
    "cross_ent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initlaize the optimizer\n",
    "optimizer_sgd = optim.SGD(wide_net.parameters(), lr = 1e-2)\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "training_loop(n_epochs = 200,\n",
    "             model = wide_net,  # we have changed the model from convnet to wide_model\n",
    "             optimizer = optimizer_sgd, \n",
    "             loss_fn=cross_ent_loss_fn, \n",
    "             train_data= train_dataloader, \n",
    "             val_data=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy : 0.9083\n",
      "validation Accuracy : 0.888\n"
     ]
    }
   ],
   "source": [
    "# lets check the accuracy of the model after adding regularization\n",
    "# lets check the training and the validation accuracy of the model by performing the computations on the GPU\n",
    "def validate(model, train_data, val_data):\n",
    "    for name, loader in [(\"train\",train_data), ('validation', val_data)]:\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label in loader:\n",
    "                image = image.to(device=device)\n",
    "                label = label.to(device=device)\n",
    "                pred = model(image)\n",
    "\n",
    "                _, predicted = torch.max(pred, dim=1)\n",
    "                # predicted = predicted.to(device=device)\n",
    "                total += label.shape[0]\n",
    "                correct += (predicted==label).sum().item()\n",
    "\n",
    "        print(f\"{name} Accuracy : {correct/total}\")\n",
    "\n",
    "validate(wide_net, train_data= train_dataloader, val_data = val_dataloader) # changed model from convnet to wide_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the model trained with the L2 regularization does not overfit(It has about the same accuracy on train and validation sets). The weight penalty keeps the model in check when it tries to memorize individual examples and makes it to only learn general characteristics which are common for all the images. That is why the training and validation accuracy is about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD optimizer already has the weight decay parameter whose value is `2*lambda` and it can automatically perform the weight decay step during the update of the parameters. It is fully equivalent to adding the L2 norm in the loss without the need of adding terms to the loss and involving autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not relying too much on a single input: Dropout\n",
    "Dropout is a regularization technique which was discovered by the Geoffrey Hinton group in Toronto. `The idea behind dropout is simple, zero out a random fraction of hidden neurons output in the network and the neurons which would be zeroed is decided randomly at every iteration.`\n",
    "1. The process generates slightly different models at each iteration and does not provide the chance to the neurons to coordinate to memorize the data that happens in overfitting.\n",
    "2. There is also and alternate expalnation : It can also be seen that dropout modifies the features that are being circulated in the model(zeroing out some neurons), which has an effect similar to data augmentation but here it occurs within the network internally and not on the data being input.\\\n",
    "In Pytorch we can implement dropout by adding nn.Dropout in-between the non-linear activation layer and the subsequent Linear or Convolutional layer.As an argument we will specify the percentage of neurons that will be zeroed(via probability). For convolutional layers we would define Dropout2d and Dropout3d, which zero out the entire channels of the input.\\\n",
    "Note that dropout is active during training. When doing evaluation it is removed or the probability is made 0. This is controlled through the train property of the dropout. Pytorch lets us switch between the two modalities by model.train() and model.eval() on any nn.Model subclass. The call will be automatically replicated on the subsequent modules and if dropout is among them it will behave accordingly in forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a model with dropout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p = 0.4)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size = 3, padding = 1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "        out  = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 16*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-12 09:07:52.401056 --> Epoch : 1 | Train Loss : 0.5566641972087824 | Val Loss : 0.45560379113469807\n",
      "2022-10-12 09:08:16.509781 --> Epoch : 20 | Train Loss : 0.31718101968970924 | Val Loss : 0.2991203594775427\n",
      "2022-10-12 09:08:41.277510 --> Epoch : 40 | Train Loss : 0.26942581412034294 | Val Loss : 0.2736488548772676\n",
      "2022-10-12 09:09:03.996908 --> Epoch : 60 | Train Loss : 0.23453680649637795 | Val Loss : 0.2503395818528675\n",
      "2022-10-12 09:09:26.256134 --> Epoch : 80 | Train Loss : 0.2042682024474723 | Val Loss : 0.23798706169639314\n",
      "2022-10-12 09:09:48.011688 --> Epoch : 100 | Train Loss : 0.17745776664906035 | Val Loss : 0.2390391364811905\n",
      "2022-10-12 09:10:09.806012 --> Epoch : 120 | Train Loss : 0.16483371013317244 | Val Loss : 0.2449912095945033\n",
      "2022-10-12 09:10:31.686943 --> Epoch : 140 | Train Loss : 0.1408273773595167 | Val Loss : 0.24434603673834648\n",
      "2022-10-12 09:10:54.471414 --> Epoch : 160 | Train Loss : 0.1239880056926808 | Val Loss : 0.2619836051312704\n",
      "2022-10-12 09:11:16.336820 --> Epoch : 180 | Train Loss : 0.11175161109755215 | Val Loss : 0.2756700576061294\n",
      "2022-10-12 09:11:38.275969 --> Epoch : 200 | Train Loss : 0.10446962920395425 | Val Loss : 0.2672603593488771\n"
     ]
    }
   ],
   "source": [
    "# Now lets define and run the training loop on our model. First we would train it without L2 regularization\n",
    "import datetime\n",
    "\n",
    "# One important thing to realize is to instantiate the optimizer and the model afterdefining the training loop and moving the data to the tensor\n",
    "\n",
    "# define the training loop\n",
    "def training_loop(n_epochs, model, optimizer, loss_fn, train_data, val_data):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0  # Initialize the loss for each epoch\n",
    "        loss_val = 0.0\n",
    "        for image, label in train_data:\n",
    "            # Move the images and labels to the GPU. This is the only step which makes the model train on the GPU. \n",
    "            image = image.to(device=device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "\n",
    "            # carry out the forward pass\n",
    "            # This does not require us to reshape the image with batch_size because the train_loader already give batches.\n",
    "            pred = model(image)\n",
    "\n",
    "            # compute the training loss per batch\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            # # ADD L2 regularization to the loss function\n",
    "            # l2_rate  = 0.01    # initialzing lambda\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # summing the squares of the parameters\n",
    "\n",
    "            # loss = loss + l2_rate * l2_norm             # Doing the L2 regularization\n",
    "\n",
    "            # zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # carry out backward pass\n",
    "            loss.backward()\n",
    "            # carry out the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # This transforms the loss from a tensor to a python number which is nescessary to escape gradients\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        # compute the validation loss and the accuracy\n",
    "        model.eval()  # Doing this deactivates dropout(makes the dropout probility to zero) because we do not need dropout in evaluation\n",
    "        with torch.no_grad():\n",
    "            for val_image, val_label in val_data:\n",
    "                # move the tensors of the loader to the GPU.\n",
    "                # This step was added after we trained once on the GPU without the validation step with a training time of 3m.\n",
    "                val_image = val_image.to(device = device)\n",
    "                val_label = val_label.to(device = device)\n",
    "                val_pred = model(val_image)\n",
    "                val_loss = loss_fn(val_pred, val_label)\n",
    "                # This transforms the val_loss from a tensor to a python number which is nescessary to escape gradients.\n",
    "                loss_val += val_loss.item()\n",
    "        # print the training loss, validation loss, accuracy per epoch\n",
    "        if epoch == 1 or epoch %20 == 0:\n",
    "            print(f\"{datetime.datetime.now()} --> Epoch : {epoch} | Train Loss : {loss_train/len(train_data)} | Val Loss : {loss_val/len(val_data)}\")\n",
    "\n",
    "        model.train() # Then we have to again activate the dropout layers because we need the layers for training in the next epoch\n",
    "\n",
    "\n",
    "# This step of defining the model and optimizer after the data_loader has been moved to the GPU is only applicable when using a GPU\n",
    "# Instantiate the model\n",
    "# convnet = Net().to(device=device)  # The .to(device = 'cuda') move the model to the GPU.\n",
    "dropout_net = DropoutNet().to(device = device)  # Transferring the model to the GPU\n",
    "\n",
    "# Initlaize the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar2, batch_size= 32, shuffle = False)\n",
    "val_dataloader = torch.utils.data.DataLoader(cifar_2_val, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Intialize loss function\n",
    "cross_ent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initlaize the optimizer\n",
    "optimizer_sgd = optim.SGD(dropout_net.parameters(), lr = 1e-2)\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "training_loop(n_epochs = 200,\n",
    "             model = dropout_net,  # we have changed the model from convnet to wide_model\n",
    "             optimizer = optimizer_sgd, \n",
    "             loss_fn=cross_ent_loss_fn, \n",
    "             train_data= train_dataloader, \n",
    "             val_data=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two very important lines in the above piece of code i.e model.train() and model.eval()\n",
    "1. `model.eval() : This deactivates the layers which are not required in validation/evaluation such as Dropout Layers, Batch Normalization layer etc.for that epoch if you want to do training and evaluation in the same loop or if you are doing evaluation in another piece of code also`\n",
    "2. `model.train() : This reactivates the Dropout and Batch Normalization layers to be used for training in the Next Epoch after they have been turned off by model.eval() for evaluation in that epoch.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy : 0.9943\n",
      "validation Accuracy : 0.9015\n"
     ]
    }
   ],
   "source": [
    "# Now lets check the aacuracy after applying the dropout\n",
    "# lets check the training and the validation accuracy of the model by performing the computations on the GPU\n",
    "def validate(model, train_data, val_data):\n",
    "    \n",
    "    model.eval()   # Deactivating the layer not required for validation/evaluation\n",
    "\n",
    "    for name, loader in [(\"train\",train_data), ('validation', val_data)]:\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        # model.eval()\n",
    "        with torch.no_grad():\n",
    "            for image, label in loader:\n",
    "                image = image.to(device=device)\n",
    "                label = label.to(device=device)\n",
    "                pred = model(image)\n",
    "\n",
    "                _, predicted = torch.max(pred, dim=1)\n",
    "                # predicted = predicted.to(device=device)\n",
    "                total += label.shape[0]\n",
    "                correct += (predicted==label).sum().item()\n",
    "\n",
    "        print(f\"{name} Accuracy : {correct/total}\")\n",
    "        \n",
    "\n",
    "validate(dropout_net, train_data= train_dataloader, val_data = val_dataloader) # changed model from convnet to wide_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dropout model still overfits by some margin, but we can adjust the dropout rate to mitigate that. But the validation accuracy is higher than any model we have trained so far. So this model generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping Activations in Check : `Batch Normalization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout was a very popular techinique till 2015, when another technique was introduced called as Batch Normalization. The paper descirbed a technique that has multiple beneficial effects on training. Such as allowing us to increase the learning rate and make training less dependent on initialization and acting as a Regularizer. Thus representing an alternative to dropout.\\\n",
    "`The main idea behind Batch Normalization is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution. This helps avoid the inputs passing to the activation functions to be too far in the saturated region of the activation function, thereby killing gradients and slowing down training.`\\\n",
    "1. In practical terms batch normalization shifts and scales the itermediate inputs using the mean and the standard deviation derived at an intermediate location over the samples of a minibatch.\n",
    "2. The regularization effect is the result, such that the individual sample and its downstream activations are always seen by the model as shifted and scaled, depending on the statistics across the randomly extracted minibatch. `This is in itself a form of principled augmentation`.\n",
    "3. The authors suggest that using batch normalization removes or atleast alleviates the need of using dropout.\\\n",
    "Batch Normalization in pytorch is provided by BatchNorm1d, Batchnorm2d and BatchNorm3d modules in pytorch. `Since the aim of batchnormalizatin is to provide scaled inputs to the activation function, its position is after the linear transformation (or convolution) in this case.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a Neural Net with Batch Normalization.\n",
    "class BatchNormNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolution, BatchNormalization and Fully Connected Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, padding = 1)\n",
    "        self.conv1_batchNorm = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size = 3, padding = 1)\n",
    "        self.conv2_batchNorm = nn.BatchNorm2d(num_features=16)\n",
    "        self.fc1 = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    # Now lets define the forward pass of the network\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchNorm(self.conv1(x))  # Calculate the convolution and the batchnorm of the data\n",
    "        out = F.max_pool2d(torch.tanh(out),2)        # Calculate the output after the activation and maxpooling\n",
    "\n",
    "        out = self.conv2_batchNorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out),2)\n",
    "\n",
    "        out = out.view(-1, 16*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-12 10:53:00.655434 --> Epoch : 1 | Train Loss : 0.4368076611060304 | Val Loss : 0.35981808035146623\n",
      "2022-10-12 10:53:12.572646 --> Epoch : 10 | Train Loss : 0.23730535801654806 | Val Loss : 0.2919472594346319\n",
      "2022-10-12 10:53:25.729694 --> Epoch : 20 | Train Loss : 0.15313790675502614 | Val Loss : 0.3412262300650279\n",
      "2022-10-12 10:53:38.964153 --> Epoch : 30 | Train Loss : 0.08798722950771404 | Val Loss : 0.43168472971708055\n",
      "2022-10-12 10:53:52.090823 --> Epoch : 40 | Train Loss : 0.04684871006995059 | Val Loss : 0.4406173593468136\n",
      "2022-10-12 10:54:05.299028 --> Epoch : 50 | Train Loss : 0.03293324935624894 | Val Loss : 0.45061630850273465\n"
     ]
    }
   ],
   "source": [
    "# Now lets train out network, We will use the same training loop\n",
    "# Now lets define and run the training loop on our model. First we would train it without L2 regularization\n",
    "import datetime\n",
    "\n",
    "# One important thing to realize is to instantiate the optimizer and the model afterdefining the training loop and moving the data to the tensor\n",
    "\n",
    "# define the training loop\n",
    "def training_loop(n_epochs, model, optimizer, loss_fn, train_data, val_data):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0  # Initialize the loss for each epoch\n",
    "        loss_val = 0.0\n",
    "        for image, label in train_data:\n",
    "            # Move the images and labels to the GPU. This is the only step which makes the model train on the GPU. \n",
    "            image = image.to(device=device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "\n",
    "            # carry out the forward pass\n",
    "            # This does not require us to reshape the image with batch_size because the train_loader already give batches.\n",
    "            pred = model(image)\n",
    "\n",
    "            # compute the training loss per batch\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            # # ADD L2 regularization to the loss function\n",
    "            # l2_rate  = 0.01    # initialzing lambda\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # summing the squares of the parameters\n",
    "\n",
    "            # loss = loss + l2_rate * l2_norm             # Doing the L2 regularization\n",
    "\n",
    "            # zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # carry out backward pass\n",
    "            loss.backward()\n",
    "            # carry out the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # This transforms the loss from a tensor to a python number which is nescessary to escape gradients\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        # compute the validation loss and the accuracy\n",
    "        model.eval()  # Doing this deactivates dropout(makes the dropout probility to zero) because we do not need dropout in evaluation\n",
    "        with torch.no_grad():\n",
    "            for val_image, val_label in val_data:\n",
    "                # move the tensors of the loader to the GPU.\n",
    "                # This step was added after we trained once on the GPU without the validation step with a training time of 3m.\n",
    "                val_image = val_image.to(device = device)\n",
    "                val_label = val_label.to(device = device)\n",
    "                val_pred = model(val_image)\n",
    "                val_loss = loss_fn(val_pred, val_label)\n",
    "                # This transforms the val_loss from a tensor to a python number which is nescessary to escape gradients.\n",
    "                loss_val += val_loss.item()\n",
    "        # print the training loss, validation loss, accuracy per epoch\n",
    "        if epoch == 1 or epoch %10 == 0:\n",
    "            print(f\"{datetime.datetime.now()} --> Epoch : {epoch} | Train Loss : {loss_train/len(train_data)} | Val Loss : {loss_val/len(val_data)}\")\n",
    "\n",
    "        model.train() # Then we have to again activate the dropout layers because we need the layers for training in the next epoch\n",
    "\n",
    "\n",
    "# This step of defining the model and optimizer after the data_loader has been moved to the GPU is only applicable when using a GPU\n",
    "# Instantiate the model\n",
    "# convnet = Net().to(device=device)  # The .to(device = 'cuda') move the model to the GPU.\n",
    "batchNorm_net = BatchNormNet().to(device = device)  # Transferring the model to the GPU\n",
    "\n",
    "# Initlaize the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar2, batch_size= 32, shuffle = False)\n",
    "val_dataloader = torch.utils.data.DataLoader(cifar_2_val, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Intialize loss function\n",
    "cross_ent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initlaize the optimizer\n",
    "optimizer_sgd = optim.SGD(batchNorm_net.parameters(), lr = 1e-2)\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "training_loop(n_epochs = 50,\n",
    "             model = batchNorm_net,  # we have changed the model from convnet to wide_model\n",
    "             optimizer = optimizer_sgd, \n",
    "             loss_fn=cross_ent_loss_fn, \n",
    "             train_data= train_dataloader, \n",
    "             val_data=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy : 0.9675\n",
      "validation Accuracy : 0.879\n"
     ]
    }
   ],
   "source": [
    "# Now lets see the accuracy of the model on the train and validation sets.\n",
    "# lets check the training and the validation accuracy of the model by performing the computations on the GPU\n",
    "def validate(model, train_data, val_data):\n",
    "    \n",
    "    model.eval()   # Deactivating the layer not required for validation/evaluation\n",
    "\n",
    "    for name, loader in [(\"train\",train_data), ('validation', val_data)]:\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        # model.eval()\n",
    "        with torch.no_grad():\n",
    "            for image, label in loader:\n",
    "                image = image.to(device=device)\n",
    "                label = label.to(device=device)\n",
    "                pred = model(image)\n",
    "\n",
    "                _, predicted = torch.max(pred, dim=1)\n",
    "                # predicted = predicted.to(device=device)\n",
    "                total += label.shape[0]\n",
    "                correct += (predicted==label).sum().item()\n",
    "\n",
    "        print(f\"{name} Accuracy : {correct/total}\")\n",
    "        \n",
    "\n",
    "validate(batchNorm_net, train_data= train_dataloader, val_data = val_dataloader) # changed model from convnet to wide_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like dropout batch Normalization also should not be done while we are evaluating because at inference time we do not want the output of a specific input to depend upon the mean and standard deviations of other inputs. So we need to turn off batchNormalization at the time of inference.\\\n",
    "\\\n",
    "`Of course we still need to Normalize the data but the Normalization should be on the basis of the entire dataset and not a sample or minibatch of samples, so what pytorch does is that in addition to keeping the running statistics of the current minibatch it also calculates and saves the statistics of the whole dataset as an approximation(it approximates the mean and std_dev of the whole dataset based on the previously seen minibatches mean and std-dev), so when we run validation by calling model.eval()  and the model contains a batchNormalization module, the running estimates of the mean and std_dev are frozen and called for normalization. When we again call model.train() the estimates are unfrozen again and we return to using the minibatch statistics for normalization and updating them as we process other minibatches.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Deeper into more complex structures : Depth\n",
    "Earlier we talked about the first step to make a model larger i.e to make the model larger. The second is obviously depth, with depth the ability of the network to approximate more complex functions generally increases. For eg in Convolution a shallow model can identify a persons face boundaries whereas a deep model can identify its mouth or eyes. Depth allows a model to learn the heirarchical information we need to understand the context in order to say something about the input.\\\n",
    "One more way to think about depth is that with increasing depth the network will be able to perform more sequence of operations on an image. For eg. Find a persons boundary, look for the head at the top of the shoulders, find the mouth at the top of the head.\\\n",
    "##### Skip Connections\n",
    "Skip connections are a very important concept in Convolutional Neural Networks. Dealing with deep neural networks comes with it's own set of challanges. Adding depth of more than 20 layers makes the model difficult to converge. Lets see it from the backpropagation perspective, we know that the gradient of the loss function w.r.t to a parameter is calculated by multiplying te local gradient with the incoming global gradient, and this is done for each of the intermediate inputs which were involved in the gradient calculation. So if these numbers are small, then the gradient reaching the parameters would be smaller as they are being multiplied, and if they are large, they would swallow the smaller numbers due to floating point approximation which would lead to the gradient of the parameter vanishing (be really small so it won't cause any change) which would lead to ineffective training of that layer because it and others like it won't be updated correctly.\\\n",
    "In december 2015 Kaiming He and coauthors presented `Residual Networks (ResNet)` architecture which introduced a trick which allowed for Deep Networs to be trained successfully ranging from 10 to 100 layers. The trick that we mentioned is using a skip-connection to short circuit blocks of layers.\\\n",
    "\\\n",
    "`A skip connection is nothing but adding the input of a block of layers to their ouput after their computation is performed. So we will perform the computation of the block of layers and after the computation has been done we will add the input of the blocks to the output. This would alleviate the problem of vanishing gradients.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_chan = 32):\n",
    "        # Define the submodules in the constructor\n",
    "        super().__init__()\n",
    "        self.n_chan = n_chan\n",
    "        self.conv1 = nn.Conv2d(3, n_chan, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(n_chan, n_chan//2, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(n_chan//2, n_chan//2, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear((n_chan//2)*4*4, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward propagation int the forward prop\n",
    "\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)),2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)),2)\n",
    "        out_1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out_1,2)\n",
    "        out = out.view(-1, (self.n_chan//2)*4*4)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What we are doing in the above model is that we are using the outputs of the first activation as inputs to the last in addition to the standard feed forward path. This is also referred to as identity mapping. So how does it alleviate the vanishing gradients problem. As we can see in backpropagation a skip connection creates a direct pathway from a deeper parameter to the loss. This makes their contribution to the gradient more direct, as partial derivatives of those parameters w.r.t the loss have a chance not to be multiplied by the long chain of operations.\n",
    "2. It has been observed that skip connections have a beneficial effect on convergence, especially in the initial phases of training. Also the loss landscape of Deep residual networks is a lot smoother than feed-forward networks with the same depth and width.\n",
    "3. It is worth noting that skip connections were not new when resnets came along. Highway nets and U-nets were used to train the model also. But ResNets made training of deep 100 layer or more models more amenable.\n",
    "4. Since the advent of the ResNets other models have taken skip connections to the next level. One model known as DenseNet proposed to connect each and every layer to later layers with skip connections achieving state of the art results with fewer parameters.\n",
    "5. By now we know how to implement Dense Nets, just arithematically add earlier intermediate outputs to downstream intermediate inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Model of 100 convolution Layers\n",
    "So we need to create a 100 or more layered Network. But that is a challenge in itself because we need to manually connect a 100 CNN layers. So for that what we will do is that `we would define a fundamental building block called as a ResBlock and then build a network dynamically in a for loop`.\\\n",
    "`The ResBlock would be like (Conv2d, Relu, Conv2d) + skip_connection.`\\\n",
    "We will first create a module subclass whose sole job is to provide the computation for one block i.e convolution, activation and skip connection. We would also remove the bias element because BatchNormalization cancels out the need for a bias and we would have a custome initialization of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super().__init__()\n",
    "\n",
    "        # We will not use a bias because a batchNorm nullifies the need of a bias\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size = 3, padding = 1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "\n",
    "        # We will also need to do custom weight initialization. The kaiming_normal initialises the weights as in the ResNet paper.\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "\n",
    "        # The batch norm is initialized to have 0 mean and 0.5 variance.\n",
    "        torch.nn.init.constant(self.batch_norm.weight, 0.5)\n",
    "\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    # Now we will write the forward pass\n",
    "    def forward(self, x):\n",
    "        # The below 3 steps perform the forward propagation of the network\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # This step performs the skip funcitonality and arithematically adds the inputs of the block to the output of the block\n",
    "        out = out + x\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since we are planning to generate a Deep Model we will have to include the batch Normalization in the Block. It will help prevent the gradients from vanishing during training.\n",
    "2. We will now have to generate the network by cutting and pasting, absolutely not we have all the ingredients for imagining how this could look like.\n",
    "3. First in init we will create a nn.Sequential containing a list of ResBlock instances. nn.Sequential would ensure that the ouput of one block will be used as an input to the next. It will also ensure that all the parameters to the block are visible to the net.\n",
    "4. Then in forward we will just call the sequential to traverse the 100 block and generate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the resnet network\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_chans = 32, n_blocks = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_chans = n_chans\n",
    "        # The first input layer of 3 channels needs to be created manually\n",
    "        self.conv1 = nn.Conv2d(3, n_chans, kernel_size = 3, padding = 1)\n",
    "\n",
    "        # Now we will create the 100 layers of the ResNet and add them to a sequential block\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans = n_chans)]) # The * symbol is used in python to unwrap the contents of an iterator\n",
    "        )\n",
    "\n",
    "        # Now we will create the fully connected layers\n",
    "        self.fc1 = nn.Linear(8*8*n_chans, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)),2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out,2)\n",
    "        out = out.view(-1, 8*8*(self.n_chans))\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the implementation we parameterize the number of blocks which is important for experimentation and reuse\n",
    "2. Also the backpropagation will work as expected\n",
    "3. Unsurprisingly the network is quite a bit slower to converge. It is also more fragile in convergence.\n",
    "4. That is the reason we used more detailed initializations and trained our network with a learning rate of 3e-3 instead of 1e-2.\n",
    "5. All this resnet architecture is probably not very suitable for 32x32 images of the cifar10 dataset, but it will be very useful for an imagenet dataset. It also provides us with the fundamentals of understanding existing implementations of model like Resnet and others which are implemented in torchvision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n",
    "Let's discuss breifly about initialization, it is one of the most important tricks in training networks. Historiclly pytorch has different initialization strategies for training netowrks, but the implementation as not ideal so we need to fix the weight initializations oureselves. In testing we found that our model did not converge and looked at what people commonly chose as initializations,(a smaller variance in weights and zero mean and unit variance outputs for batch_norm), then we halved the output variance in batch_norm when the model did not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's already outdated\n",
    "The techniques that we discussed here are outdated ones, but they are important building blocks. The deep learning research landscape is contantly growing at a very fast pace and new architectures are being discovered everyday. So great techniques of today become outdated in 6 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some salient points\n",
    "1. We have solved just a part of the problem, but there are other challenges that we have to solve\n",
    "    * Finding the bird from a larger picture. Creating bounding boxes aroung the image is something out model cannot do.\n",
    "    * What happens when a cat walks infront of the camera. Our model will still classify how much bird or plane like the cat is and would classify it. This is called as `overgeneralization`. It is one of the problems that when we take a model to production. The input cannot be trusted and the model should be equipped to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
