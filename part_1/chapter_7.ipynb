{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 : Learning from Images --> Telling birds from airplanes\n",
    "* Building a feed forward neural network\n",
    "* Loading data using dataloaders and dataset.\n",
    "* Understanding the classification loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we saw what the mechanics of pytorch are by building a simple linear model and using gradient descent to optimize it. In this chapter we will use the Neural Network for an image recognition task. We will load a small dataset of images and try to classify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with images : Introduction to CIFAR_10\n",
    "1. CIFAR_10 is like a sibling of CIFAR_100. It consists of 60,000 images of 32x32 size and they belong to 10 classes.\n",
    "2. We would use the torchvision module to automatically download the dataset and load it as a collection of pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the CIFAR_10 dataset\n",
    "As we discussed let's import the torchvision module and download the datasets module to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "data_path = \"./datasets/CIFAR_10/\"\n",
    "cifar10 = datasets.CIFAR10(data_path, download=True, train=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, download = True, train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first argument to the datasets.CIFAR10 is the path that the data will be downloaded to.\n",
    "2. The download argument tells pytorch to download the dataset if not found in the path provided in the first argument. \n",
    "3. The train argument is a boolean argument which specifies whether we are interested in downloading the training or the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like CIFAR10 the dataset module gives us access to some of the most well-known datasets like MNIST, Fashion-MNIST, CIFAR_100, SVHN, Coco, Omniglot, etc. In each case the data returned is a subclass of `torch.utils.data.Dataset`. We can see the method resolution order of our cifar10 instance includes it as a base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " typing.Generic,\n",
       " object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Dataset class: A look into pytorch Datasets\n",
    "This is a good time to see what a pytorch dataset is all about: It is a subclass of pytorch.uitls.data.Dataset. It is an object that is required to implement two methods. First is `__len__` and the second is `__getitem__`. The former should return the length of the dataset and the latter should return each item of the dataset consisting of an example and its corresponding label.\\\n",
    "`Note : For some use cases pytorch also provides us with an iterable dataset which is used in cases where random access to the dataset is prohibitively expensive, like data generated on the fly.`\\\n",
    "For images torchvision provides images in the PIL image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'automobile')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhn0lEQVR4nO2deZCl1Xnen/fuvU5P92zNMDAMDAyL2DIi2MgqhGIZy46FEgstkU3Jikd/mEpUkSuF5UokubLYroCDrJRSIwsJYwWhtUSMIgljZEkBCQY0MAPDNswwKz1L7913v2/+uJekGZ3ndNPL7Q7f86vq6u7z3nO+9577vfe79zzf+x5zdwgh3vykltsBIUR7ULALkRAU7EIkBAW7EAlBwS5EQlCwC5EQFOxiUTGzH5rZvyS2c8xs0szSsz1WLD4K9jchKzWI3P2Qu3e7e325fUkiCnYhEoKCfQVjZreZ2X4zmzCzZ83sva32T5vZ38x43GYzczPLmNl/BPArAD7X+sj8udZjftnMHjezsdbvX57R/4dm9h/M7JFWn/9pZgNm9hUzG289fvOMx9OxWpxvZo+17N8xs/4z/STP9/fMbJ+ZjZjZ983s3MWaS6FgX+nsRzNwVwH4DIC/MbPBWAd3/2MAPwZwa+sj862tYHsAwGcBDAC4A8ADZjYwo+sHAPwOgI0AzgfwKIAvAegHsA/ApwBgjmP9LoDfA3AWgFrrsVHM7CYAnwTwzwCsbT2He2frJ+aOgn0F4+5fd/dj7t5w9/sAvAjgmnkM9RsAXnT3e9y95u73AngOwD+d8Zgvuft+dx8D8L8A7Hf3v3P3GoCvA7jqDYx1j7vvdfcpAP8OwM2vLcpF+BiA/+zu+1rH/E8ArtTVffFQsK9gzOx3zWy3mY2a2SiAywCsmcdQZwF45Yy2V9C8ir/G0Iy/i4H/u9/AWIfPsGUxu9/nArhzxnMdBmBnjCsWgIJ9hdK6on0BwK0ABty9D8BeNANgCkDnjIdvOKP7mamMx9AMppmcA+DoPFyby1ibzrBVAZyaZdzDAD7m7n0zfjrc/ZF5+CgCKNhXLl1oBu1JADCzj6B5ZQeA3QDe3tKtVwH4ozP6DgHYMuP/7wK40Mw+1FrEez+ASwD87Tz8mstYHzazS8ysE8CfAPjGHOS2/w7gj8zsUgAws1Vm9r55+CcICvYVirs/C+B2NBfKhgC8BcD/btkeBHAfgKcBPIFfDNo7Afx2a1X7s+5+GsBvAvgEgNMA/i2A33T32a62Ib/mMtY9AL4M4FUABQD/ag7jfhvAnwH4qpmNo/kp5tffqH+CYypeIUQy0JVdiISgYBciISjYhUgICnYhEkLwHuWlIpvNeKGQDdoaDa7MeKMRNpjRPqno2xjvF7O5h/2IuIHYAmjsprKoh5EDpjPh+U2nw+0AUJyejByNzD2AjkIHtXV1dgfbp6enaJ9qtUhtqchzzqb5aZzK5IPtnd3hdgCoR87FYoX7n83wky6bibzWqfA5kknz8aanw31GRoqYmqoEJ2tBwW5mN6Ip86QB/JW7/2ns8YVCFldvvyBomxwfpv1qlXKwPZ3lk9HZGQnaRuRpp7itUg77kY0MV69WqC2b6aE2i4R7NsdP1NUD64Ptfb1n3nfz/9iz5yfUBuf+X3zRZdR27RVn5sY0eeKpx2ifV4/tpbbOPH+zOqtnLbV1rTkv2H75dVuC7QAwXh6ltn0HuP8b1vPXc/0At+U7w28ufZE3pKd314Ltn/1Lfg/SvD/Gt+51/m9oaqGXAPigmV0y3/GEEEvLQr6zXwPgJXd/2d0rAL4K4D2L45YQYrFZSLBvxOsTHo4gkLRgZjvMbJeZ7apWVaBEiOViIcEe+lL5C6sG7r7T3be7+/ZsdrYsRyHEUrGQYD+C12c3nY1mRpQQYgWykNX4xwFsNbPz0Exv/ACAD0V7mMOMrGhHLvqpXCHYnslH3qsi2pU5P1hpKuwfADSIDBVbHbdMRHrLhFdUm+SoZWR8jNpOjYwE24vF3dyPiLzW1RGeewAYGjlNbQ8++vfB9obxr3LjlRK1dUT8GC/xfn29YQmwIx9WhQBg0yBfOR8d49ez/gHuR08vP+emy2E5b3KanwOFzrA6kUrxE3/ewe7uNTO7FcD30ZTe7nL3Z+Y7nhBiaVmQzu7u30Uzv1kIscLR7bJCJAQFuxAJQcEuREJQsAuRENqa9eYOVOthKaqjp4v2K5FcjEadSx31Gk+cKJe4vNbdHZZqAMCr4+Fjsaw8AA3j76f5TEQfTPFMtGyBy1CViXDmWL7AZRwYlwDdeCLMsROHqC1LsoPK01x6y0UqpHXkuB/lFB+zcjCcXDNd4YV1C/nV1HbWprOprTTxLLUNTXAf07nweTDhPMPuxHD4HK7W+GupK7sQCUHBLkRCULALkRAU7EIkBAW7EAmhravxKQPyJHllbHya9jMPryTHkjRiiRNTxTdeZw4AipXwcnFnd2Slu85XR4vTvOZatcT9yBSq1GYW7peJ1EDz2Hs+UU8AoCPLFY9qNXxqpercj4ZzdWU6kqDU0cETV4rT4cSgoZP8WJPTh6mtt/8Gait08tJf46UhaisVw3NcB1cgTo2F56NW5+eNruxCJAQFuxAJQcEuREJQsAuREBTsQiQEBbsQCaGt0lu90cAUSdSociUEfavCMlqpyOW6eiQhYGyMSxrj4+FkFwAYILt6dHOVD2PjEeltksta2Rx/aaanIokrRDp05+/r5SJP0mhUIzX00lzmyWfDY1qBj1eLVRqP1FbrTHNbMbwTEk6O8CSTfD5S726U190bIXIYAJw4xW29veHXJnIKozgVfl5ej2yJxocTQryZULALkRAU7EIkBAW7EAlBwS5EQlCwC5EQ2pz1ZsgVwllPhQLPoJok2x1VI1pNpcKfWrnM67v1D3A/envD7UPH+HiVBs9Qy5O5AIBIQhkykbkqTYell1KJ+1HIR+YqknnlDa4NseS2bKQmX70akY0iUmSxwPuNToX9r9UjNeFW8/k9PnSE2ioNnsVYimjLpWJY6qtHMtiK5bD/sT4LCnYzOwhgAkAdQM3dty9kPCHE0rEYV/Z3uPupRRhHCLGE6Du7EAlhocHuAH5gZk+Y2Y7QA8xsh5ntMrNd1WrsfkghxFKy0I/x17n7MTNbB+BBM3vO3X808wHuvhPATgDo6clHtgEQQiwlC7qyu/ux1u8TAL4N4JrFcEoIsfjM+8puZl0AUu4+0fr7XQD+JNan0QCmJ8PSQCrNZYsM8TKd5YUePSJBXHBxH7X1dPEpGT8Vlq/qqyNZV5GMslSkCGSFSCsA0NfP+61eE5aNJse5j+Uin6v+9XxbrrxxiWp8Mix5VRHbBomPV4zIrNMNPh81skVYvcglxQnjxypXuNy4ur+f2iJ1OzHtYek2n+Hnd70xEWx3574v5GP8egDfNrPXxvkf7v69BYwnhFhC5h3s7v4ygCsW0RchxBIi6U2IhKBgFyIhKNiFSAgKdiESQnuz3lJAb2f4/SUdyWqamgjLJNlMpGBjgcsWDVKEEACqxrPDPBeWqAZINhwAHDvMj8VkSACoO/cjU+Bztbo3LF/VI/vb5SLjdcbmscH9b5Bss741vJhjkdeAxMQYzxobPhXOigSA7s6w/xnSDgD1Bj+vqmVuGxsLy2FAPNOyQPYlzPbx1+ysjWvDfXK8IKau7EIkBAW7EAlBwS5EQlCwC5EQFOxCJIS2rsY7gEojvMI4McRXK1f3h5e7G3W+/VPVIivMnXwrnsnIamu9El5hLuT4ym5PD7et6uIJHMOjfKV7bDiyil8O+5gBf17dER9L03yuKuRYANDblw+251hWE4B8RNU4PcRXpju6+TxOlcPnSD6iQJRj58A0V0k663weM/lYslR4jj2SNFQk0kU1kqijK7sQCUHBLkRCULALkRAU7EIkBAW7EAlBwS5EQmir9NaoNzAxGZYM6nUu40wRaWJ8lMtC+SyXSNJpXussnYpsQUTaK5VI3a8st3XkuMRTrPL3YfeYPBiW5RqR51wa5kkmuTQ/RbLpDu6HhyWv2NxXivw5pyyyxdMYP3dWD4QlwGKZnzvlCp/fgb5YIg+XvabL3NYgp8jYCPdjcP3qYLtzVVZXdiGSgoJdiISgYBciISjYhUgICnYhEoKCXYiE0OYadCn0FMJyzdAE3/5pujgebHfn2U5ej2wXNMHf4867uJvaSqTU2egkl3E8UqetXOO2wir+3Lq6I/LVWHjM0dPcx0aaSzwN45KRg9s6+8Jz3EhxmWzV2k5qOy/PbWOjXDqsVYmPkf2Yelbx86M3UhcODR5Oh47xDM3+/vAWW72RbMRKJRwvHtHeZr2ym9ldZnbCzPbOaOs3swfN7MXW77DoJ4RYMczlY/yXAdx4RtttAB5y960AHmr9L4RYwcwa7K391ofPaH4PgLtbf98N4KbFdUsIsdjM9zv7enc/DgDuftzM1rEHmtkOADsAIJfj30OFEEvLkq/Gu/tOd9/u7tuzWQW7EMvFfIN9yMwGAaD1+8TiuSSEWArm+zH+fgC3APjT1u/vzKVTKmXoJFvdpCJX/RTZjqfAE5CwZj03rlnPn3atziWq8cmwnFfhqgpqVS4B9p/Fs8b6+vmY5TIfc4JkCNYikoyX+Xv+hgu4/FMtcT/SFralM7wPUlzKy+S4raubv54nT4Slvq58JJsvUhxybJL70dPF5+qsLi7pjhDptjcivxYKYVtqHlmb/xczuxfAowAuMrMjZvZRNIP8V83sRQC/2vpfCLGCmfXK7u4fJKZ3LrIvQoglRLfLCpEQFOxCJAQFuxAJQcEuREJoa9ZbuVzFCy8fCRuNZ3IVOsLvSWsHuXQ1MBDL/uEZT7UKn5Ku7rCs0ZHnvh96hUtNFnmvnZzgEs/oaW6rVclzi2Sv5bt5RlktsndYOhO5VtTD0ufoCJc2sxmuYWYjp6rVI9mPRPpsGD8HIuoVGpHCkVN5Ph+b1/NzJDUeztpr1GKFRcPP2X0B0psQ4s2Bgl2IhKBgFyIhKNiFSAgKdiESgoJdiITQVunN3dBohCWIaoXvzTawNrxf15Zt4UJ9ADBynEs8w8Pc1h2pptfbF56ukZNcMho4i0sunT1cWhk5ySWUamRvuWvOuzDYvnUtT6P7+t7HqQ0ZLmu9vI8/77WD4Qwwj0hetRq/9pQj2YP1iC1TCEuwg1sihUXHuWxbOs4Lo3ZVuW2kFCmKScKwMs1jIlcInx8ekZV1ZRciISjYhUgICnYhEoKCXYiEoGAXIiG0dTU+l0lj0+pVQdtLR4dovylSo+uZPbzOZbXEV1Q7Cnwl9vABvsLcNxBema6V+appw8JKAgAMHeX9Orr4KnhpmidjXL1ha7D9Xde+lfYZK/MtmfYeOExtN1x8MbU9dXR/sN06uRJSK/K5OmvjALUd3M/PnfWd4fNtQ46rJJPpyOvSy5OGTp0epbZsB0/aqlXDc9LTzWva9VvYljElwgiReBTsQiQEBbsQCUHBLkRCULALkRAU7EIkhLZKb5l0Gv2re4O21cUx2m9kKHxzvze4PNUTqUE3NTVFbRlS7w4ASpPh4xX5cCjVuXFqlPdbt76H2qolLuO8VJwItnf+9Ena513ncAlta3YNtV187hZq2/FXzwXbh09O0j5vveoKatu8mW4UjBKRZgFgbDgso50c4klU5cIotVWJTAYA1SzPolq3gfvvk8eJgXZBptAXbDd7lfaZy/ZPd5nZCTPbO6Pt02Z21Mx2t37ePds4QojlZS4f478M4MZA+1+4+5Wtn+8urltCiMVm1mB39x8BGG6DL0KIJWQhC3S3mtnTrY/59MuKme0ws11mtqtS5bd5CiGWlvkG++cBnA/gSgDHAdzOHujuO919u7tvz2Xbuh4ohJjBvILd3Yfcve7uDQBfAHDN4rolhFhs5nWpNbNBd39NL3gvgL2xx79G3euYrI0Hbd29YUkOACYnw3LS1BiXQQp5njG0eg2X7E6c5Blgq/vDtmqZayQnh/l4jUhm3vhp/txSFt5aCQDe8isfDrZPvnqU9pl8NZyhBgDjkyPUduowH/MT778p2P7Dnz9N+3RtPI/aNvSvpbbiNi7bHj20L9g+fJTIXQBKXfz1tCw/d6oT/LV+4TCXxMaL4Tle3xfO2AOAvgvOCbansy/TPrMGu5ndC+B6AGvM7AiATwG43syuRFMJPAjgY7ONI4RYXmYNdnf/YKD5i0vgixBiCdHtskIkBAW7EAlBwS5EQlCwC5EQ2nqXS7lSw/4D4Ttvq3W+hU9nV1hGW7eRFw0sFfndeuNTXPKK3fdz4Ei435oe/p556TqeXTUFnlFWrXIZJ5/nRQ+vuOofBdvrRZ5R1tizi9oeeoBLRseOPkttH/jQh4LtE8M86+2bT4Uz5QDgHR+5ktpiL1qFyKJnG9+OKfvsU9TWk+fnXMa4bdS4j2OFsMRWy3GJtTpyKtjudX7e68ouREJQsAuREBTsQiQEBbsQCUHBLkRCULALkRDMPVLVbpHJZbO+fk24zkU2y+WwXCG8f1XVuDxVn+K2gS1c0shUeKHHX5sIZzzdfPIY7XP/us3U9r0enulndZ71VuEqJX7p+ncG2//FO26gfWovv0RtD+9+hNqOn+DP+22XXBZsPzXGs+ga6Ug2YoHPVfk03+ut54LNwfaLavx8+61OXhwyCz75HtnPzUuR/QCPhPcsLB7jmXmH9v882P7+5w/jmelSMGB0ZRciISjYhUgICnYhEoKCXYiEoGAXIiG0NREmnXH09oVXM/t6+Sr40ZPhm/5LE+FVegAYm+S27f391Pap8y+htkvfsinYnjrBV5gPvMzL830jspWQRRKDUs6f2yPfD+/XcdUGPr/26iFqu+ySDdT2WzeHihg1mUB4ZX0Q/Dnv/NxfUtu6C7ZR2ypSjw0ABj28Qn55J69R6Nv4tlaVi3lCUerCS6kNT++mpsaDPwi2Z08cpn22VcIJL4WIuqYruxAJQcEuREJQsAuREBTsQiQEBbsQCUHBLkRCmMuOMJsA/DWADQAaAHa6+51m1g/gPgCb0dwV5mZ35xoUgAwMa9NhyaM4PE37FSbDckJPJ3+vuqWLS01/WOK1wlYdD8t8AFA6Gk5YyBw4SPv8WpFLTUdX5antW5EkmVHjslwpE5a8nvj7H9M+a4wnoFx3kieFZF7lSTLdp0+G24s8IeQj+/jpM/Dco9S2qsCTWrrHwjXvss7n0Mo8ico2cCnStnLZttHN6wamJ8PbV6VG+Xx4x2DYkArPOzC3K3sNwCfc/WIA1wL4AzO7BMBtAB5y960AHmr9L4RYocwa7O5+3N2fbP09AWAfgI0A3gPg7tbD7gZw0xL5KIRYBN7Qd3Yz2wzgKgA/A7D+tZ1cW7/55z0hxLIz59tlzawbwDcBfNzdx834LZtn9NsBYAcA5LNaDxRiuZhT9JlZFs1A/4q7f6vVPGRmgy37IIDg6pW773T37e6+PZtWsAuxXMwafda8hH8RwD53v2OG6X4At7T+vgXAdxbfPSHEYjFrDTozexuAHwPYg6b0BgCfRPN7+9cAnAPgEID3uXt4b6cW6/oK/s+vD2codfdH6rGRrXPW7+e1x37/EJdj0lsuoLbMuVw+sZ/+NNjuh/bxPuDyGhp8q56T/eEtgQDgdM8AtU3mwl+vzst30z79q/h41sFlOcvxb4HeGT5eupf7kV7L/UAnl1K9k9cUbGTCUm+9xuW1Rop/Rc308y270ik+V8jyLLsGOZw//DAf73t/F2z+xwefxxPF6eCIs35nd/efAGDPPlzdUAix4tCXaCESgoJdiISgYBciISjYhUgICnYhEkJbC05msxmcTeSVbJbLFvVGWB684aUp2ifXwyWS1Kr11IY9T1KTnTwabr/sl3ifK3mBQmzaSE0b+8LbZAHAxjyXcVAKZ9k1TnGZEiRDDQDqpLAhAKQ6uIxmjbC0VZ/k2Y3+Mt9OynP8uuTGffRy2OblIu8Tkd4qkcKo6QKXS7Ga2+pnh8/V9AW88GX6ox8OG+78r7SPruxCJAQFuxAJQcEuREJQsAuREBTsQiQEBbsQCaGt0lsmlUJ/Z1fQls/wIpCdQ+PB9vMnI4UBJ1+ltvqRB6htegOX5VIXXRg2XLSV9sEaLtWkhg5QW+PnXAJMj05QW71cCra/5Fym7CXyFAD0F8PjAUC+wjMLG/nwqWVVXugRVe6H5Xj2YAOR4pHkeKl0JGMvMh4ixT7rfKpgkaKehUJYSj1S5/MxRS7TpVOnaR9d2YVICAp2IRKCgl2IhKBgFyIhKNiFSAhtXY33hqNaDidqVMp8lXPbc+EkjoLzFc5ajW8zVANf5SyMhrfiAYDOU6PBdn/scdrHG9yPamQLomqkNqBF3qMtHU7i2Jzmakc2xU+DtEeSTJyvxqcQfm1ifSxiQ4PPVaTyG+Dh+UiR5Kpmn8jcW+z6yG3VyAr/HSTx5t7IocaJi0dqkcQlPpwQ4s2Egl2IhKBgFyIhKNiFSAgKdiESgoJdiIQwq/RmZpsA/DWADWhu/7TT3e80s08D+H0ArxUw+6S7fzc2VjqTRl9/uAZdbYxLE4MHw3JYZTqcIAMAsW2t0hHVpVTi9dgeyYblq6mNvF6cVbj0NjjBMycumOQ2oxv0AKiF5zEbkWRi1Il01fSD48wa6RQR3mY5VozYqGHqkYNZJBEmF/HknshWWbf3hrev2nYh36ZsUz7s5OnHnqV95qKz1wB8wt2fNLMeAE+Y2YMt21+4+3+ZwxhCiGVmLnu9HQdwvPX3hJntA8DLogohViRv6Du7mW0GcBWaO7gCwK1m9rSZ3WVm/LOsEGLZmXOwm1k3gG8C+Li7jwP4PIDzAVyJ5pX/dtJvh5ntMrNdE9O82IQQYmmZU7CbWRbNQP+Ku38LANx9yN3r3rzZ+QsArgn1dfed7r7d3bf3dEY2NxBCLCmzBruZGYAvAtjn7nfMaB+c8bD3Ati7+O4JIRaLuazGXwfgdwDsMbPdrbZPAvigmV2JpvJxEMDHZhsolUqhUAjLDJlHuWTQNzoabC9HpI6YPFUxbvtMJ691tnvTumD7ORdvo33WbthMbadeeIbaLvgJz6T7N5GacWnyvBuR9/WYdBWZKtTtjc9/KqqTxcbjxMZ08gSizzlytEyDS3ljkfm4L8tDbctguO7hzb/x27RPV1f4PN3zwh3BdmBuq/E/QXiuo5q6EGJloTvohEgICnYhEoKCXYiEoGAXIiEo2IVICG0vOFmZDstGb9nPM9gy+fDNOFYMF69swrOTvpfroLYf9PO7fi9f0x1sz2GS9hno5scqDYTHA4AHNq2ltmsOhAtwAsDbSSHFyIZGyEUyBGM5Y+lIv/kIfTEfI8l38yI2XKyA5eFz+6ntUJFnOB6NTOTlZIuw5w8+R/sMrO4Ntper/C5VXdmFSAgKdiESgoJdiISgYBciISjYhUgICnYhEkJbpTekMkh3hqWLx9/KM8fs+bDMUHjxedqnt84FlN0pLvJk+JZoKBAJ8JyuLtqncmo/H8+5ZNe7ahW1/UPhNLXdMBl+bpnIvnKxDLD5nyDhUed9rHlqbz5LOcoQFunTUeJy7zHn185UnmdTDpBMy8bUAdqnUgpLul7lhUp1ZRciISjYhUgICnYhEoKCXYiEoGAXIiEo2IVICG2V3syAXC6c/jN0djjzBwC+fiwsGz25jktetTEuQbxY5zKUNfj7X64nLBtuWBcuGNgcb5raXpnipbUr5SK1nXL+so0MhiW74W2X0j7ZOi9gmYlIXql6ZD89ZotVsIzl2DUi0mHqje8E1yB74gFAKnIN7Jzgr2flyEvUZl1cCq6RIpZb+jbQPo16OMMuk4rIf9QihHhToWAXIiEo2IVICAp2IRKCgl2IhDDraryZFQD8CEC+9fhvuPunzKwfwH0ANqO5/dPN7j4SGyudSqOrK7yinS/wFeF/KITfk34aWUWeTPGV3UykAlnPOK+Fl+0I16cbvPR62mfq9ClqO3H4YWqbLPPV4idqXGn4Uim86nv41DHaJx1ZzM6l+CpyzritQVbI02nex6Ir9ZGtoSKKAdvKydL8OhfdOqyXKyjPZ3g/jwgNE/VwGFY6eY3CQp7YMty/uVzZywBucPcr0Nye+UYzuxbAbQAecvetAB5q/S+EWKHMGuze5LVczGzrxwG8B8Ddrfa7Ady0FA4KIRaHue7Pnm7t4HoCwIPu/jMA6939OAC0foe3OBVCrAjmFOzuXnf3KwGcDeAaM7tsrgcwsx1mtsvMdo1N8rvChBBLyxtajXf3UQA/BHAjgCEzGwSA1u8TpM9Od9/u7ttXRTZMEEIsLbMGu5mtNbO+1t8dAP4JgOcA3A/gltbDbgHwnSXyUQixCMwlEWYQwN1mlkbzzeFr7v63ZvYogK+Z2UcBHALwvtkGyuZyOOvsjUGbZ7lkcF0xXKvtokG+TDBV4vJUo851kINDvL7b3r17gu3bLrqa9unu4vLJqydGqW1seJjayh1c4vlSKrz9T+owr2c2UeJbBlWrsYSRiNTE2iMl4cy4MVZJLibYsatZLHcmF5HQ+rp5wtYJkpwCANURLumeGJ4I9zF+rC3nXhVsz+Xup31mDXZ3fxrAL4zs7qcBvHO2/kKIlYHuoBMiISjYhUgICnYhEoKCXYiEoGAXIiGYx7SQxT6Y2UkAr7T+XQOAp4S1D/nxeuTH6/n/zY9z3X1tyNDWYH/dgc12ufv2ZTm4/JAfCfRDH+OFSAgKdiESwnIG+85lPPZM5MfrkR+v503jx7J9ZxdCtBd9jBciISjYhUgIyxLsZnajmT1vZi+Z2bIVqjSzg2a2x8x2m9muNh73LjM7YWZ7Z7T1m9mDZvZi63e4lO3S+/FpMzvampPdZvbuNvixycweNrN9ZvaMmf3rVntb5yTiR1vnxMwKZvaYmT3V8uMzrfaFzYe7t/UHQBrAfgBbAOQAPAXgknb70fLlIIA1y3DctwO4GsDeGW1/DuC21t+3AfizZfLj0wD+sM3zMQjg6tbfPQBeAHBJu+ck4kdb5wTNFP3u1t9ZAD8DcO1C52M5ruzXAHjJ3V929wqAr6JZqTYxuPuPAJxZnaLt1XqJH23H3Y+7+5OtvycA7AOwEW2ek4gfbcWbLHpF5+UI9o0ADs/4/wiWYUJbOIAfmNkTZrZjmXx4jZVUrfdWM3u69TF/yb9OzMTMNqNZLGVZKxif4QfQ5jlZiorOyxHsoZo/y6X/XefuVwP4dQB/YGZvXyY/VhKfB3A+mhuCHAdwe7sObGbdAL4J4OPuzus4td+Pts+JL6CiM2M5gv0IgE0z/j8bAN+baAlx92Ot3ycAfBvNrxjLxZyq9S417j7UOtEaAL6ANs2JmWXRDLCvuPu3Ws1tn5OQH8s1J61jj+INVnRmLEewPw5gq5mdZ2Y5AB9As1JtWzGzLrNmRT8z6wLwLgB7472WlBVRrfe1k6nFe9GGObHmJm9fBLDP3e+YYWrrnDA/2j0nS1bRuV0rjGesNr4bzZXO/QD+eJl82IKmEvAUgGfa6QeAe9H8OFhF85PORwEMoLln3out3/3L5Mc9APYAeLp1cg22wY+3oflV7mkAu1s/7273nET8aOucALgcwM9bx9sL4N+32hc0H7pdVoiEoDvohEgICnYhEoKCXYiEoGAXIiEo2IVICAp2IRKCgl2IhPB/AAU+WmR+tpVJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "image, label = cifar10[99]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(cifar10.classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset transform\n",
    "Plotting the image is good but we would like to convert our data into pytorch tensors before we can do anything with it. This is where `torchvision.transform` comes in. It defines a set of funcitons that can be passed as arguments to a torchvision dataset and it can perform those transformations on the data after its loaded but before it is returned by `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AugMix',\n",
       " 'AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_pil_constants',\n",
       " '_presets',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'functional_pil',\n",
       " 'functional_tensor',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see the list of available objects as follows\n",
    "from torchvision import transforms\n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the above transfroms we have the ToTensor transfrom which transforms numpy and PIL images to tensors. It also takes care to lay out the dimensions of the output tensor as `C x H x W` (channel, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy image shape: (32, 32, 3)\n",
      "Pytorch tensor image shape : torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# lets try out the ToTensor transform. Once instantiated it can be used by like a funciton by passing the image to it as an argument.\n",
    "totensor = transforms.ToTensor()\n",
    "print(f\"Numpy image shape: {np.array(image).shape}\")\n",
    "image = totensor(image)\n",
    "print(f\"Pytorch tensor image shape : {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also pass the transform to the dataset directly. Then we would get returned a tensor instead of a PIL image\n",
    "cifar10 = datasets.CIFAR10(\"./datasets/CIFAR_10/\", download = False, transform=transforms.ToTensor(), train = True)\n",
    "cifar10_val = datasets.CIFAR10(\"./datasets/CIFAR_10/\", download = False, transform = transforms.ToTensor(), train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "image, label = cifar10[99]\n",
    "print(type(image))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the untransformed image was 0-255 and it was an 8-bit integer. But the transformed images have float32 as their datatype and their values range from 0.0 to 1.0. We can verify that also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(cifar10[99][0].dtype)\n",
    "print(cifar10[99][0].min(), cifar10[99][0].max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'automobile')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets also verify that we are returning the same image as the output.\n",
    "image_t, label_t = cifar10[99]\n",
    "image_t = image_t.permute(1,2,0)   # changing from c*h*w to h*w*c because matplotlib.imshow() understands h*w*c format\n",
    "plt.imshow(image_t)\n",
    "plt.title(cifar10.classes[label_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing data\n",
    "1. Transformation are really handy because we can chain the transformations usig transforms.Compose, they can handle transformation and augmentation directly in the dataloader. For instance it is handy for our data to have 0 mean and 1 standard deviation, because we will be choosing activation functions that are linear around 0 or -1 as lower limit and 1 as upper limit, then if the data falls within that range we would have a greater chance that the gradients are non-zero, and hence the model will learn sooner.\n",
    "2. Also normalizing each channel so that it has the same distribution will ensure that channel information can be mixed and updated using the same learning rate.\n",
    "3. In order to normalize the dataset we will need to compute the mean and standard deviation of the data and apply the transform\n",
    "``` \n",
    "                                        data[c] = (data[c]-mean[c])/std_dev[c]\n",
    "```\n",
    "This is what transform.Normalize() does, but the values for `mean` and `std_dev` should be computed offline because the transform does not compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n",
      "tensor([[[0.2314, 0.6039, 1.0000,  ..., 0.1373, 0.7412, 0.8980],\n",
      "         [0.1686, 0.4941, 0.9922,  ..., 0.1569, 0.7294, 0.9255],\n",
      "         [0.1961, 0.4118, 0.9922,  ..., 0.1647, 0.7255, 0.9176],\n",
      "         ...,\n",
      "         [0.6196, 0.3569, 0.9922,  ..., 0.3882, 0.6863, 0.8510],\n",
      "         [0.5961, 0.3412, 0.9922,  ..., 0.3098, 0.6745, 0.8667],\n",
      "         [0.5804, 0.3098, 0.9922,  ..., 0.3490, 0.6627, 0.8706]],\n",
      "\n",
      "        [[0.0627, 0.5490, 1.0000,  ..., 0.2235, 0.7608, 0.8706],\n",
      "         [0.0000, 0.5686, 1.0000,  ..., 0.1725, 0.7490, 0.9373],\n",
      "         [0.0706, 0.4902, 1.0000,  ..., 0.1961, 0.7451, 0.9137],\n",
      "         ...,\n",
      "         [0.4824, 0.3765, 1.0000,  ..., 0.6118, 0.6784, 0.8745],\n",
      "         [0.4667, 0.3020, 1.0000,  ..., 0.5529, 0.6706, 0.8902],\n",
      "         [0.4784, 0.2784, 1.0000,  ..., 0.4549, 0.6549, 0.8235]],\n",
      "\n",
      "        [[0.0980, 0.5490, 1.0000,  ..., 0.3843, 0.8157, 0.8353],\n",
      "         [0.0627, 0.5451, 0.9961,  ..., 0.2510, 0.8039, 0.9176],\n",
      "         [0.1922, 0.4510, 0.9961,  ..., 0.2706, 0.8000, 0.9059],\n",
      "         ...,\n",
      "         [0.4627, 0.3098, 0.9961,  ..., 0.7373, 0.6863, 0.8627],\n",
      "         [0.4706, 0.2667, 0.9961,  ..., 0.4667, 0.6745, 0.8627],\n",
      "         [0.4275, 0.2627, 0.9961,  ..., 0.2392, 0.6627, 0.7922]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.8157, 0.6863, 0.4431,  ..., 0.2863, 0.8118, 0.5882],\n",
      "         [0.7882, 0.6118, 0.4353,  ..., 0.2078, 0.7961, 0.5490],\n",
      "         [0.7765, 0.6039, 0.4118,  ..., 0.2118, 0.7961, 0.5176],\n",
      "         ...,\n",
      "         [0.6275, 0.1647, 0.2824,  ..., 0.0667, 0.5294, 0.8784],\n",
      "         [0.2196, 0.2392, 0.2824,  ..., 0.0824, 0.6353, 0.9020],\n",
      "         [0.2078, 0.3647, 0.2824,  ..., 0.1294, 0.6588, 0.9451]],\n",
      "\n",
      "        [[0.7059, 0.6471, 0.4353,  ..., 0.2392, 0.7765, 0.5373],\n",
      "         [0.6784, 0.6118, 0.4078,  ..., 0.2157, 0.7412, 0.5098],\n",
      "         [0.7294, 0.6235, 0.3882,  ..., 0.2235, 0.7059, 0.4902],\n",
      "         ...,\n",
      "         [0.7216, 0.4039, 0.2667,  ..., 0.0941, 0.6980, 0.7098],\n",
      "         [0.3804, 0.4824, 0.2745,  ..., 0.0667, 0.6863, 0.7922],\n",
      "         [0.3255, 0.5137, 0.3059,  ..., 0.0275, 0.6863, 0.8314]],\n",
      "\n",
      "        [[0.6941, 0.6392, 0.4157,  ..., 0.1725, 0.7765, 0.4784],\n",
      "         [0.6588, 0.6196, 0.3882,  ..., 0.1804, 0.7412, 0.4627],\n",
      "         [0.7020, 0.6392, 0.3725,  ..., 0.1922, 0.6980, 0.4706],\n",
      "         ...,\n",
      "         [0.8471, 0.5608, 0.3059,  ..., 0.1059, 0.7647, 0.7020],\n",
      "         [0.5922, 0.5608, 0.3098,  ..., 0.0824, 0.7686, 0.6431],\n",
      "         [0.4824, 0.5608, 0.3137,  ..., 0.0471, 0.7647, 0.6392]]])\n"
     ]
    }
   ],
   "source": [
    "# Now we would compute the mean and std_dev of the cifar10 dataset. Since it is small dataset we can manipulate it in memory.\n",
    "# Now we would stack all of the tensors along an extra dimension, dimension 3.\n",
    "images = torch.stack([img for img,_ in cifar10], dim = 3)\n",
    "print(images.shape)\n",
    "print(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily compute the mean/channel of the images. We can just keep the 3 channels and flatten out all the other dimension so that the matrix would be 3x1024. Then we take the mean of each channel over the 1024 elements is taken and we get the mean/channel. Same process is done for standard deviation also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "print(images.view(3,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n",
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "image_reshape = images.view(3, -1) # We give the first dimension here and the rest are figured out according to the shape.\n",
    "images_mean = image_reshape.mean(dim = 1)  # The dim=1 means that we are taking the mean along the first dimension i.e adding every column value\n",
    "print(images_mean)\n",
    "images_std = image_reshape.std(dim=1)\n",
    "print(images_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the values, we can initialize the normalize transform and normalize our images across all 3 channels our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2470, 0.2435, 0.2616]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize =  transforms.Normalize(images_mean, images_std)\n",
    "normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply it at the time of creating the dataset and we would `concatenate` it to the `ToTensor()` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(\"./datasets/CIFAR_10/\",\n",
    "                                        download= False,\n",
    "                                        train = True,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(images_mean, images_std)]))\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(\"./datasets/CIFAR_10/\",\n",
    "                                        download= False,\n",
    "                                        train = False,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(images_mean, images_std)]))\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot an image from the transformed dataset and see how the smae image looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'automobile')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = transformed_cifar10[99]\n",
    "plt.imshow(np.array(image.permute(1,2,0)))\n",
    "plt.title(transformed_cifar10.classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a black colors because the normalization has shifted the values between -1 to 1 and changed the overall magnitude of the channels. It's just that matplotlib renders it as black. We would keep that in mind for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinguishing Birds from Planes\n",
    "Jane our freind has set up cameras in the jungle south of the airport to click pictures of birds when they pass. But the passing planes also trigger the cameras and give out False positives. What she needs is an auotomated system that can distinguish between birds and planes. We have got the perfect dataset, that can distinguish between birds and airplanes and from that dataset we can just pick up the pictures for birds and planes and train our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Building the Dataset\n",
    "This is the first step of any Machine Learning problem. First we need our data in the correct format. Currently what we have is a dataset of 50000 images belonging to 10 classes.\n",
    "1. We could create a dataset subclass that only includes the birds and the planes, but the data for that is very small and as the dataset is also very small we could just filter out the images of the birds and the planes and just remap the labels so that they are contigous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}  # Birds are the zero class in  the dataset and 2 is the class number for planes.\n",
    "class_names = ['airplanes', 'birds']\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label == 0 or label == 2]\n",
    "cifar_2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label == 0 or label == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'airplanes')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(cifar2[112][0].permute(1,2,0))) # reshape the image, convert to a numpy array and display\n",
    "plt.title(class_names[cifar2[112][1]])              # display the title of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cifar2 object satisfies the criteria of a pytorch dataset i.e it has `__len__` and `__getitem__` methods. But this is done using a clever shortcut, if we run into some limitations with it we should implement a proper object of the dataset subclass. Now we have a dataset. We need to implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Defining a fully Connected Model\n",
    "Now we have the dataset ready and we need to feed the dataset into the model. Our model taking in an input which is tensor and outputs an output which is also a tensor. After all an image is just a set of numbers laid out in a Spatial configuration. If we take an image and stretch out its numbers in a 1D vector, we could consider those numbers as the input vector to our model.\\\n",
    "So lets start :-->\n",
    "1. We will calculate how many input features per image i,e `3*32*32 = 3072`  features per image. So we will build a model which has 3072 input neurons and some hidden neurons. 1 neuron for each feature. Then we have a hidden layer of some neurons and then an acitvation layer and then we would have an output linear layer of neurons which would give us the correct number of outputs(2 for this case).\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('Input Layer', nn.Linear(3072, 512)),\n",
    "    ('Activation Layer', nn.Tanh()),\n",
    "    ('Output Layer', nn.Linear(512,n_out))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (Input Layer): Linear(in_features=3072, out_features=512, bias=True)\n",
      "  (Activation Layer): Tanh()\n",
      "  (Output Layer): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a model defined with the correct input, output and hidden features. Next we would discuss what the model output should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 : The Output of the classifier\n",
    "In the thermomoeter experiment we output a predicted temperature(a quantitative number). We could do something similar here and cast our outputs to 0 and 1, 0 for birds and 1 for planes and use the MSE loss to calculate the value and if we do that we would turn this problem to a regression problem. But if we look at the problem closely we would find out that we do not have to predict the value, we just have to see if the image is a bird or a plane i.e the output is categorical, so what we can do is that we can switch to categorical variables and represent 0 for bird and 1 for plane and this would still work even if we had 10 classes. We would have a one-hot vector of length 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal case the network would output tensor([1.0, 0.0]) for bird and tensor([0.0, 1.0]) for a plane, but realisticly it would be somewhere in between, so we can interpret the output as probabilities that the first value is a bird and the second value is a plane.\\\n",
    "Casting out problems outputs into probabilities puts some constraints:\n",
    "1. Each output should be in the range between [0.0, 1.0].\n",
    "2. The sum of each of the classes probabilities should not be greater than 1.\n",
    "This sounds like a very tough task to represent this in a differentiable way so that we can backpropagate, but there is a very smart trick that does that and it is called as `Softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representing the output as probabilities : The `Softmax Function`\n",
    "1. Softmax is a function that takes in a vector of values of a dimension and outputs another vector of the same dimension where the outputs satify the above constraints.\n",
    "2. In the softmax function we take the vector and calculate the exponential of its individual elements and then we divide each element by the sum of the exponentials.\n",
    "3. The softmax is a `monotone` function i.e lower values in the input will correspond to lower values in the output. However it is not `scale invariant` i.e the ratio between the values is not preserved. This is not an issue because the learning process of the model will drive the values in such a way that the ratio of the values is preserved.\n",
    "4. The nn module makes softmax available as a submodule. As usual the input tensor may have a batch dimension along the 0th dimension ot they might have dimensions along which they encode probabilities and other dimensions along which they do not, so softamx requires us to pass the dimension as an argument to perform the softmax operation along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Lets test the softmax on an input vector\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum()\n",
    "x = torch.tensor([1.0,2.0, 3.0])\n",
    "print(softmax(x))\n",
    "\n",
    "# using the nn.Softmax module. In this case we have two input vectors along the 0th dimension so we are choosing to perform softmax on dim=1.\n",
    "softmax = nn.Softmax(dim=1)\n",
    "x = torch.tensor([[1.0,2.0,3.0],\n",
    "        [1.0,2.0,3.0]])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, now we can add softmax to the end of our model and the network will be equipped to output the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"Input Layer\", nn.Linear(3072, 512)),\n",
    "    (\"Activation Layer\", nn.Tanh()),\n",
    "    (\"Output Layer\", nn.Linear(512,n_out)),\n",
    "    (\"Softmax Layer\", nn.Softmax(dim=1))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try running the model before even we train it to just see what result do we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17581211490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZC0lEQVR4nO2de5RdVZGHvyIPA3SgCQkkJtFOMDpBHkkMD8GgBHkunYgKwqiDLjS+mIWzcA34BB11xKUoCgJxYABHEBxAYIQlEBlBQaBBSANBEyBIoMmD0CYRQgip+ePeDIE5Vd253X27cf++tXp1d/269t733FP39D11q7a5O0KIv322GugFCCGag4JdiEJQsAtRCAp2IQpBwS5EISjYhSiEob1xNrPDgDOBIcC/u/u3sr8fOdp8x7Zq7bFFiePwavNWI2KXYTYk1LbeJn7YO7SMDrXt2KnSPjR5zVzL6lBbumZxqLWMjFOiraESP6HrEp/XJFp2NciSti8G9m0Tn/6gK7CvT3zWMSzUPHnUq9dsCLX1zyUTPptoEX8J7C+Cb3SrkqzRPLuZDQH+BBwMLAXuAo519wcjn7aZ5l9ur9Y+dmgyWVu1ebupcdCOGbZ9qE3bfcdQO3LWx0LtYPtMpX0ntgl9buOmUPuXX7871N46Ow7P2Auil6rstXRSorUkWvYCsjaw7534NMrGRLs2sC9JfB5ifKi9QBzQN/16Wag9tjCZ8J5Ei7gusK8Ef6E62Hvzb/zewGJ3f8Td1wM/A+b0YjwhRD/Sm2AfDzy+2e9L6zYhxCCkN8Fe9a/C/3tPYGZzzazdzNrXrOjFbEKIXtGbYF8KTNzs9wnAk6/8I3ef5+4z3X3myDG9mE0I0St6E+x3AVPMbJKZDQeOAa7pm2UJIfqahlNv7r7BzE4AfkUt9XaBuz+Q+Qwhubv73sTxk9Xm1VPjO6Ord3861B7tjLXFd54ba7Or03nHzjgw9NkvSZT9aHacMViU3Ou+K1TiO+QTEp/4KMLKRGtNtMbuuk9OtD1CpT05Iv929ROV9paJleYaY+LzY/6Z8fMyfHoy5p2J1pVoEdEykuRar/Ls7n4dcRJACDGI0CfohCgEBbsQhaBgF6IQFOxCFIKCXYhC6NXd+C1lGfDDQDvoE7Hf/CBft+f0Gclscfrkvs8/FmvXPBJrR5xUae84NU4LHbz3glDrChVICvpYmmhREurwxGdsor010bZj50SNjn+W6Hs+VG7jyFC78erWULvjPRdWC0mqd5+zkhKf3WNpfZYTfTTRoii8OfFpAF3ZhSgEBbsQhaBgF6IQFOxCFIKCXYhCaOrd+L8+Ab/7UrW2y9djvxM+WG0/64qkn0/WBmivRMvq9oIqgN9/Or7jnrWQ6kq0MxIt4+DAnt0Dz9pSbZf2I4nvxn8qaJL2Tt4Y+uyV5CBWJN3rOiYGJwgAF1abkwPyd+NirWtWrP0xu+OejNms6hJd2YUoBAW7EIWgYBeiEBTsQhSCgl2IQlCwC1EITU298RTwjWrp4cTtrL8PhKxaZIdY2iXKTwEPZym7S6rNTySN2j5ySzJesv6dG9w6JdoRJnnIvJG4F152ivyS6v5uAM8FY04iLl66nbiX3zG8L9SSIQkf+aQbQ4+bwj2N4Imzk6myCqXHEy3aPqeP0ZVdiEJQsAtRCAp2IQpBwS5EISjYhSgEBbsQhdCr1JuZLQHWAC8CG9x9ZsODBWktIK5gS/qBcXwsbUj2QtrnrFi7I2pNtihZR0ZSYbfm+7H26WTromij3FuTZXQEFWoAbYl2bzLmXkFFXFfSWO1S3h8PWLVncI8IxnwhrqJ74uxfxMMtSaZqTbRBsINxX+TZD3T3bEswIcQgQP/GC1EIvQ12B24ws7vNbG5fLEgI0T/09t/4/d39STPbCbjRzB5y95d9QLT+IqAXAiEGmF5d2d39yfr35cBVVGzL7e7z3H1mr27eCSF6TcPBbmbbmtnITT8DhwD399XChBB9i7l7Y45mk6ldzaH2duASdw9q2v7Pp7HJ/jWwX5z4ZPsWJdVmbzov1j4e2HdLplqZNGyce2dcNfZsRzzmnklaMSqgyqoK90+0TyVaVGEHMI7XV9o7kk6PH1rwunjAPQ9IZjs90Roge2CzEy3YpgyA2xMtqohrsBrO3SsTlQ2/Z3f3R4A9G/UXQjQXpd6EKAQFuxCFoGAXohAU7EIUgoJdiEJoOPXW0GSNpt6OCOxZqiOrROtKtKi5JUC0z1fSHPJ9SYVaazLV+ck2dmnzwmAtb072GjsqGS5LKyYPjbbA/hA7hj5v//Wu8YAHPZ/MdmcsRefI1GS4oxMta0j6VKI1aT83iFNvurILUQgKdiEKQcEuRCEo2IUoBAW7EIXQ3O2fMrKVRHfW/7HBuS5PtCsS7YbAPikZ7tPJeMld/K2mx9q4ZLujNwT2DyfLmJJoGVlbtUjbwNOx0w1vbGwhFyZ34wPmHBdrSeKCcz+RiNH5MUjQlV2IQlCwC1EICnYhCkHBLkQhKNiFKAQFuxCFMHhSb3FrstoGU1VkRQkZSQ86hiXaIYE9K8jJiiMuiqWhJ8ba27I1BmRb9hy85cMBjR3+aActAE7Pmr+9NlTOPe7wUJvC9ZX2LG24JNHSA5mdw4MAXdmFKAQFuxCFoGAXohAU7EIUgoJdiEJQsAtRCN2m3szsAuBdwHJ3361uGwVcRq3V2BLgaHd/pt9WGaWvLkl8kqqxtMwr613XGtiz6ruswi7Z3md90mduyeRYmxDYR7Bz6HM9y0KtNZ6KaxNtcaLFbJ9o8f5JbUkPutbAnj3NG4ir7/Y88U+hdt/uyaBfTbToXM1SxGMC+29il55c2S8EDnuF7RRgvrtPAebXfxdCDGK6Dfb6fuurXmGew0sfCbkIeE/fLksI0dc0+p59Z3fvBKh/36nvliSE6A/6/eOyZjYXmNvf8wghchq9si8zs3EA9e/Loz9093nuPtPdZzY4lxCiD2g02K8BNnXxOg64um+WI4ToL7rd/snMLgXeAYwGlgGnAr+gllR6HfBn4Ch3f+VNvKqxmrfXVMbYRMuq1KLUStaEMEufRFV05NtGHcW2yaDVJXhjktTbChaE2u+Tmb7/XCJ+N7BfnPgsOj/Wpn4mlD7wYFxLF+3YNZU9Qp+ZnBlqGzgv1Iby+lBrJ86XdgbHfy1xmu8hf7jSfsleS1nW/nzl9k/dvmd392MD6aDufIUQgwd9gk6IQlCwC1EICnYhCkHBLkQhKNiFKITB03BysJClyjoCe1YG9J1YOi5Jr+2QDLmEuDHj6CDFMzR5qu9N5vp+9gmKmxMt6uiYlZtRnU4C4JDXhFJX0sYyyqS2JOnGoZwcamN5LNTeGOYbYTYfDLWodecqngg9Rtk7K+23En92TVd2IQpBwS5EISjYhSgEBbsQhaBgF6IQFOxCFMKrO/WWrT7bd6sr0dLNyAKSxpF5qimukhqadPpam1RsTQi0lTwd+tx6TyjB7TfGWrZhWkP7nn0zVPZpjXOi/5yMGG3NljXEvDVpYJmdHl/jQ6E2mbsSz/GV1ruSE+tQDkzGq0ZXdiEKQcEuRCEo2IUoBAW7EIWgYBeiEF7dd+MbuuNLY3fcG6W6JRwAI5/5RqitW3dMqL1h9JB40OAZHZpkDD4+49BQ++iM2G9x3FSYjhseqbT/8vLT4wH5RajM2hA/aYfykVD7LhdW2hvZWQmgM9GWJNqEpK9d9NRk/f/an7uo0t65MV6hruxCFIKCXYhCULALUQgKdiEKQcEuRCEo2IUohG5Tb2Z2AfAuYLm771a3nQZ8nJdKIb7g7tf11yIHPUl6bdTar4Taz8/eJdRG7xCn156ZEs+3Nsi8LF70fOjTNiXu7zaiNZ5r1ux4p+6xb63Wrn/viaHPxit/EWq3Pxqv48EgvQYwPbC3BcUnAEuT3m8tSchsSLT/SPK9EwL74aEHjNi6ei+yS7bqCn16cmW/EDiswv49d59W/yo30IV4ldBtsLv7LUC3mzYKIQY3vXnPfoKZLTCzC8ws63wshBgENBrs5wC7ANOofYIwbJhtZnPNrN3M2hucSwjRBzQU7O6+zN1fdPeNwI+BvZO/nefuM9097l4vhOh3Ggp2Mxu32a9HAvf3zXKEEP1FT1JvlwLvAEab2VLgVOAdZjYNcGrFPp/ovyXGjJ8U1y61zdor1Iauix/2by7P9jQKmHRSKK16dFbstyLeSmjZlG1DrbMzThut6vhTtdDxYOjzwJqkJG7tX0Lpir3ikrjhM6q3odp4ZdLTLuF30dZb1N5TRkRZ0RVJem1qMt7BSalla6J1JWNWJ9Fgb76deFWH3Na8PfToNtjd/dgK8/nd+QkhBhf6BJ0QhaBgF6IQFOxCFIKCXYhCULALUQhNbTg5fsfX8k9zPlmpjTggTuOMmL5rpf3ASZNCn5aR8TqSIjXeO+6EUJt/5s+qhY4/xgN2xOk1WpKVrPxDKK1aEVebsbK60SNJqgl2TLR4HdwaV/StvzUac/tkroQk9fZc4nZ9YH/464lT1lUyacD5yeNj7dZkyKgebj+uSryier6/hh66sgtRCAp2IQpBwS5EISjYhSgEBbsQhaBgF6IQmpp6G9s2jpPP/3Izp9xiHloRpy7g6cD+341NlhSbsTBLh70/llr3q7Z3xVVvkKQHWZZoGdGxiuyNk+2JFp7g2Zn/o0RLSuLObU38otI24IEgg3ztsNtDn29wcKV9dbIEXdmFKAQFuxCFoGAXohAU7EIUgoJdiEJo6t34VwMrOrLig2aS3bU+L5a6oj5oLybjBQU+g4nkTH3g6sTvgGrzW06JXe5+PBnvqUTL/I7Ycr+7l8Yu5wSPa3kyja7sQhSCgl2IQlCwC1EICnYhCkHBLkQhKNiFKISebP80EbgYGAtsBOa5+5lmNgq4DGijtgXU0e7+TP8tdctYz5OhNjxJaw3tiLc7Wt+rFTWLv9HNeuYm2sRECzKRHcmZ+qakP11rUry0MOldt/XWsbYs6Jf45qjNHLAuaLznG2OfnlzZNwAnuftUYF/gM2a2K3AKMN/dpwDz678LIQYp3Qa7u3e6+z31n9cAC4HxwBzgovqfXQS8p5/WKIToA7boPbuZtVHrYXsHsLO7d0LtBQFI+hsLIQaaHge7mbUAVwCfdfesRv6VfnPNrN3M2lesWNHIGoUQfUCPgt3MhlEL9J+6+5V18zIzG1fXxxF8LNfd57n7THefOWbMmL5YsxCiAboNdjMzard4F7r7GZtJ1wDH1X8+DsjKEYQQA0xPqt72Bz4MdJjZvXXbF4BvAZeb2fHAn4Gj+mWFwKrAvpZHQ58uvzHUxiZ91Z7t6aJEU9nn7Fi744ZY2y7Yrik78TuT6rWPTtwj1I6cuCDUolpEgC8FW1vtOzv2iVraPZhcvrsNdnf/LWCBfFB3/kKIwYE+QSdEISjYhSgEBbsQhaBgF6IQFOxCFMKrouHkqMDeQrBvDvDUzU+E2vUrbg21bVridTybbdckek/WlDHjnlja4ZBqe1aeeWRSRXcUI0ItVuDmRNv/wGp7Vsx36W3V9lXJOaoruxCFoGAXohAU7EIUgoJdiEJQsAtRCAp2IQrhVZF6a4TRbeNDbdKBQSkUML0jTsv97hvVtUtvOTlex92xlOdqFiXaJdmgTeStiXZ7A+N9MZYOZvtQm35KfBovCpqL3uXxXOuisi/ge9wZaofFbiTbtjErmG9lssalQcHn+qQrqq7sQhSCgl2IQlCwC1EICnYhCkHBLkQhNPVu/EbiHm9rg+1sAFqDrXOGJh3jJk+eHGpr12z5HfeMheclYlbcsTLR3rDFy2g+XQ34TEi0pIjj67PjbbmYmoz5pWrzVknB02Vxa8N0jdfvF2uHJ0O+LbB3JVmBZ94brOG7sY+u7EIUgoJdiEJQsAtRCAp2IQpBwS5EISjYhSiEblNvZjYRuBgYSy17Ns/dzzSz04CPA5u2Zv2Cu1+XjbUVsE2greyK/YYHqbflXBv6/PyyY0LthFhKX/02BvZnuxKnRotWki2NBg1bnqWE4LkEXto5sIqnEi1r8LZ3tXlj1oQuK+I5OpYe/k6snRVne5kW7JL4UeJirvu3ru6xOKQ32z9Re0pPcvd7zGwkcLeZbdpI7XvunjxEIcRgoSd7vXUCnfWf15jZQkhecoQQg5Ites9uZm3AdOCOuukEM1tgZheY2Q59vTghRN/R42A3sxbgCuCz7r4aOAfYBZhG7cpf+UE9M5trZu1m1r5ixYqqPxFCNIEeBbuZDaMW6D919ysB3H2Zu7/o7huBHxPcCnH3ee4+091njhkzpq/WLYTYQroNdjMz4HxgobufsZl93GZ/diRwf98vTwjRV/Tkbvz+wIeBDjO7t277AnCsmU0DHFgCfKK7gdayjttYWKl1Pv5I6NdR7cJPfh3n0C5rMHUVpdcGFScm2pl9PNepsTR891hb//5AyHrrNUqSDgur1LKKw8sTLUsuN7g92A+Dis/dg/QawLwF1fYXkurRntyN/y1QVWyX5tSFEIMLfYJOiEJQsAtRCAp2IQpBwS5EISjYhSiEpjacXP3C09zYeVGl1nHbf4Z+axdVpyBu/kMyWdY08FXOQUmqaX5fp94ujqX1WeXYXoH9rt4sJmBSokUNLhs98xtMrzEllu4LzuOrkgaWYyZW25cPj310ZReiEBTsQhSCgl2IQlCwC1EICnYhCkHBLkQhNDX19vxfn2bRndUptqEj4wqf0UHTwFm7xnPN/1ysbRdLrE60iEOT/dx+1WC50EFR6gqYPj3W5kcVcY2m5LIUZmuiRammrElllkrNaKTx5YGJ9g+J1mgD0azaL9gr8FtJmm/PQ6rtq4bEPrqyC1EICnYhCkHBLkQhKNiFKAQFuxCFoGAXohCam3r7ywssvq46xdYSVPEALA1WOTZIyQHMCfbPAliZtK/vStax7pZq++2NpmMS5ifVYfM/nziOrjZvc27s8uxpyXgfiKU3JymqNwTPzYhkqquS52x9lsKMKtsgbiy5LvGZmmj9QZRyTA5WR1u1fWPyuHRlF6IQFOxCFIKCXYhCULALUQgKdiEKodu78WY2ArgFeE397//L3U81s1HAZUAbte2fjnb3rCsZO46AjwYFEg8ld8FbA/uGSADGzYi1p+6JtYXJnfWNlfvUDgBrEq26xR/PdsUub/nXWFuabJP0wOmJFhRqbJMU8XxzTqwtTLSbPNYei7Zyeir2YVyiJQVKDfenS6OmmqFbV9tfSC7fPbmyPw/Mdvc9qW3PfJiZ7QucAsx39ynA/PrvQohBSrfB7jU2vWYNq385MIeXriMXAe/pjwUKIfqGnu7PPqS+g+ty4EZ3vwPY2d07Aerfd+q3VQohek2Pgt3dX3T3adQ+q7S3me3W0wnMbK6ZtZtZ+9pG39MIIXrNFt2Nd/cu4H+Aw4BlZjYOoP59eeAzz91nuvvMlpbeLVYI0TjdBruZjTGz1vrPWwPvBB4CrgGOq//ZcUDyyWYhxEBj7kneAjCzPajdgBtC7cXhcnf/mpntCFwOvA74M3CUu6/Kxpo2zvyG46u1pSfHn/q/9LvVn+6/NCmOeCYpZtghSbs8c0OsPRtLfU9Q0AJAUgBEgz3vQmYlWlJ0sV2Qelud9LR7fXBuALx7dqxlh+oHj1TbV/0gcUrSgyRFVGm/vjGJdmVgz3rrRYVNc8EfcquSus2zu/sCKh6+uz8NHNSdvxBicKBP0AlRCAp2IQpBwS5EISjYhSgEBbsQhdBt6q1PJzNbATxW/3U0cYewZqJ1vByt4+W82tbxenevTPQ1NdhfNrFZu7vPHJDJtQ6to8B16N94IQpBwS5EIQxksM8bwLk3R+t4OVrHy/mbWceAvWcXQjQX/RsvRCEMSLCb2WFm9kczW2xmA9a7zsyWmFmHmd1rZu1NnPcCM1tuZvdvZhtlZjea2aL69x0GaB2nmdkT9WNyr5kd0YR1TDSzm81soZk9YGYn1u1NPSbJOpp6TMxshJndaWb31dfx1bq9d8fD3Zv6Ra1U9mFgMjAcuA/YtdnrqK9lCTB6AOY9AJgB3L+Z7dvAKfWfTwFOH6B1nAZ8rsnHYxwwo/7zSOBPwK7NPibJOpp6TAADWuo/DwPuAPbt7fEYiCv73sBid3/E3dcDP6PWvLIY3P0W4JW1/01v4Bmso+m4e6e731P/eQ2wEBhPk49Jso6m4jX6vMnrQAT7eODxzX5fygAc0DoO3GBmd5vZ3AFawyYGUwPPE8xsQf3f/H5/O7E5ZtZGrX/CgDY1fcU6oMnHpD+avA5EsFd10RiolMD+7j4DOBz4jJkdMEDrGEycA+xCbY+ATqBpW2OYWQtwBfBZd1/drHl7sI6mHxPvRZPXiIEI9qXA5vu/TACeHIB14O5P1r8vB64ib/jU3/SogWd/4+7L6ifaRuDHNOmYmNkwagH2U3ff1Kip6cekah0DdUzqc3exhU1eIwYi2O8CppjZJDMbDhxDrXllUzGzbc1s5KafgUOA+3OvfmVQNPDcdDLVOZImHBMzM+B8YKG7n7GZ1NRjEq2j2cek35q8NusO4yvuNh5B7U7nw8AXB2gNk6llAu4DHmjmOoBLqf07+AK1/3SOB3akto3Wovr3UQO0jp8AHcCC+sk1rgnreBu1t3ILgHvrX0c0+5gk62jqMQH2AP5Qn+9+4Ct1e6+Ohz5BJ0Qh6BN0QhSCgl2IQlCwC1EICnYhCkHBLkQhKNiFKAQFuxCFoGAXohD+F2pcIoEc1GgMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, _ = cifar2[0]\n",
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we input the image in the model we need to see if the image has the correct shape accepted by the model. So we know that it expects a tensor of 3072 dimensions and that nn module works with batches of data which is an extra index along the 0th dimension. So we flatten our image and add an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4910, 0.5090]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image = image.view(-1).unsqueeze(0)  # reshapes the image to a 1D vector and adds an extra dimension.\n",
    "\n",
    "# Now we are ready to invoke our model\n",
    "out = model(image)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see that our model outputs a pair of probabilities which it thinks are the correct ones, but the model has not been trained yet. It has just initialized some random numbers between -1.0 to 1.0 to the input. We can also see there is a grad_fn printed in the output. This is the function at the tip of the output and will be useful when we are trying to backpropagate.\n",
    "2. Secondly we also don't know which probability is which i.e bird or airplane because it does not the network cannot associate a class to the output at this point. It is the responsibility of the loss function to associate meanings to these two numbers after backpropagation.\n",
    "3. If labels are provided as 0:'bird' and 1:'airplane', then that is the order the outputs will be expected to take. Thus after training if we take the argmax of the output vector then that is the index of the label, the one which the network predicts most likely. Coviniently `torch.max` also provides the `max of a vector and the index at which it occurs in the vector` along the provided dimension. So we will provide the dimension of the vector and not along the batches i.e dim = 1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.max(out, dim = 1)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says that the image is a plane. Now we have adapted our model to the current classification task by outputting class probabilities instead of discreet values. Now we need to start training the model which is just minimizing loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A loss for Classification\n",
    "We have just discussed that the loss give  meaning to the output probabilities of the Model. Previously we used MSE as our loss metric. We could still use MSE and make the Model converge on [1.0, 0.0] or [0.0, 1.0] but we do not want that becuase that would be unnescesary optimization. We just want to penalize misclassification rather than painstakingly penalize anything that does not look like a 0.0 or 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What we need to maximize in this case is the probability associated with the correct class. This quantity i.e the probability associated with the correct class is known as `likelihood(of model parameters given the data)`. In other words we want a loss function that is very high when the likelihood is low, so high that the other classes probabilities are higher that it and we are not fixated on driving the probility to 1.\n",
    "2. There is a loss function that behaves that way, it's called as `Negative Log Likelihood(NLL).` It has the expression `NNL = -sum(out_i[c_i])` where sum is taken over n examples and the correct class for sampel i is c_i.\n",
    "3. When low probabilities are assigned to the data, NLL grows to infinity, while is grows shallow when the assigned probabilities are above 0.5. Remember that NLL takes probabilities as inputs, so as the likelihood increases other probabilities will nescessarily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up our classification can be computed as follows.\n",
    "1. Compute the forward pass.\n",
    "2. apply the softmax function to compute the class probabilites.\n",
    "3. Get the probabilities of the correct class(likelihood). We know the correct classes because it is a supervised problem.\n",
    "4. `Compute its logarithm`, `slap a - sign` in front of it and `add it to the loss`.\n",
    "\\\n",
    "\\\n",
    "But how do we compute these probabilites in pytorch. Pytorch has nn.NLLLoss class. However it takes a tensor of log probabilities as input, It then computes the NLL of our model given the batch of our data. `There is a good reason behind this convention : because taking the logarithm of a probability is tricky when the probability goes very close to zero. The workaround is to use nn.LogSoftmax instead of nn.Softmax which takes care to make the calculation numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to modify our model to include nn.LogSoftmax instead of nn.Softmax\n",
    "from typing import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"Input Layer\", nn.Linear(3072, 512)),\n",
    "    (\"Activation Layer\", nn.Tanh()),\n",
    "    (\"Output Layer\", nn.Linear(512,n_out)),\n",
    "    (\"Log_Softmax Layer\", nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "# Then we instantialte or NLL loss function\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss takes two arguments : \n",
    "1. The tensor of log-probabilites of a batch of images from the LogSoftmax layer \n",
    "2. A vector of class_indices(0 and 1 in this case because we have two classes)\n",
    "\\\n",
    "\\\n",
    "We can now test it with our bird images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5747, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = cifar2[0]\n",
    "\n",
    "# Flattening the image and adding a batch dimension and then passing it to our mode to get the probability tensor\n",
    "pred = model(image.view(-1).unsqueeze(0))\n",
    "\n",
    "# Calculating how much the probability differs from the actual label\n",
    "loss(pred, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the network is only 50% sure that it is a bird, because the ground truth label says that the image is a bird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Entropy vs MSE\n",
    "Now We can compare how the cross-entropy loss is better than the MSE : \n",
    "1. Even for correct classes, cross-entropy loss has some margin for error. The correct class is assigned a prbability of 99.97%, but MSE get saturated very easily, many times even for the wrong input it says that the classes are correct.\n",
    "2. The reason for the above point is that the slope of MSE is often too low to compensate for the flatness of the softamx for incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Classifier\n",
    "Now we are ready to bring back the training loop and see how it trains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be 3 different ways of training a classifier.\n",
    "1. We train on the individual images, calculate the loss, backpropagate and update the parameters per image.\n",
    "2. We train on individual images, accumulate the loss, accumulate the gradients and update the parameters once per epoch.\n",
    "3. We train on idividual images in a batch of images, compute the loss and accumulate the gradients per batch and update the parameters once per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we will try to update the gradients once per image\n",
    "# # We would define the model\n",
    "# from typing import OrderedDict\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# n_out = 2\n",
    "\n",
    "# # define the model\n",
    "# model = nn.Sequential(OrderedDict([\n",
    "#     (\"Input_linear_layer\", nn.Linear(3072,512)),\n",
    "#     (\"Activation_layer\", nn.Tanh()),\n",
    "#     (\"Output_layer\", nn.Linear(512, n_out)),\n",
    "#     (\"Log_Softmax_1\", nn.LogSoftmax(dim=1))\n",
    "# ]))\n",
    "\n",
    "# # define the learning rate\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer_sgd = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# # define the loss function\n",
    "# nn_loss = nn.NLLLoss()\n",
    "\n",
    "# # Now define the training loop\n",
    "# def training_loop(n_epochs, optimizer, loss_fn, X_train):\n",
    "#     for epoch in range(1, n_epochs+1):\n",
    "#         # carry out the forward pass for each image in the dataset\n",
    "#         for image, label in X_train:\n",
    "#             # make the image in proper shape of the input for the model\n",
    "#             input = image.view(-1).unsqueeze(0)\n",
    "#             pred = model(input)\n",
    "\n",
    "#             # compute the training loss\n",
    "#             loss = loss_fn(pred, torch.tensor([label]))\n",
    "\n",
    "#             # zero out the gradients of the optimizer\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # carry out the backward pass\n",
    "#             loss.backward()\n",
    "\n",
    "#             # update the paramters\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # compute the validation loss\n",
    "\n",
    "#         # print the training and the validation loss\n",
    "#         print(f\"Epochs : {epoch} | Training loss : {loss}\")\n",
    "\n",
    "#     return\n",
    "\n",
    "# training_loop(n_epochs=10, optimizer= optimizer_sgd, loss_fn = loss, X_train = cifar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training_loop we are calculating the forward pass, and then updating the gradients per image. However what is good dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we would try to divide the dataset into minibatches and train and update the parameters per minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(cifar2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our previous examples what we did is that we evaluated the gradients according to each image and updated the parameters per image. But the problem for this approach is that the direction for the gradient for reducing the loss for  one image might not be a good direction for reducing the loss for others. So what we do is that we shuffle the dataset into random batches of the data and estimate the gradients on a few samples at a time. This introduces a sort of randomness in the gradient descent. `This is called as Stochastic Gradient Descent`. It turns out that these gradient approximations of random batch of data, which are poor approximations of the whole dataset help in better convergence and prevents the optimization process from getting stuck in the local minima.\n",
    "2. Shuffling the dataset ensures that the sample of the gradients computed at each step is representative of the whole dataset.\n",
    "3. Typically the number of mini-batches is needed to be set previously and it is a hyperparameter that differs from problem to problem and the sie of memory or other computational constraints. It is a `hyperparamter` just like the learning rate.\n",
    "4. In our training code previously we implement a minibatch of size 1, by picking 1 item from the dataset. The `torch.utils.data` module has a class that can help with shuffiling and  orgainizing data into minibatches : `DataLoader` Class.\n",
    "5. The job of the dataloader class is to sample and return the data from the dataset giving us the flexibilty of a particular sampling strategy.\n",
    "6. On a bare minimum the `dataloader` constructor takes in dataset as the input with the `batch_size` and `shuffle = True/False` which indicates that the data needs to be shuffled per epoch or not.\n",
    "7. As the dataloader can be iterated we can use it in our inner training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 | Loss : 0.3541380763053894\n",
      "Epoch : 20 | Loss : 0.15739838778972626\n",
      "Epoch : 30 | Loss : 0.09402313828468323\n",
      "Epoch : 40 | Loss : 0.05393235385417938\n",
      "Epoch : 50 | Loss : 0.016827018931508064\n",
      "Epoch : 60 | Loss : 0.01260560005903244\n",
      "Epoch : 70 | Loss : 0.0056186397559940815\n",
      "Epoch : 80 | Loss : 0.003184625646099448\n",
      "Epoch : 90 | Loss : 0.006937910337001085\n",
      "Epoch : 100 | Loss : 0.008527797646820545\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the dataloader class and provide it with the dataset, batch_size and shuffle arguments.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"Input layer\", nn.Linear(3072,512)),\n",
    "    (\"Activation_layer\", nn.Tanh()),\n",
    "    (\"Output_layer\", nn.Linear(512, n_out)),\n",
    "    (\"Log_softmax_1\", nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "# Define the loss function\n",
    "loss_nll = nn.NLLLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_sgd = torch.optim.SGD(model.parameters(), lr = 1e-2, )\n",
    "\n",
    "# define the batch_size\n",
    "batch_size, n_epochs = 32, 100\n",
    "\n",
    "# Define the training loop\n",
    "def training_loop(n_epochs, dataloader, optimizer, loss_fn, model):\n",
    "    # Run the loop for epochs\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # get the image and labels from the dataloader\n",
    "        for img, labels in dataloader:\n",
    "            # get the batch_size\n",
    "            batch_size = img.shape[0]\n",
    "\n",
    "            # get the predictions of the model\n",
    "            pred = model(img.view(batch_size, -1))\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(pred, labels)\n",
    "\n",
    "            # zero the gradients of the optimizer before backpropagating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagate through the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print the training and the validation losses\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch : {epoch} | Loss : {loss}\")\n",
    "\n",
    "\n",
    "# call the training loop\n",
    "training_loop(n_epochs = n_epochs, dataloader = train_loader, optimizer=optimizer_sgd, loss_fn = loss_nll, model = model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss has gone down enough but we do not know whether it is low enough. So to test the correctness of our model,  we can check the accuracy of our model on the correct classification over the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.669\n"
     ]
    }
   ],
   "source": [
    "# Now lets make a validatin set dataloader.\n",
    "val_loader = torch.utils.data.DataLoader(cifar_2_val, shuffle = False, batch_size = 32)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# now as we do not want to compute the gradients, we would use the torch.no_grad() context\n",
    "with torch.no_grad():\n",
    "    for img, labels in val_loader:\n",
    "        # get the batch_size of the image\n",
    "        batch_size = img.shape[0]\n",
    "\n",
    "        # reshape the image accoring to the batch_size and the input. This inputs an expected tenor of (32, 3072) in the model.\n",
    "        outputs = model(img.view(batch_size, -1))\n",
    "\n",
    "        # get the predictions and the index of the class with the max probability in the output \n",
    "        _, max_predicted_index = torch.max(outputs, dim=1)\n",
    "\n",
    "        # get the total labels in the batch\n",
    "        total += labels.shape[0]\n",
    "\n",
    "        # get the total number labels predicted correctly in the batch\n",
    "        correct += int((max_predicted_index == labels).sum())\n",
    "\n",
    "# print out the accuracy of the model.\n",
    "print(f\"Accuracy : {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The accuracy on the validation set is not very high, but in the models defence we can say that it is relatively shallow model, so it is not able to find the subtle nuances in the data which can differentiate between a bird and a Plane. We would focus on improving our model later.\n",
    "2.  What we can do is that we can add more layers to our model and getly taper the number of hidden neurons in the, so that the intermediate layers have a greater chance of squeezing out the information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 | Loss : 0.3814864158630371\n",
      "Epoch : 20 | Loss : 0.11479400843381882\n",
      "Epoch : 30 | Loss : 0.20899197459220886\n",
      "Epoch : 40 | Loss : 0.02062891237437725\n",
      "Epoch : 50 | Loss : 0.02024221606552601\n",
      "Epoch : 60 | Loss : 0.014094071462750435\n",
      "Epoch : 70 | Loss : 0.0018639344489201903\n",
      "Epoch : 80 | Loss : 0.007711926009505987\n",
      "Epoch : 90 | Loss : 0.004203984048217535\n",
      "Epoch : 100 | Loss : 0.002794005209580064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the dataloader class and provide it with the dataset, batch_size and shuffle arguments.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"Input layer\", nn.Linear(3072,512)),\n",
    "    (\"Activation_layer\", nn.Tanh()),\n",
    "    (\"Output_layer\", nn.Linear(512, n_out)),\n",
    "    (\"Log_softmax_1\", nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "# Define the loss function\n",
    "loss_nll = nn.NLLLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_sgd = torch.optim.SGD(model.parameters(), lr = 1e-2, )\n",
    "\n",
    "# define the batch_size\n",
    "batch_size, n_epochs = 32, 100\n",
    "\n",
    "# Define the training loop\n",
    "def training_loop(n_epochs, dataloader, optimizer, loss_fn, model):\n",
    "    # Run the loop for epochs\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # get the image and labels from the dataloader\n",
    "        for img, labels in dataloader:\n",
    "            # get the batch_size\n",
    "            batch_size = img.shape[0]\n",
    "\n",
    "            # get the predictions of the model\n",
    "            pred = model(img.view(batch_size, -1))\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(pred, labels)\n",
    "\n",
    "            # zero the gradients of the optimizer before backpropagating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagate through the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print the training and the validation losses\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch : {epoch} | Loss : {loss}\")\n",
    "\n",
    "\n",
    "# call the training loop\n",
    "training_loop(n_epochs = n_epochs, dataloader = train_loader, optimizer=optimizer_sgd, loss_fn = loss_nll, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.815\n"
     ]
    }
   ],
   "source": [
    "# Lets write the validation code to check the accuracy of the improved model.\n",
    "# Now lets make a validatin set dataloader.\n",
    "val_loader = torch.utils.data.DataLoader(cifar_2_val, shuffle = False, batch_size = 32)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# now as we do not want to compute the gradients, we would use the torch.no_grad() context\n",
    "with torch.no_grad():\n",
    "    for img, labels in val_loader:\n",
    "        # get the batch_size of the image\n",
    "        batch_size = img.shape[0]\n",
    "\n",
    "        # reshape the image accoring to the batch_size and the input. This inputs an expected tenor of (32, 3072) in the model.\n",
    "        outputs = model(img.view(batch_size, -1))\n",
    "\n",
    "        # get the predictions and the index of the class with the max probability in the output \n",
    "        _, max_predicted_index = torch.max(outputs, dim=1)\n",
    "\n",
    "        # get the total labels in the batch\n",
    "        total += labels.shape[0]\n",
    "\n",
    "        # get the total number labels predicted correctly in the batch\n",
    "        correct += int((max_predicted_index == labels).sum())\n",
    "\n",
    "# print out the accuracy of the model.\n",
    "print(f\"Accuracy : {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the validation accuracy of the model has significantly improved from 66.9% to 81%, and that to with adding just two hidden layers, our model was able to better classify pictures into birds and planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The combination of nn.LogSoftmax and nn.NLLLoss(negative loglikelihood) is equivalent to using nn.CrossEntropyLoss(). This is particular to pytorch as The nn.NLLLoss computes the crossentropy loss but with log probability predictions as inputs whereas the nn.CrossEntropyLoss() takes score(sometimes called as logits) as inputs.\n",
    "2. So technically both the losses are the negative log-likelihood of the model parameters given the data when our model predicts the softmax applied probabilities\n",
    "3. `It is quite common to drop the nn.LogSoftmax() layer from the model and use nn.CrossEntoryLoss() as the loss_function.`\n",
    "4. `Note that the numbers will be exactly the same as nn.LogSoftmax() and nn.NLLLoss, the only convienience being that we can compute the result in one swoop but the output will not be interpretable as probabilities. We will need to explicitly pass the output through a softmax to get that as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model has a very low loss on the training set but the accuracy is not very high on the validation se i.e it is memorizing the pictures of birds and planes but not generalizing very well. In short it is `overfitting`.\\\n",
    "1. Pytorch offers a quick way to look at how many paramters a model has by `model.parameters()` method. To find out how many elements are in each tensor instance we call the `numel()` method. Summing those gives us the total count. But we might want to also see if the tensors have the requires_grad attribute set to True. We might want to differentiate between the number of trainable parameters and non-trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574402, [1572864, 512, 1024, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of our model parameters in our latest fully connected model is about twice as our first model.\\\n",
    "The number of parameters of the first layer of an input Linear layer is 3072*1024 = 3145728. This means that our model would not scale well with the number of inputs. Because a `1024*1024`image would have 3.1 million input parameters and using a 1024 hidden parameters we go to 3 billion parameters. If we use 32-bit floats we are already at 12GB of GPU/CPU ram and we haven't even hit the second layer yet and calculated the gradients. That's just not going to fit on present day GPU's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The limits of going Fully Connected\n",
    "1. If we are using a linear model and using a image represented as vecotrs we are just relying on the combination of pixel values to be unique to be relevant to our task at hand, but we are not taking advantage of the spatial raltionship between our pixels i.e we are not using the information about how much distance one pixels is away from the other pixel i.e if one pixel is neighbouring to the other pixel and far away from the other.\n",
    "2. A fully connected network will se an airplane image and would think that, a bright pixel at 1,1 and dark at 1,2 and dark at 1,3 and so on is a good indication of an airplane and would learn that, but the same if it is at the bottom of the screen, then it would again need to learn that the dark pixeld at 25,25,27 etc. is a good indication. So it would need to learn it again. `Being fully connected means that it is not Translation Invariant.` This means that a network trained to recognize a spitfire at 4,4 will not be able to recognize the same plane if it is at 8,8.\n",
    "3. So to combat this problem we would need to augment the image in many ways so that the network has a chance to see planes all over the image and we would need to do so all over the image. But this augmentation comes at a cost, the number of parameters must be large enough to keep all the information about these augmentations.\n",
    "4. So we need to find out a way so that our model has to save upon storing learning the pixel arrangement for all of these augmented images, rather it should learn in such a way that spatial information can also be utilised. Convolutional Networks help in these cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
